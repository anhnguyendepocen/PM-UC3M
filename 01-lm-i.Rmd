
# Linear models I: multiple linear model {#lm-i}

The multiple linear model is a simple but useful statistical model. In short, it allows us to analyse the (assumed) linear relation between a response $Y$ and *multiple* predictors, $X_1,\ldots,X_p$ in a proper way:
\[
Y=\beta_0+\beta_1X_1+\beta_2X_2+\ldots+\beta_pX_p+\varepsilon
\]
The simplest case corresponds to $p=1$, known as the *simple* linear model:
\[
Y=\beta_0+\beta_1X+\varepsilon
\]
This model would be useful, for example, to predict $Y$ given $X$ from a sample $(X_1,Y_1),\ldots,(X_n,Y_n)$ such that its scatterplot is the one in Figure \@ref(fig:linrel).

(ref:linreltitle) Scatterplot of a sample $(X_1,Y_1),\ldots,(X_n,Y_n)$ showing a linear pattern.

```{r, linrel, echo = FALSE, out.width = '70%', fig.pos = 'h!', fig.cap = '(ref:linreltitle)', cache = TRUE, fig.asp = 1}
set.seed(234567)
x <- rnorm(200)
y <- 1 + 2 * x + rnorm(200)
plot(x, y, pch = 16, xlab = "X", ylab = "Y")
```

## Case study: *The Bordeaux equation* {#lm-i-lab-wine}

```{r, video1, echo = FALSE, out.width = '90%', fig.pos = 'h!', fig.cap = 'ABC interview to Orley Ashenfelter, broadcasted in 1992.', screenshot.opts = list(delay = 15), dev = 'png', cache = TRUE}
knitr::include_url("https://www.youtube.com/embed/Ec8hPHLMyzY")
```

> Calculate the winter rain and the harvest rain (in millimeters). Add summer heat in the vineyard (in degrees centigrade). Subtract 12.145. And what do you have? A very, very passionate argument over wine.
>
> --- "Wine Equation Puts Some Noses Out of Joint", [The New York Times](http://www.nytimes.com/1990/03/04/us/wine-equation-puts-some-noses-out-of-joint.html), 04/03/1990

This case study is motivated by the study of Princeton professor Orley Ashenfelter [@Ashenfelter1995] on the quality of red Bordeaux vintages. The study became mainstream after disputes with the wine press, especially with Robert Parker, Jr., one of the most influential wine critics in America. See a short review of the story at the [Financial Times](http://www.ft.com/cms/s/0/1e9cb152-5824-11dc-8c65-0000779fd2ac.html) ([Google's cache](https://webcache.googleusercontent.com/search?q=cache:1mRF68v_Uz4J:https://www.ft.com/content/1e9cb152-5824-11dc-8c65-0000779fd2ac)) and at the video in Figure \@ref(fig:video1).

Red Bordeaux wines have been produced in Bordeaux, one of most famous and prolific wine regions in the world, in a very similar way for hundreds of years. However, *the quality of vintages is largely variable* from one season to another due to a long list of random factors, such as the weather conditions. Because Bordeaux wines taste better when they are older (young wines are astringent, when the wines age they lose their astringency), there is an incentive to store the young wines until they are mature. Due to the important difference in taste, it is hard to determine the quality of the wine when it is so young just by tasting it, because it is going to change substantially when the aged wine is in the market. Therefore, being able to *predict the quality of a vintage* is valuable information for investing resources, for determining a fair price for vintages, and for understanding what factors are affecting the wine quality. The purpose of this case study is to answer:

- Q1. *Can we predict the quality of a vintage effectively?*
- Q2. *What is the interpretation of such prediction?*

The `wine.csv` file ([download](https://raw.githubusercontent.com/egarpor/PM-UC3M/master/datasets/wine.csv)) contains 27 red Bordeaux vintages. The data is the same data originally employed by @Ashenfelter1995, except for the inclusion of the variable `Year`, the exclusion of `NA`s and the reference price used for the wine. The original source is [here](http://www.liquidasset.com/winedata.html). Each row has the following variables:

- `Year`: year in which grapes were harvested to make wine.
- `Price`: *logarithm* of the average market price for Bordeaux vintages according to 1990--1991 auctions. The price is relative to the price of the 1961 vintage, regarded as the best one ever recorded.
- `WinterRain`: winter rainfall (in mm).
- `AGST`: Average Growing Season Temperature (in Celsius degrees).
- `HarvestRain`: harvest rainfall (in mm).
- `Age`: age of the wine measured as the number of years stored in a cask.
- `FrancePop`: population of France at `Year` (in thousands).

The *quality* of the wine is quantified as the `Price`, a clever way of quantifying a qualitative measure. The data is shown in Table \@ref(tab:winetable).

(ref:winetabletitle) First 15 rows of the `wine` dataset.

```{r, winetable, echo = FALSE, out.width = '90%', fig.pos = 'h!', cache = TRUE}
wine <- read.csv(file = "datasets/wine.csv", header = TRUE)
knitr::kable(
  head(wine, 15),
  booktabs = TRUE,
  longtable = TRUE,
  caption = '(ref:winetabletitle)'
)
```

We will see along the chapter how to answer Q1 and Q2 and how to obtain quantitative insights on the effects of the predictors on the price.

## Model formulation and least squares {#lm-i-model}

In order to simplify the introduction of the foundations of the linear model, we first present the simple linear model and then we extend to the multiple case.

### Simple linear model {#lm-i-model-simp}

The simple linear model is *constructed by assuming* that the linear relation
\begin{align}
Y = \beta_0 + \beta_1 X + \varepsilon (\#eq:1)
\end{align}
holds between $X$ and $Y$. In \@ref(eq:1), $\beta_0$ and $\beta_1$ are known as the *intercept* and *slope*, respectively. $\varepsilon$ is a random variable with mean zero and independent from $X$. It describes the *error* around the mean, or the effect of other variables that we do not model. Another way of looking at \@ref(eq:1) is
\begin{align}
\mathbb{E}[Y|X=x]=\beta_0+\beta_1x, (\#eq:2)
\end{align}
since $\mathbb{E}[\varepsilon|X=x]=0$.

The Left Hand Side (LHS) of \@ref(eq:2) is the *conditional expectation* of $Y$ given $X$. It represents how the mean of the random variable $Y$ is changing according to a particular value $x$ of the random variable $X$. With the RHS, what we are saying is that the mean of $Y$ is changing in a *linear* fashion with respect to the value of $X$. Hence the **interpretation of the coefficients**:

- $\beta_0$: is the mean of $Y$ when $X=0$.
- $\beta_1$: is the increment in mean of $Y$ for an increment of one unit in $X=x$.

If we have a sample $(X_1,Y_1),\ldots,(X_n,Y_n)$ for our random variables $X$ and $Y$, we can estimate the *unknown* coefficients $\beta_0$ and $\beta_1$. A possible way of estimating $(\beta_0,\beta_1)$ is by looking for certain optimality, for example the minimization of the *Residual Sum of Squares* (RSS):
\begin{align*}
\text{RSS}(\beta_0,\beta_1):=\sum_{i=1}^n(Y_i-\beta_0-\beta_1X_i)^2.
\end{align*}
In other words, we look for the estimators $(\hat\beta_0,\hat\beta_1)$ such that
\begin{align*}
(\hat\beta_0,\hat\beta_1)=\arg\min_{(\beta_0,\beta_1)\in\mathbb{R}^2} \text{RSS}(\beta_0,\beta_1).
\end{align*}

The motivation for minimizing the RSS is geometrical, as shown by Figure \@ref(fig:leastsquares). We look to minimize the squares of the distances of points projected vertically onto the line determined by $(\hat\beta_0,\hat\beta_1)$.

(ref:leastsquarestitle) The effect of the kind of distance in the error criterion. The choices of intercept and slope that minimize the sum of squared distances for one kind of distance are not the optimal choices for a different kind of distance. Application also available [here](https://ec2-35-177-34-200.eu-west-2.compute.amazonaws.com/least-squares/).

```{r, leastsquares, echo = FALSE, fig.cap = '(ref:leastsquarestitle)', screenshot.alt = "images/screenshots/least-squares.png", dev = 'png', cache = TRUE, fig.pos = 'h!', out.width = '90%'}
knitr::include_app('https://ec2-35-177-34-200.eu-west-2.compute.amazonaws.com/least-squares/', height = '900px')
```

It can be seen that *the minimizers of the RSS*^[They are unique and always exist. They can be obtained by solving $\frac{\partial}{\partial \beta_0}\text{RSS}(\beta_0,\beta_1)=0$ and $\frac{\partial}{\partial \beta_1}\text{RSS}(\beta_0,\beta_1)=0$.] are
\begin{align}
\hat\beta_0=\bar{Y}-\hat\beta_1\bar{X},\quad \hat\beta_1=\frac{s_{xy}}{s_x^2},(\#eq:3)
\end{align}
where:

- $\bar{X}=\frac{1}{n}\sum_{i=1}^nX_i$ is the sample mean.
- $s_x^2=\frac{1}{n}\sum_{i=1}^n(X_i-\bar{X})^2$ is the sample variance. (The sample standard deviation is $s_x=\sqrt{s_x^2}$.)
- $s_{xy}=\frac{1}{n}\sum_{i=1}^n(X_i-\bar{X})(Y_i-\bar{Y})$ is the sample covariance. It measures the degree of linear association between $X_1,\ldots,X_n$ and $Y_1,\ldots,Y_n$. Once scaled by $s_xs_y$, it gives the sample correlation coefficient $r_{xy}=\frac{s_{xy}}{s_xs_y}$.

There are some important points hidden behind the election of RSS as the error criterion for obtaining $(\hat\beta_0,\hat\beta_1)$:

- *Why the vertical distances and not horizontal or perpendicular?* Because we want to minimize the error in the *prediction* of $Y$! Note that the treatment of the variables is *not symmetrical*.
- *Why the squares in the distances and not the absolute value?* Due to mathematical convenience. Squares are nice to differentiate and are closely related with maximum likelihood estimation under the normal distribution (see Appendix \@ref(app-mle)).

Let's see how to obtain automatically the minimizers of the error in Figure \@ref(fig:leastsquares) by the `lm` (**l**inear **m**odel) function. The data of the figure has been generated with the following code:
```{r, lscheck-1, collapse = TRUE, cache = TRUE}
# Generates 50 points from a N(0, 1): predictor and error
set.seed(34567)
x <- rnorm(n = 50)
eps <- rnorm(n = 50)

# Responses
yLin <- -0.5 + 1.5 * x + eps
yQua <- -0.5 + 1.5 * x^2 + eps
yExp <- -0.5 + 1.5 * 2^x + eps

# Data
leastSquares <- data.frame(x = x, yLin = yLin, yQua = yQua, yExp = yExp)
```
For a simple linear model, `lm` has the syntax `lm(formula = response ~ predictor, data = data)`, where `response` and `predictor` are the names of two variables in the data frame `data`. Note that the LHS of `~` represents the response and the RHS the predictors.
```{r, lscheck-2, collapse = TRUE, cache = TRUE, out.width = '70%', fig.pos = 'h!'}
# Call lm
lm(yLin ~ x, data = leastSquares)
lm(yQua ~ x, data = leastSquares)
lm(yExp ~ x, data = leastSquares)

# The lm object
mod <- lm(yLin ~ x, data = leastSquares)
mod

# mod is a list of objects whose names are
names(mod)

# We can access these elements by $
mod$coefficients

# We can produce a plot with the linear fit easily
plot(x, yLin)
abline(coef = mod$coefficients, col = 2)
```

```{block2, exdistances, type = 'rmdexercise', cache = TRUE}
Check that you can *not* improve the error in Figure \@ref(fig:leastsquares) when using the coefficients given by `lm`, *if* vertical distances are selected. Check also that these coefficients are *only* optimal for vertical distances.
```

An interesting exercise is to check that `lm` is actually implementing the estimates given in \@ref(eq:3):
```{r, lmcov, echo = TRUE, collapse = TRUE, cache = TRUE}
# Covariance
Sxy <- cov(x, yLin)

# Variance
Sx2 <- var(x)

# Coefficients
beta1 <- Sxy / Sx2
beta0 <- mean(yLin) - beta1 * mean(x)
c(beta0, beta1)

# Output from lm
mod <- lm(yLin ~ x, data = leastSquares)
mod$coefficients
```

```{block, caupopreg, type = 'rmdcaution', cache = TRUE}
The *population regression coefficients*, $(\beta_0,\beta_1)$, **are not the same** as the *estimated regression coefficients*, $(\hat\beta_0,\hat\beta_1)$:

- $(\beta_0,\beta_1)$ are the theoretical and **always** unknown quantities (except under controlled scenarios).
- $(\hat\beta_0,\hat\beta_1)$ are the estimates computed from the data. In particular, they are the output of `lm`. They are *random variables*, since they are computed from the random sample $(X_1,Y_1),\ldots,(X_n,Y_n)$.

In an abuse of notation, the term *regression line* is often used to denote both the *theoretical* ($y=\beta_0+\beta_1x$) and the *estimated* ($y=\hat\beta_0+\hat\beta_1x$) regression lines.
```

#### Case study application {#lm-i-model-simp-case}

Let's get back to the `wine` dataset and compute some simple linear regressions. Prior to that, let's begin by summarizing the information in Table \@ref(tab:winetable) to get a grasp of the structure of the data. For that, we first correctly import the dataset into `R`:
```{r, wintab-1, collapse = TRUE, cache = TRUE, eval = FALSE}
# Read data
wine <- read.table(file = "wine.csv", header = TRUE, sep = ",")
```

Now we can conduct a quick exploratory analysis to have insights into the data:
```{r, wintab-2, collapse = TRUE, cache = TRUE, fig.asp = 1, out.width = '90%', fig.pos = 'h!'}
# Numerical - marginal distributions
summary(wine)

# Graphical - pairwise relations
scatterplotMatrix(wine)
```

As we can see, `Year` and `FrancePop` are very dependent, and `Year` and `Age` are *perfectly* dependent. This is so because `Age` = 1983 - `Year`. Therefore, we opt to remove the predictor `Year` and use it to set the case names, which can be helpful later for identifying outliers:
```{r, wintab-3, collapse = TRUE, cache = TRUE, fig.asp = 1}
# Set row names to Year - useful for outlier identification
row.names(wine) <- wine$Year
wine$Year <- NULL
```

Remember that the objective is to **predict** `Price`. Based on the above matrix scatterplot, the best we can predict `Price` by a simple linear regression seems to be with `AGST` or `HarvestRain`. Let's see which one yields the larger $R^2$, which, as we will see later in the chapter, is indicative of the performance of the linear model.
```{r, pred, collapse = TRUE, cache = TRUE}
# Price ~ AGST
modAGST <- lm(Price ~ AGST, data = wine)

# Summary of the model
summary(modAGST)

# The summary is also an object
sumModAGST <- summary(modAGST)
names(sumModAGST)

# R^2
sumModAGST$r.squared
```

```{block, exR2, type = 'rmdexercise', cache = TRUE}
Complete the analysis by computing the linear models `Price ~ FrancePop`, `Price ~ Age` and `Price ~ WinterRain`. Name them as `modFrancePop`, `modAge`, and `modWinterRain`. Obtain their $R^2$ and display them in a table like:

| Predictor | $R^2$ |
|:----------|:------|
|`AGST`| $0.4456$ |
|`HarvestRain`| $0.2572$ |
|`FrancePop`| $0.2314$ |
|`Age`| $0.2120$ |
|`WinterRain`| $0.0181$ |

```

```{r, mods, echo = FALSE, cache = TRUE}
modFrancePop <- lm(Price ~ FrancePop, data = wine)
modAge <- lm(Price ~ Age, data = wine)
modWinterRain <- lm(Price ~ WinterRain, data = wine)
```

It seems that none of these simple linear models on their own are properly explaining `Price`. Intuitively, it would make sense to *bind* them together to achieve a better explanation of `Price`. Let's see how to do that.

### Multiple linear model {#lm-i-model-mult}

The multiple linear model extends the simple linear model by describing the relation between several random variables $X_1,\ldots,X_p$^[Note that now $X_1$ represents the first predictor and not the first element of a sample of $X$.] and $Y$. Therefore, as before, the multiple linear model is *constructed by assuming* that the linear relation
\begin{align}
Y = \beta_0 + \beta_1 X_1 + \ldots + \beta_p X_p + \varepsilon (\#eq:k1)
\end{align}
holds between the predictors $X_1,\ldots,X_p$ and the response $Y$. In \@ref(eq:k1), $\beta_0$ is the *intercept* and $\beta_1,\ldots,\beta_p$ are the *slopes*, respectively. $\varepsilon$ is a random variable with mean zero and independent from $X_1,\ldots,X_p$. Another way of looking at \@ref(eq:k1) is
\begin{align}
\mathbb{E}[Y|X_1=x_1,\ldots,X_p=x_p]=\beta_0+\beta_1x_1+\ldots+\beta_px_p, (\#eq:k2)
\end{align}
since $\mathbb{E}[\varepsilon|X_1=x_1,\ldots,X_p=x_p]=0$.

The LHS of \@ref(eq:k2) is the conditional expectation of $Y$ given $X_1,\ldots,X_p$. It represents how the mean of the random variable $Y$ is changing, now according to particular values of several predictors. With the RHS, what we are saying is that the mean of $Y$ is changing in a *linear* fashion with respect to the values of $X_1,\ldots,X_p$. Hence the interpretation of the coefficients:

- $\beta_0$: is the mean of $Y$ when $X_1=\ldots=X_p=0$.
- $\beta_j$, $1\leq j\leq p$: is the increment in mean of $Y$ for an increment of one unit in $X_j=x_j$, provided that the remaining variables $X_1,\ldots,X_{j-1},X_{j+1},\ldots,X_p$ *do not change*.

Figure \@ref(fig:leastsquares2) illustrates the geometrical interpretation of a multiple linear model: a plane in the $(p+1)$-dimensional space. If $p=1$, the plane is the regression line for simple linear regression. If $p=2$, then the plane can be visualized in a three-dimensional plot.

(ref:leastsquares2title) The least squares regression plane $y=\hat\beta_0+\hat\beta_1x_1+\hat\beta_2x_2$ and its dependence on the kind of squared distance considered. Application also available [here](https://ec2-35-177-34-200.eu-west-2.compute.amazonaws.com/least-squares-3D/).

```{r, leastsquares2, echo = FALSE, fig.cap = '(ref:leastsquares2title)', screenshot.alt = "images/screenshots/least-squares-3D.png", dev = 'png', cache = TRUE, fig.pos = 'h!', out.width = '90%'}
knitr::include_app('https://ec2-35-177-34-200.eu-west-2.compute.amazonaws.com/least-squares-3D/', height = '700px')
```

The estimation of $\beta_0,\beta_1,\ldots,\beta_p$ is done as in simple linear regression, by minimizing the RSS (which now accounts for the sum of squared distances of the data to the vertical projections in the plane). First we need to introduce some helpful matrix notation:

- A sample of $(X_1,\ldots,X_p,Y)$ is $(X_{11},\ldots,X_{1p},Y_1),\ldots,(X_{n1},\ldots,X_{np},Y_n)$, where $X_{ij}$ denotes the $i$-th observation of the $j$-th predictor $X_j$. We denote with $\mathbf{X}_i:=(X_{i1},\ldots,X_{ip})$ to the $i$-th observation of $(X_1,\ldots,X_p)$, so the sample simplifies to $(\mathbf{X}_{1},Y_1),\ldots,(\mathbf{X}_{n},Y_n)$.

- The *design matrix* contains all the information of the predictors and a column of ones
\[
\mathbf{X}:=\begin{pmatrix}
1 & X_{11} & \cdots & X_{1p}\\
\vdots & \vdots & \ddots & \vdots\\
1 & X_{n1} & \cdots & X_{np}
\end{pmatrix}_{n\times(p+1)}.
\]

- The *vector of responses* $\mathbf{Y}$, the *vector of coefficients* $\boldsymbol{\beta}$ and the *vector of errors* are, respectively,
\[
\mathbf{Y}:=\begin{pmatrix}
Y_1 \\
\vdots \\
Y_n
\end{pmatrix}_{n\times 1},\quad\boldsymbol{\beta}:=\begin{pmatrix}
\beta_0 \\
\beta_1 \\
\vdots \\
\beta_p
\end{pmatrix}_{(p+1)\times 1}\text{ and }
\boldsymbol{\varepsilon}:=\begin{pmatrix}
\varepsilon_1 \\
\vdots \\
\varepsilon_n
\end{pmatrix}_{n\times 1}.
\]
Thanks to the matrix notation, we can turn the sample version of the multiple linear model, namely
\[
Y_i=\beta_0 + \beta_1 X_{i1} + \ldots +\beta_p X_{ip} + \varepsilon_i,\quad i=1,\ldots,n,
\]
into something as compact as
\[
\mathbf{Y}=\mathbf{X}\boldsymbol{\beta}+\boldsymbol{\varepsilon}.
\]

```{block2, insdesign1, type = 'rmdinsight', cache = TRUE}
Recall that if $p=1$ we have the simple linear model. In this case:
\[
\mathbf{X}=\begin{pmatrix}
1 & X_{11}\\
\vdots & \vdots\\
1 & X_{n1}
\end{pmatrix}_{n\times2}\text{ and } \beta=\begin{pmatrix}
\beta_0 \\
\beta_1
\end{pmatrix}_{2\times 1}.
\]
```

With this notation, the RSS for the multiple linear regression is
\begin{align}
\text{RSS}(\boldsymbol{\beta}):=&\,\sum_{i=1}^n(Y_i-\beta_0-\beta_1X_{i1}-\ldots-\beta_pX_{ip})^2\nonumber\\
=&(\mathbf{Y}-\mathbf{X}\boldsymbol{\beta})'(\mathbf{Y}-\mathbf{X}\boldsymbol{\beta}).(\#eq:rss)
\end{align}
The RSS aggregates the *squared vertical distances* from the data to a regression plane given by $\boldsymbol{\beta}$. The least squares estimators are *the minimizers of the RSS*^[They are unique and always exist.]:
\begin{align*}
\hat{\boldsymbol{\beta}}:=\arg\min_{\boldsymbol{\beta}\in\mathbb{R}^{p+1}} \text{RSS}(\boldsymbol{\beta}).
\end{align*}
Luckily, thanks to the matrix form of \@ref(eq:rss), it is possible^[If follows from $\frac{\partial \mathbf{A}\mathbf{x}}{\partial \mathbf{x}}=\mathbf{A}$ and $\frac{\partial f(\mathbf{x})'g(\mathbf{x})}{\partial \mathbf{x}}=f(\mathbf{x})'\frac{\partial g(\mathbf{x})}{\partial \mathbf{x}}+g(\mathbf{x})'\frac{\partial}{\partial f(\mathbf{x})}$ for two vector-valued functions $f$ and $g$.] to compute a closed-form expression for the least squares estimates:
\begin{align}
\hat{\boldsymbol{\beta}}=(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{Y}.(\#eq:k3)
\end{align}

```{block2, insestcoefinterp, type = 'rmdinsight', cache = TRUE}
There are some similarities between \@ref(eq:k3) and $\hat\beta_1=(s_x^2)^{-1}s_{xy}$ from the simple linear model: both are related to the covariance between $\mathbf{X}$ and $Y$ weighted by the variance of $\mathbf{X}$.
```

Let's check that indeed the coefficients given by `lm` are the ones given by equation \@ref(eq:k3). For that purpose we consider the `leastSquares3D` data frame in the `least-squares-3D.RData` dataset ([download](https://raw.githubusercontent.com/egarpor/PM-UC3M/master/datasets/least-squares-3D.RData)). Among other variables, the data frame contains the response `yLin` and the predictors `x1` and `x2`.
```{r, lscheck3d-1, echo = FALSE, cache = TRUE}
# Generates 50 points from a N(0, 1): predictors and error
set.seed(34567) # Fixes the seed for the random generator
x1 <- rnorm(50)
x2 <- rnorm(50)
x3 <- x1 + rnorm(50, sd = 0.05) # Make variables dependent
eps <- rnorm(50)

# Responses
yLin <- -0.5 + 0.5 * x1 + 0.5 * x2 + eps
yQua <- -0.5 + x1^2 + 0.5 * x2 + eps
yExp <- -0.5 + 0.5 * exp(x2) + x3 + eps

# Data
leastSquares3D <- data.frame(x1 = x1, x2 = x2, yLin = yLin,
                             yQua = yQua, yExp = yExp)
```
```{r, lscheck3d-2, echo = TRUE, cache = TRUE, eval = FALSE}
load(file = "leastSquares3D.RData")
```
Let's compute the coefficients of the regression of `yLin` into the predictors `x1` and `x2`, which is denoted by `yLin ~ x1 + x2`. Note the use of `+` for including all the predictors. This does *not* mean that they are all summed and then the regression is done on the sum!^[If you wanted to do so, you will need the function `I()` for indicating that `+` is not including predictors in the model, but is acting as a sum operator.]. Instead, this notation is designed to resemble the multiple linear model.
].
```{r, ls3dcheck-3, echo = TRUE, collapse = TRUE, cache = TRUE}
# Output from lm
mod <- lm(yLin ~ x1 + x2, data = leastSquares3D)
mod$coefficients

# Matrix X
X <- cbind(1, x1, x2)

# Vector Y
Y <- yLin

# Coefficients
beta <- solve(t(X) %*% X) %*% t(X) %*% Y
# %*% multiplies matrices
# solve() computes the inverse of a matrix
# t() transposes a matrix
beta
```

```{block2, excheck, type = 'rmdexercise', cache = TRUE}
Compute $\boldsymbol{\beta}$ for the regressions `yLin ~ x1 + x2`, `yQua ~ x1 + x2`, and `yExp ~ x2 + x3` using:

- equation \@ref(eq:k3) and
- the function `lm`.

Check that both are the same.
```

Once we have the least squares estimates $\hat{\boldsymbol{\beta}}$, we can define the next concepts:

- The *fitted values* $\hat Y_1,\ldots,\hat Y_n$, where
\begin{align*}
\hat Y_i:=\hat\beta_0+\hat\beta_1X_{i1}+\ldots+\hat\beta_pX_{ip},\quad i=1,\ldots,n.
\end{align*}
They are the vertical projections of $Y_1,\ldots,Y_n$ into the fitted plane (see Figure \@ref(fig:leastsquares2)). In a matrix form, inputting \@ref(eq:rss)
\[
\hat{\mathbf{Y}}=\mathbf{X}\hat{\boldsymbol{\beta}}=\mathbf{X}(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{Y}=\mathbf{H}\mathbf{Y},
\]
where $\mathbf{H}:=\mathbf{X}(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'$ is called the *hat matrix* because it "puts the hat into $\mathbf{Y}$". What it does is to project $\mathbf{Y}$ into the regression plane (see Figure \@ref(fig:leastsquares2)).

- The *residuals* (or estimated errors) $\hat \varepsilon_1,\ldots,\hat \varepsilon_n$, where
\begin{align*}
\hat\varepsilon_i:=Y_i-\hat Y_i,\quad i=1,\ldots,n.
\end{align*}
They are the vertical distances between actual data and fitted data.

These objects are present in the output of `lm`;
```{r, fitres, echo = TRUE, collapse = TRUE, cache = TRUE, eval = FALSE}
# Fitted values
mod$fitted.values

# Residuals
mod$residuals
```

We conclude with an insight on the relation of multiple and simple linear regressions. It is illustrated in Figure \@ref(fig:multmarg).

```{block, insjointmargcoef, type = 'rmdinsight', cache = TRUE}
Consider the multiple linear model $Y=\beta_0+\beta_1X_1+\beta_2X_2+\varepsilon$ and its associated simple linear models $Y=\alpha_0+\alpha_1X_1+\varepsilon$ and $Y=\gamma_0+\gamma_1X_2+\varepsilon$. Assume that we have a sample $(X_{11},X_{12},Y_1),\ldots, (X_{n1},X_{n2},Y_n)$. Then, in general, $\hat\alpha_0\neq\hat\beta_0$, $\hat\alpha_1\neq\hat\beta_1$, $\hat\gamma_0\neq\hat\beta_0$, and $\hat\gamma_1\neq\hat\beta_1$. That is, in general, **the inclusion of a new predictor changes the coefficient estimates**.
```

```{r, multmarg, echo = FALSE, out.width = '70%', fig.pos = 'h!', fig.cap = 'The regression plane (blue) and its relation with the simple linear regressions (green lines). The red points represent the sample for $(X_1,X_2,Y)$ and the black points the subsamples for $(X_1,X_2)$ (bottom), $(X_1,Y)$ (left), and $(X_2,Y)$ (right).', cache = TRUE}
knitr::include_graphics("images/R/multmarg.png")
```

The data used in Figure \@ref(fig:multmarg) is:
```{r, datamarg, echo = TRUE, collapse = TRUE, cache = TRUE}
set.seed(212542)
n <- 100
x1 <- rnorm(n, sd = 2)
x2 <- rnorm(n, mean = x1, sd = 3)
y <- 1 + 2 * x1 - x2 + rnorm(n, sd = 1)
data <- data.frame(x1 = x1, x2 = x2, y = y)
```

```{block, excoef, type = 'rmdexercise', cache = TRUE}
With the above `data`, check how the fitted coefficients change for `y ~ x1`, `y ~ x2`, and `y ~ x1 + x2`.
```

#### Case study application {#lm-i-model-mult-case}

A natural step now is to extend these simple regressions to increase both the $R^2$ and the prediction accuracy for `Price` by means of the multiple linear regression:
```{r, case1-1, collapse = TRUE, cache = TRUE}
# Regression on all the predictors
modWine1 <- lm(Price ~ Age + AGST + FrancePop + HarvestRain + WinterRain, data = wine)

# A shortcut
modWine1 <- lm(Price ~ ., data = wine)
modWine1

# Summary
summary(modWine1)
```
The fitted regression is `Price` $= -2.343 + 0.013\,\times$ `Age` $+ 0.614\,\times$ `AGST` $- 0.000\,\times$ `FrancePop` $- 0.003\,\times$ `HarvestRain` $+ 0.001\,\times$ `WinterRain`. Recall that the `'Multiple R-squared'` has almost doubled with respect to the best simple linear regression! This tells us that combining several predictors may lead to important performance gains in the prediction of the response. However, note that the $R^2$ of the multiple linear model is *not* the sum of the $R^2$'s of the simple linear models. The performance gain of combining predictors is hard to anticipate from the single-predictor models.

## Assumptions of the model {#lm-i-assumps}

Why do we need assumptions? To make **inference** on the model parameters. In other words, to infer properties about the *unknown* population coefficients $\boldsymbol{\beta}$ from the sample $(\mathbf{X}_1, Y_1),\ldots,(\mathbf{X}_n, Y_n)$.

(ref:linearmodeltitle) The key concepts of the simple linear model. The yellow band denotes where $95\%$ of the data is, according to the model.

```{r, linearmodel, echo = FALSE, out.width = '70%', fig.pos = 'h!', fig.cap = '(ref:linearmodeltitle)', cache = TRUE}
knitr::include_graphics("images/R/linearmodel.png")
```

(ref:linearmodeltitle2) The key concepts of the multiple linear model when $p=2$. The space between the yellow planes denotes where $95\%$ of the data is, according to the model.

```{r, linearmodel2, echo = FALSE, out.width = '70%', fig.pos = 'h!', fig.cap = '(ref:linearmodeltitle2)', cache = TRUE}
knitr::include_graphics("images/R/linearmodel2.png")
```

The assumptions of the multiple linear model are:

i. **Linearity**: $\mathbb{E}[Y|X_1=x_1,\ldots,X_p=x_p]=\beta_0+\beta_1x_1+\ldots+\beta_px_p$.
ii. **Homoscedasticity**: $\mathbb{V}\text{ar}[\varepsilon|X_1=x_1,\ldots,X_p=x_p]=\sigma^2$.
iii. **Normality**: $\varepsilon\sim\mathcal{N}(0,\sigma^2)$.
iv. **Independence of the errors**: $\varepsilon_1,\ldots,\varepsilon_n$ are independent (or uncorrelated, $\mathbb{E}[\varepsilon_i\varepsilon_j]=0$, $i\neq j$, since they are assumed to be normal).

A good one-line summary of the linear model is the following (independence is implicit)
\begin{align}
Y|(X_1=x_1,\ldots,X_p=x_p)\sim \mathcal{N}(\beta_0+\beta_1x_1+\ldots+\beta_px_p,\sigma^2).(\#eq:condnorm)
\end{align}

Recall that, except assumption iv, the rest are expressed in terms of the random variables, not in terms of the sample. Thus are *population versions*, rather than *sample versions*. It is however trivial to express \@ref(eq:condnorm) in terms of assumptions about the sample $(\mathbf{X}_1,Y_1),\ldots,(\mathbf{X}_n, Y_n)$:
\begin{align}
Y_i|(X_{i1}=x_{i1},\ldots,X_{ip}=x_{ip})\sim \mathcal{N}(\beta_0+\beta_1x_{i1}+\ldots+\beta_px_{ip},\sigma^2),\quad i=1,\ldots,n(\#eq:sampcond1)
\end{align}
with $Y_1,\ldots,Y_n$ being independent conditionally on the sample of predictors. Equivalently stated in a compact matrix way, the assumptions of the model on the sample are:
\begin{align}
\mathbf{Y}|\mathbf{X}\sim\mathcal{N}_n(\mathbf{X}\boldsymbol{\beta},\sigma^2\mathbf{I}).(\#eq:sampcond2)
\end{align}

```{block2, insrempreds, type = 'rmdinsight', cache = TRUE}
Recall:

- Nothing is said about the distribution of $X_1,\ldots,X_p$. They could be deterministic (called *fixed design*) or random (*random design*). They could be discrete or continuous.

- $X_1,\ldots,X_p$ are **not required to be independent** between them.

- **$Y$ has to be continuous**, since the errors are normal -- recall \@ref(eq:1).

```

Figures \@ref(fig:linearmodelgood) and \@ref(fig:linearmodelbad) represent situations where the assumptions of the model for $p=1$ are respected and violated, respectively. 
```{r, linearmodelgood, echo = FALSE, out.width = '70%', fig.pos = 'h!', fig.cap = 'Perfectly valid simple linear models (all the assumptions are verified).', cache = TRUE}
knitr::include_graphics("images/R/linearmodelgood.png")
```

```{r, linearmodelbad, echo = FALSE, out.width = '70%', fig.pos = 'h!', fig.cap = 'Problematic simple linear models (a single assumption does not hold).', cache = TRUE}
knitr::include_graphics("images/R/linearmodelbad.png")
```

Figure \@ref(fig:linearmodelassump) represents situations where the assumptions of the model are respected and violated, for the situation with two predictors. Clearly, the inspection of the scatterplots for identifying strange patterns is more complicated than in simple linear regression -- and here we are dealing only with two predictors. 

(ref:linearmodelassumptitle) Valid (all the assumptions are verified) and problematic (a single assumption does not hold) multiple linear models, when there are two predictors. Application also available [here](https://ec2-35-177-34-200.eu-west-2.compute.amazonaws.com/assump-lm-3D/).

```{r, linearmodelassump, echo = FALSE, fig.cap = '(ref:linearmodelassumptitle)', screenshot.alt = "images/screenshots/assump-lm-3D.png", dev = 'png', cache = TRUE, fig.pos = 'h!', out.width = '90%'}
knitr::include_app('https://ec2-35-177-34-200.eu-west-2.compute.amazonaws.com/assump-lm-3D/', height = '700px')
```

```{block, exassump-1, type = 'rmdexercise', cache = TRUE}
The dataset `assumptions.RData` ([download](https://raw.githubusercontent.com/egarpor/PM-UC3M/master/datasets/assumptions.RData)) contains the variables `x1`, ..., `x9` and `y1`, ..., `y9`. For each regression `y1 ~ x1`, ..., `y9 ~ x9`:

- Check whether the assumptions of the linear model are being satisfied (make a scatterplot with a regression line).
- State which assumption(s) are violated and justify your answer.
```

## Inference for model parameters {#lm-i-inference}

The assumptions introduced in the previous section allow to specify what is the distribution of the *random vector* $\hat{\boldsymbol{\beta}}$. The distribution is derived conditionally on the sample predictors $\mathbf{X}_1,\ldots,\mathbf{X}_n$. In other words, we assume that the randomness of $\mathbf{Y}=\mathbf{X}\boldsymbol{\beta}+\boldsymbol{\varepsilon}$ comes only from the error terms and not from the predictors. To denote this, we employ lowercase for the sample predictors $\mathbf{x}_1,\ldots,\mathbf{x}_n$.

### Distributions of the fitted coefficients {#lm-i-inference-dists}

The distribution of $\hat{\boldsymbol{\beta}}$ is:
\begin{align}
\hat{\boldsymbol{\beta}}\sim\mathcal{N}_{p+1}\left(\boldsymbol{\beta},\sigma^2(\mathbf{X}'\mathbf{X})^{-1}\right).
(\#eq:normp)
\end{align}
This result can be obtained from the form of $\hat{\boldsymbol{\beta}}$ given in \@ref(eq:k3) and the sample version of the model assumptions given in \@ref(eq:sampcond2). Equation \@ref(eq:normp) implies that each *individual* $\hat\beta_j$ satisfies
\begin{align}
\hat{\beta}_j\sim\mathcal{N}\left(\beta_j,\mathrm{SE}(\hat\beta_j)^2\right),(\#eq:normp1b)
\end{align}
where $\mathrm{SE}(\hat\beta_j)$ is the *standard error*, $\mathrm{SE}(\hat\beta_j)^2:=\sigma^2v_j$, and
\[
v_j\text{ is the }j\text{-th element of the diagonal of }(\mathbf{X}'\mathbf{X})^{-1}.
\]
Recall that an equivalent form for \@ref(eq:normp1b) is (why?)
\begin{align*}
\frac{\hat\beta_j-\beta_j}{\mathrm{SE}(\hat\beta_j)}\sim\mathcal{N}(0,1).
\end{align*}

The interpretation of \@ref(eq:normp1b) is simpler in the case with $p=1$, where
\begin{align}
\hat\beta_0\sim\mathcal{N}\left(\beta_0,\mathrm{SE}(\hat\beta_0)^2\right),\quad\hat\beta_1\sim\mathcal{N}\left(\beta_1,\mathrm{SE}(\hat\beta_1)^2\right)(\#eq:norm1)
\end{align}
with
\begin{align}
\mathrm{SE}(\hat\beta_0)^2=\frac{\sigma^2}{n}\left[1+\frac{\bar X^2}{s_x^2}\right],\quad \mathrm{SE}(\hat\beta_1)^2=\frac{\sigma^2}{ns_x^2}.(\#eq:se1)
\end{align}

Therefore, some important remarks on \@ref(eq:norm1) and \@ref(eq:se1) are

- **Bias**. Both estimates are unbiased. That means that their expectations are the true coefficients.
- **Variance**. The variances $\mathrm{SE}(\hat\beta_0)^2$ and $\mathrm{SE}(\hat\beta_1)^2$ have an interesting interpretation in terms of its components:

    - *Sample size $n$*. As the sample size grows, the precision of the estimators increases, since both variances decrease.
    - *Error variance $\sigma^2$*. The more disperse the error is, the less precise the estimates are, since more vertical variability is present.
    - *Predictor variance $s_x^2$*. If the predictor is spread out (large $s_x^2$), then it is easier to fit a regression line: we have information about the data trend over a long interval. If $s_x^2$ is small, then all the data is concentrated on a narrow vertical band, so we have a much more limited view of the trend.

    - *Mean $\bar X$*. It has influence only on the precision of $\hat\beta_0$. The larger $\bar X$ is, the less precise $\hat\beta_0$ is.

(ref:randomcoefstitle) Illustration of the randomness of the fitted coefficients $(\hat\beta_0,\hat\beta_1)$ and the influence of $n$, $\sigma^2$ and $s_x^2$. The sample predictors $x_1,\ldots,x_n$ are fixed and new responses $Y_1,\ldots,Y_n$ are generated each time from a linear model $Y=\beta_0+\beta_1X+\varepsilon$. Application also available [here](https://ec2-35-177-34-200.eu-west-2.compute.amazonaws.com/lm-random/).

```{r, randomcoefs, echo = FALSE, fig.cap = '(ref:randomcoefstitle)', screenshot.alt = "images/screenshots/lm-random.png", dev = 'png', cache = TRUE, fig.pos = 'h!', out.width = '90%'}
knitr::include_app('https://ec2-35-177-34-200.eu-west-2.compute.amazonaws.com/lm-random/', height = '1000px')
```

The insights about \@ref(eq:normp) are more convoluted. The following broad remarks, extensions of what happened when $p=1$, apply:

- **Bias**. Both estimates are unbiased. 
- **Variance**. Depends on:

    - *Sample size $n$*. Hidden inside $\mathbf{X}'\mathbf{X}$. As $n$ grows, the precision of the estimators increases.
    - *Error variance $\sigma^2$*. The larger $\sigma^2$ is, the less precise $\hat{\boldsymbol{\beta}}$ is.
    - *Predictor sparsity $(\mathbf{X}'\mathbf{X})^{-1}$*. The more *sparse* the predictor is (small $|(\mathbf{X}'\mathbf{X})^{-1}|$), the more precise $\hat{\boldsymbol{\beta}}$ is.

The problem with \@ref(eq:normp) is that *$\sigma^2$ is unknown* in practice, so we need to estimate $\sigma^2$ from the data. We do so by computing a rescaled sample variance of the residuals $\hat\varepsilon_1,\ldots,\hat\varepsilon_n$:
\begin{align}
\hat\sigma^2:=\frac{1}{n-p-1}\sum_{i=1}^n\hat\varepsilon_i^2.(\#eq:varhat)
\end{align}
Note the $n-p-1$ in the denominator. Now $n-p-1$ are the *degrees of freedom*, the number of data points minus the number of already fitted parameters ($p$ slopes and $1$ intercept). For the interpretation of $\hat\sigma^2$, it is key to realize that *the mean of the residuals $\hat\varepsilon_1,\ldots,\hat\varepsilon_n$ is zero*, this is $\bar{\hat\varepsilon}=0$. Therefore, $\hat\sigma^2$ is indeed a rescaled sample variance of the residuals which estimates the variance of $\varepsilon$.

If we use the estimate $\hat\sigma^2$ instead of $\sigma^2$, we get more useful distributions than \@ref(eq:normp1b)
\begin{align}
\frac{\hat\beta_j-\beta_j}{\hat{\mathrm{SE}}(\hat\beta_j)}\sim t_{n-p-1},\quad\hat{\mathrm{SE}}(\hat\beta_j)^2:=\hat\sigma^2v_j^2(\#eq:normp2)
\end{align}
where $t_{n-p-1}$ represents the Student's $t$ distribution with $n-p-1$ degrees of freedom. 

The LHS of \@ref(eq:normp2) is the $t$-statistic for $\beta_j$, $j=0,\ldots,p$. We will employ them for building confidence intervals and hypothesis tests.

### Confidence intervals for the coefficients {#lm-i-inference-cis}

Thanks to \@ref(eq:normp2), we can have the $100(1-\alpha)\%$ Confidence Intervals (CI) for the coefficient $\beta_j$, $j=0,\ldots,p$:
\begin{align}
\left(\hat\beta_j\pm\hat{\mathrm{SE}}(\hat\beta_j)t_{n-p-1;\alpha/2}\right)(\#eq:cip)
\end{align}
where $t_{n-p-1;\alpha/2}$ is the *$\alpha/2$-upper quantile of the $t_{n-p-1}$*. Usually, $\alpha=0.10,0.05,0.01$ are considered.

This *random* CI *contains the unknown coefficient $\beta_j$ "with a probability of $1-\alpha$"*. The previous quoted statement has to be understood as follows. Suppose you have 100 samples generated according to a linear model. If you compute the CI for a coefficient, then in approximately $100(1-\alpha)$ of the samples the true coefficient would be actually inside the random CI. Note also that the CI is symmetric around $\hat\beta_j$. This is illustrated in Figure \@ref(fig:ci).

(ref:cititle) Illustration of the randomness of the CI for $\beta_0$ at $100(1-\alpha)\%$ confidence. The plot shows 100 random CIs for $\beta_0$, computed from 100 random datasets generated by the same simple linear model, with intercept $\beta_0$. The illustration for $\beta_1$ is completely analogous. Application also available [here](https://ec2-35-177-34-200.eu-west-2.compute.amazonaws.com/ci-random/).

```{r, ci, echo = FALSE, fig.cap = '(ref:cititle)', screenshot.alt = "images/screenshots/ci-random.png", dev = 'png', cache = TRUE, fig.pos = 'h!', out.width = '90%'}
knitr::include_app('https://ec2-35-177-34-200.eu-west-2.compute.amazonaws.com/ci-random/', height = '900px')
```

### Testing on the coefficients {#lm-i-inference-tests}

The distributions in \@ref(eq:normp2) also allow us to conduct a formal *hypothesis test* on the coefficients $\beta_j$, $j=0,\ldots,p$. For example the test for *significance* (shortcut for *significantly difference from zero*) is especially important, that is, the test of the hypotheses
\begin{align*}
H_0:\beta_j=0
\end{align*}
for $j=0,\ldots,p$. The test of $H_0:\beta_j=0$ with $1\leq j\leq p$ is especially interesting, since it allows us to answer whether *the variable $X_j$ has a significant linear effect on $Y$*. The statistic used for testing for significance is the $t$-statistic
\begin{align*}
\frac{\hat\beta_j-0}{\hat{\mathrm{SE}}(\hat\beta_j)},
\end{align*}
which is distributed as a $t_{n-p-1}$ *under the (veracity of) the null hypothesis*. 

The null hypothesis $H_0$ is tested *against* the *alternative hypothesis*, $H_1$. If $H_0$ is rejected, it is *rejected in favor* of $H_1$. The alternative hypothesis can be *bilateral*, such as
\begin{align*}
H_0:\beta_j= 0\quad\text{vs}\quad H_1:\beta_j\neq 0
\end{align*}
or *unilateral*, such as
\begin{align*}
H_0:\beta_j\geq (\leq)0\quad\text{vs}\quad H_1:\beta_j<(>)0.
\end{align*}
For the moment, we will focus only on bilateral hypothesis.

Remember the following insights about hypothesis testing.

```{block2, inshyp-1, type = 'rmdinsight', cache = TRUE}
The analogy of conducting an hypothesis test and a **trial** can be seen in Appendix \@ref(app-ht).
```

```{block, inshyp-2, type = 'rmdinsight', cache = TRUE}
In an hypothesis test, the *$p$-value measures the degree of veracity of $H_0$ according to the data*. The rule of thumb is the following:

**Is the $p$-value lower than $\alpha$?**

- **Yes $\rightarrow$ reject $H_0$**.
- **No $\rightarrow$ do not reject $H_0$**.
```

The connection of a $t$-test for $H_0:\beta_j=0$ and the CI for $\beta_j$, both at level $\alpha$, is the following:

> **Is $0$ inside the CI for $\beta_j$?**
>
> - **Yes $\leftrightarrow$ do not reject $H_0$**.
> - **No $\leftrightarrow$ reject $H_0$**.

The unilateral test $H_0:\beta_j\geq 0$ (respectively, $H_0:\beta_j\leq 0$) vs $H_1:\beta_j<0$ ($H_1:\beta_j>0$) can be done by means of the CI for $\beta_j$. If $H_0$ is rejected, they allow us to conclude that *$\hat\beta_j$ is significantly negative (positive)* and that *for the considered regression model, $X_j$ has a significant negative (positive) effect on $Y$*. The rule of thumb is the following:

> **Is the CI for $\beta_j$ below (above) $0$ at level $\alpha$?**
>
> - **Yes $\rightarrow$ reject $H_0$ at level $\alpha$. Conclude $X_j$ has a significant negative (positive) effect on $Y$ at level $\alpha$**.
> - **No $\rightarrow$ the criterion is not conclusive**.

### Case study application {#lm-i-inference-tests-case}

Let's analyse the multiple linear model we have considered now that we know how to make inference on the model parameters. The relevant information is obtained with the `summary` of the model:
```{r, case1-2, collapse = TRUE, cache = TRUE}
# Fit
modWine1 <- lm(Price ~ ., data = wine)

# Summary
sumModWine1 <- summary(modWine1)
sumModWine1

# Contains the estimation of sigma ("Residual standard error")
sumModWine1$sigma

# Which is the same as
sqrt(sum(modWine1$residuals^2) / modWine1$df.residual)
```

The `Coefficients` block of the output of `summary` contains the next elements regarding the significance of each coefficient $\beta_j$, this is, the test $H_0:\beta_j=0$ vs $H_1:\beta_j\neq0$:

- `Estimate`: least squares estimate $\hat\beta_j$.
- `Std. Error`: estimated standard error $\hat{\mathrm{SE}}(\hat\beta_j)$.
- `t value`: $t$-statistic $\frac{\hat\beta_j}{\hat{\mathrm{SE}}(\hat\beta_j)}$.
- `Pr(>|t|)`: $p$-value of the $t$-test.
- `Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1`: codes indicating the size of the $p$-value. The more stars, the more evidence supporting that $H_0$ does not hold.

Note then that **many predictors are not significant** in `modWine1`: `FrancePop`, `Age`, and the intercept are not significant. This is an indication of an **excess of predictors** adding little information to the response. An explanation is the almost perfect correlation between `FrancePop` and `Age` shown before: one of them is not adding any extra information to explain `Price`. This complicates the model unnecessarily and, more importantly, it has the undesirable effect of making the **coefficient estimates less precise**. We opt to remove the predictor `FrancePop` from the model since it is exogenous to the wine context (notice this is a context-guided decision, not data-driven). A data-driven justification of the removal of this variable is that it is the least significant in `modWine1`.

Then, the model without `FrancePop` (notice the use of `-` for *excluding* a particular predictor) is:
```{r, case1-3, collapse = TRUE, cache = TRUE}
modWine2 <- lm(Price ~ . - FrancePop, data = wine)
summary(modWine2)
```
All the coefficients are significant at level $\alpha=0.05$. Therefore, there is no clear redundant information. In addition, the $R^2$ is very similar to the full model, but the `'Adjusted R-squared'`, a weighting of the $R^2$ to account for the number of predictors used by the model, is slightly larger. As we will see in Section \@ref(lm-i-modfit-R2Adj), this means that, compared to the number of predictors used, `modWine2` explains more variability of `Price` than `modWine1`.

A handy way of comparing the coefficients of both models is `compareCoefs`:
```{r, case1-4, collapse = TRUE, cache = TRUE}
compareCoefs(modWine1, modWine2)
```
Note how the coefficients for `modWine2` have smaller errors than `modWine1`.

The individual CIs for the unknown $\beta_j$'s can be obtained by applying the `confint` function to  an `lm` object. Let's compute the CIs for the model coefficients of `modWine1`, `modWine2`, and a new `modWine3`:
```{r, case1-5, collapse = TRUE, cache = TRUE}
# Fit a new model
modWine3 <- lm(Price ~ Age + WinterRain, data = wine)
summary(modWine3)

# Confidence intervals at 95%
# CI: (lwr, upr)
confint(modWine3)

# Confidence intervals at other levels
confint(modWine3, level = 0.90)
confint(modWine3, level = 0.99)

# Compare with previous models
confint(modWine1)
confint(modWine2)
confint(modWine3)
```
In `modWine3`, the 95% CI for $\beta_0$ is $(4.7460, 7.2201)$, for $\beta_1$ is $(0.0077, 0.0644)$, and for $\beta_2$ is $(-0.0010, 0.0026)$. Therefore, we can say with a 95% confidence that *the coefficient of* `WinterRain` *is non significant* (`0` is inside the CI). But inspecting the CI of $\beta_2$ in `modWine2` we can see that *it is significant* for the model! How is this possible? The answer is that the presence of extra predictors affects the coefficient estimate, as we saw in Figure \@ref(fig:multmarg). Therefore, the precise statement to make is: **in the model `Price ~ Age + WinterRain`**, with $\alpha=0.05$, the coefficient of `WinterRain` is non significant. Note that this **does not** mean that it will be always non significant: in `Price ~ Age + AGST + HarvestRain + WinterRain` it is.

```{block, excis, type = 'rmdexercise', cache = TRUE}
Compute and interpret the CIs for the coefficients, at levels $\alpha=0.10,0.05,0.01$, for the following regressions:

- `Price ~ WinterRain + HarvestRain + AGST` (`wine`)
- `AGST ~ Year + FrancePop` (`wine`)
```

```{block, exassump-2, type = 'rmdexercise', cache = TRUE}
For the `assumptions` dataset, do the following:

- Regression `y7 ~ x7`. Check that:
    - The intercept of  is not significant for the regression at any reasonable level $\alpha$.
    - The slope is significant for any $\alpha>10^{-7}$.

- Regression `y6 ~ x6`. Assume the linear model assumptions are verified.
    - Check that $\hat\beta_0$ is significantly different from zero at any level
$\alpha$.
    - For which $\alpha=0.10,0.05,0.01$ is $\hat\beta_1$ significantly different from zero?
```

## Prediction {#lm-i-prediction}

The forecast of $Y$ from $\mathbf{X}=\mathbf{x}$ (this is, $X_1=x_1,\ldots,X_p=x_p$) is approached by two different ways:

1. Inference on the **conditional mean** of $Y$ given $\mathbf{X}=\mathbf{x}$, $\mathbb{E}[Y|\mathbf{X}=\mathbf{x}]$. This is a deterministic quantity, which equals $\beta_0+\beta_1x_1+\ldots+\beta_{p}x_p$.
2. Prediction of the **conditional response** $Y|\mathbf{X}=\mathbf{x}$. This is a random variable distributed as $\mathcal{N}(\beta_0+\beta_1x_1+\ldots+\beta_{p}x_p,\sigma^2)$.

Similarities and differences in the prediction of the conditional mean $\mathbb{E}[Y|\mathbf{X}=\mathbf{x}]$ and conditional response $Y|\mathbf{X}=\mathbf{x}$:

- *Similarities*. The estimate is the same, $\hat y=\hat\beta_0+\hat\beta_1x_1+\ldots+\hat\beta_px_p$. Both CI are centered in $\hat y$.
- *Differences*. $\mathbb{E}[Y|\mathbf{X}=\mathbf{x}]$ is deterministic and $Y|\mathbf{X}=\mathbf{x}$ is random. The prediction of the latter is more noisy, because it has to take into account the variability of $Y$. Therefore, the variance is larger for the prediction of $Y|\mathbf{X}=\mathbf{x}$ than for the prediction of $\mathbb{E}[Y|\mathbf{X}=\mathbf{x}]$. This has a direct consequence on the length of the prediction intervals, which are longer for $Y|\mathbf{X}=\mathbf{x}$ than for $\mathbb{E}[Y|\mathbf{X}=\mathbf{x}]$.

The inspection of the CIs for the conditional mean and conditional response in the simple linear model help to fix ideas:

- The $100(1-\alpha)\%$ CI for the *conditional mean* $\beta_0+\beta_1x$ is 
\begin{align}
\left(\hat y \pm t_{n-2:\alpha/2}\sqrt{\frac{\hat\sigma^2}{n}\left(1+\frac{(x-\bar x)^2}{s_x^2}\right)}\right).(\#eq:ci1)
\end{align}
- The $100(1-\alpha)\%$ CI for the *conditional response* $Y|X=x$ is 
\begin{align}
\left(\hat y \pm t_{n-2:\alpha/2}\sqrt{\hat\sigma^2+\frac{\hat\sigma^2}{n}\left(1+\frac{(x-\bar x)^2}{s_x^2}\right)}\right).(\#eq:ci2)
\end{align}

Notice the dependence of both CIs in $x$, $n$, and $\sigma^2$, each of them with a clear effect on the resulting length of the interval. Figure \@ref(fig:cipred) helps visualize these concepts and the difference between CIs interactively.

(ref:cipredtitle) Illustration of the CIs for the conditional mean and response. Note how the width of the CIs is influenced by $x$, especially for the conditional mean (the conditional response has a constant term affecting the width). Application also available [here](https://ec2-35-177-34-200.eu-west-2.compute.amazonaws.com/ci-prediction/).

```{r, cipred, echo = FALSE, fig.cap = '(ref:cipredtitle)', screenshot.alt = "images/screenshots/ci-prediction.png", dev = 'png', cache = TRUE, fig.pos = 'h!', out.width = '90%'}
knitr::include_app('https://ec2-35-177-34-200.eu-west-2.compute.amazonaws.com/ci-prediction/', height = '1000px')
```

### Case study application {#lm-i-prediction-case}

The prediction and computation of prediction CIs can be done with `predict`. The objects required for `predict` are: first, an `lm` object; second, a `data.frame` containing the locations $\mathbf{x}=(x_1,\ldots,x_p)$ where we want to predict $\beta_0+\beta_1x_1+\ldots+\beta_{p}x_p$. The prediction is $\hat\beta_0+\hat\beta_1x_1+\ldots+\hat\beta_{p}x_p$ and the CIs returned are either \@ref(eq:ci1) or \@ref(eq:ci2).

```{block, caunamespred, type = 'rmdcaution', cache = TRUE}
It is mandatory to name the columns of the data frame with the same names of the predictors used in `lm`. Otherwise `predict` will generate an error, see below.
```

```{r, case1-6, echo = TRUE, error = TRUE, collapse = TRUE, cache = TRUE}
# Fit a linear model for the price on WinterRain, HarvestRain, and AGST
modWine4 <- lm(Price ~ WinterRain + HarvestRain + AGST, data = wine)
summary(modWine4)

# Data for which we want a prediction
# Important! You have to name the column with the predictor name!
weather <- data.frame(WinterRain = 500, HarvestRain = 123,
                      AGST = 18)
weatherBad <- data.frame(500, 123, 18)

# Prediction of the mean

# Prediction of the mean at 95% - the defaults
predict(modWine4, newdata = weather)
predict(modWine4, newdata = weatherBad) # Error

# Prediction of the mean with 95% confidence interval (the default)
# CI: (lwr, upr)
predict(modWine4, newdata = weather, interval = "confidence")
predict(modWine4, newdata = weather, interval = "confidence", level = 0.95)

# Other levels
predict(modWine4, newdata = weather, interval = "confidence", level = 0.90)
predict(modWine4, newdata = weather, interval = "confidence", level = 0.99)

# Prediction of the response

# Prediction of the mean at 95% - the defaults
predict(modWine4, newdata = weather)

# Prediction of the response with 95% confidence interval
# CI: (lwr, upr)
predict(modWine4, newdata = weather, interval = "prediction")
predict(modWine4, newdata = weather, interval = "prediction", level = 0.95)

# Other levels
predict(modWine4, newdata = weather, interval = "prediction", level = 0.90)
predict(modWine4, newdata = weather, interval = "prediction", level = 0.99)

# Predictions for several values
weather2 <- data.frame(WinterRain = c(500, 200), HarvestRain = c(123, 200),
                       AGST = c(17, 18))
predict(modWine4, newdata = weather2, interval = "prediction")
```

```{block2, exwine, type = 'rmdexercise', cache = TRUE}
For the `wine` dataset, do the following:

- Regress `WinterRain` on `HarvestRain` and `AGST`. Name the fitted model `modExercise`.
- Compute the estimate for the conditional mean of `WinterRain` for `HarvestRain`$=123.0$ and `AGST`$=16.15$. What is the CI at $\alpha=0.01$?
- Compute the estimate for the conditional response for `HarvestRain`$=125.0$ and `AGST`$=15$. What is the CI at $\alpha=0.10$?
- Check that `modExercise$fitted.values` is the same as `predict(modExercise, newdata = data.frame(HarvestRain = wine$HarvestRain, AGST = wine$AGST))`. Why is this so?

```

## ANOVA {#lm-i-anova}

The variance of the error, $\sigma^2$, plays a fundamental role in the inference for the model coefficients and prediction. In this section we will see how the variance of $Y$ is decomposed into two parts, each one corresponding to the regression and to the error, respectively. This decomposition is called the *ANalysis Of VAriance* (ANOVA).

An important fact to highlight prior to introducing the ANOVA decomposition is that $\bar Y=\bar{\hat{Y}}$. This is an important result that can be checked if we use matrix notation. The ANOVA decomposition considers the following measures of variation related with the response:

- $\text{SST}:=\sum_{i=1}^n\left(Y_i-\bar Y\right)^2$, the **total sum of squares**. This is the *total variation* of $Y_1,\ldots,Y_n$, since $\text{SST}=ns_y^2$, where $s_y^2$ is the sample variance of $Y_1,\ldots,Y_n$.
- $\text{SSR}:=\sum_{i=1}^n\left(\hat Y_i-\bar Y\right)^2$, the **regression sum of squares**. This is the variation explained by the regression plane, that is, *the variation from $\bar Y$ that is explained by the estimated conditional mean $\hat Y_i=\hat\beta_0+\hat\beta_1X_{i1}+\ldots+\hat\beta_pX_{ip}$*. $\text{SSR}=ns_{\hat y}^2$, where $s_{\hat y}^2$ is the sample variance of $\hat Y_1,\ldots,\hat Y_n$.
- $\text{SSE}:=\sum_{i=1}^n\left(Y_i-\hat Y_i\right)^2$, the **sum of squared errors**^[SSE and RSS are two names for the same quantity (that appears in different contexts): $\text{SSE}=\sum_{i=1}^n\left(Y_i-\hat Y_i\right)^2=\sum_{i=1}^n\left(Y_i-\hat \beta_0-\hat \beta_1X_{i1}-\ldots-\hat \beta_pX_{ip}\right)^2=\mathrm{RSS}(\hat{\boldsymbol{\beta}})$.]. Is the variation around the conditional mean. Recall that $\text{SSE}=\sum_{i=1}^n \hat\varepsilon_i^2=(n-p-1)\hat\sigma^2$, where $\hat\sigma^2$ is the sample variance of $\hat \varepsilon_1,\ldots,\hat \varepsilon_n$.

The ANOVA decomposition is:
\begin{align}
\underbrace{\text{SST}}_{\text{Variation of }Y_i's} = \underbrace{\text{SSR}}_{\text{Variation of }\hat Y_i's} + \underbrace{\text{SSE}}_{\text{Variation of }\hat \varepsilon_i's} (\#eq:anovamult)
\end{align}
or, equivalently (dividing by $n$ in \@ref(eq:anovamult)),
\begin{align*}
\underbrace{s_y^2}_{\text{Variance of }Y_i's} = \underbrace{s_{\hat y}^2}_{\text{Variance of }\hat Y_i's} + \underbrace{(n-p-1)/n\times\hat\sigma^2}_{\text{Variance of }\hat\varepsilon_i's}.
\end{align*}
The graphical interpretation of \@ref(eq:anovamult) when $p=1$ is shown in Figures \@ref(fig:anova) and \@ref(fig:anovaillus).

(ref:anovatitle) Visualization of the ANOVA decomposition. SST measures the variation of $Y_1,\ldots,Y_n$ with respect to $\bar Y$. SST measures the variation with respect to the conditional means, $\hat \beta_0+\hat\beta_1X_i$. SSE collects the variation of the residuals.

```{r, anova, echo = FALSE, out.width = '70%', fig.pos = 'h!', fig.cap = '(ref:anovatitle)', fig.show = 'hold', cache = TRUE}
knitr::include_graphics("images/R/anova.png")
```

(ref:anovaillustitle) Illustration of the ANOVA decomposition and its dependence on $\sigma^2$ and $\hat\sigma^2$. Application also available [here](https://ec2-35-177-34-200.eu-west-2.compute.amazonaws.com/anova/).

```{r, anovaillus, echo = FALSE, fig.cap = '(ref:anovaillustitle)', screenshot.alt = "images/screenshots/anova.png", dev = 'png', cache = TRUE, fig.pos = 'h!', out.width = '90%'}
knitr::include_app('https://ec2-35-177-34-200.eu-west-2.compute.amazonaws.com/anova/', height = '800px')
```

The ANOVA table summarizes the decomposition of the variance:

|     | Degrees of freedom | Sum Squares | Mean Squares | $F$-value | $p$-value |
|-----|------------|--------|---------|----------------|----------------|
| Predictors | $p$ | SSR | $\frac{\text{SSR}}{p}$ | $\frac{\text{SSR}/p}{\text{SSE}/(n-p-1)}$ | $p$ |
| Residuals | $n-p-1$ | SSE | $\frac{\text{SSE}}{n-p-1}$ | | |

The *$F$-value* of the ANOVA table represents the value of the $F$-statistic $\frac{\text{SSR}/p}{\text{SSE}/(n-p-1)}$. This statistic is employed to test
\begin{align*}
H_0:\beta_1=\ldots=\beta_p=0\quad\text{vs.}\quad H_1:\beta_j\neq 0\text{ for any }j,
\end{align*}
that is, the hypothesis of *no linear dependence of $Y$ on $X_1,\ldots,X_p$* (the plane is completely flat, with no inclination). This is the so-called *$F$-test* and, if $H_0$ is rejected, allows to conclude that **at least one $\beta_j$ is significantly different from zero**. It happens that
\begin{align*}
F=\frac{\text{SSR}/p}{\text{SSE}/(n-p-1)}\stackrel{H_0}{\sim} F_{p,n-p-1},
\end{align*}
where $F_{p,n-p-1}$ represents the *Snedecor's $F$ distribution* with $p$ and $n-p-1$ degrees of freedom. If $H_0$ is true, then $F$ is expected to be *small* since SSR will be close to zero (little variation is explained by the regression model since $\hat{\boldsymbol{\beta}}\approx\mathbf{0}$).

```{block, cauanova, type = 'rmdcaution', cache = TRUE}
The "ANOVA table" is a broad concept in statistics, with different variants. Here we are only covering the basic ANOVA table from the relation $\text{SST} = \text{SSR} + \text{SSE}$. However, further sophistications are possible when $\text{SSR}$ is decomposed into the variations contributed by *each* predictor. In particular, for multiple linear regression `R`'s `anova` implements a *sequential (type I) ANOVA table*, which is **not** the previous table!
```

The `anova` function in `R` takes a model as an input and returns the following *sequential* ANOVA table^[More complex -- included here just for clarification of the `anova`'s output.]:

|              | Degrees of freedom | Sum Squares | Mean Squares | $F$-value |$p$-value|
|--------------|------------|--------|---------|----------------|-------------|
| Predictor 1| $1$ | SSR$_1$ | $\frac{\text{SSR}_1}{1}$ | $\frac{\text{SSR}_1/1}{\text{SSE}/(n-p-1)}$ | $p_1$ |
| Predictor 2| $1$ | SSR$_2$ | $\frac{\text{SSR}_2}{1}$ | $\frac{\text{SSR}_2/1}{\text{SSE}/(n-p-1)}$ | $p_2$ |
| $\vdots$| $\vdots$ | $\vdots$ | $\vdots$ | $\vdots$ | $\vdots$ |
| Predictor $p$| $1$ | SSR$_p$ | $\frac{\text{SSR}_p}{1}$ | $\frac{\text{SSR}_p/1}{\text{SSE}/(n-p-1)}$ | $p_p$ |
| Residuals | $n - p - 1$ | SSE | $\frac{\text{SSE}}{n-p-1}$ | | |

Here the SSR$_j$ represents the regression sum of squares associated to the inclusion of $X_j$ in the model with predictors $X_1,\ldots,X_{j-1}$, this is:
\[
\text{SSR}_j=\text{SSR}(X_1,\ldots,X_j)-\text{SSR}(X_1,\ldots,X_{j-1}).
\]
The $p$-values $p_1,\ldots,p_p$ correspond to the testing of the hypotheses
\begin{align*}
H_0:\beta_j=0\quad\text{vs.}\quad H_1:\beta_j\neq 0,
\end{align*}
carried out *inside the linear model* $Y=\beta_0+\beta_1X_1+\ldots+\beta_jX_j+\varepsilon$. This is like the $t$-test for $\beta_j$ for the model with predictors $X_1,\ldots,X_j$. Recall that there is no $F$-test in this version of the ANOVA table.

In order to compute the simplified ANOVA table, we need to rely on an ad-hoc *function*. The function takes as input a fitted `lm`:

```{r, simpleAnova, cache = TRUE}
# This function computes the simplied anova from a linear model
simpleAnova <- function(object, ...) {

  # Compute anova table
  tab <- anova(object, ...)

  # Obtain number of predictors
  p <- nrow(tab) - 1

  # Add predictors row
  predictorsRow <- colSums(tab[1:p, 1:2])
  predictorsRow <- c(predictorsRow, predictorsRow[2] / predictorsRow[1])

  # F-quantities
  Fval <- predictorsRow[3] / tab[p + 1, 3]
  pval <- pf(Fval, df1 = p, df2 = tab$Df[p + 1], lower.tail = FALSE)
  predictorsRow <- c(predictorsRow, Fval, pval)

  # Simplified table
  tab <- rbind(predictorsRow, tab[p + 1, ])
  row.names(tab)[1] <- "Predictors"
  return(tab)

}
```

### Case study application {#lm-i-anova-case}

Let's compute the ANOVA decomposition of `modWine1` and `modWine2` to test the existence of linear dependence. 

```{r, case1-7, collapse = TRUE, cache = TRUE}
# Models
modWine1 <- lm(Price ~ ., data = wine)
modWine2 <- lm(Price ~ . - FrancePop, data = wine)

# Simplified table
simpleAnova(modWine1)
simpleAnova(modWine2)
# The null hypothesis of no linear dependence is emphatically rejected in 
# both models

# R's ANOVA table - warning this is now what we saw in lessons
anova(modWine1)
```

```{block, exanova, type = 'rmdexercise', cache = TRUE}
Compute the ANOVA table for the regression `Price ~ WinterRain + AGST + HarvestRain + Age` in the `wine` dataset. Check that the $p$-value for the $F$-test given in `summary` and by `simpleAnova` are the same.
```

```{block, exassump-3, type = 'rmdexercise', cache = TRUE}
For the `y6 ~ x6` and `y7 ~ x7` in the `assumptions` dataset, compute their ANOVA tables. Check that the $p$-values of the $t$-test for $\beta_1$ and the $F$-test are the same (any explanation of why this is so?).
```

## Model fit {#lm-i-modfit}

### The $R^2$ {#lm-i-modfit-R2}

The *coefficient of determination* $R^2$ is closely related with the ANOVA decomposition. It is defined as
\begin{align*}
R^2:=\frac{\text{SSR}}{\text{SST}}=\frac{\text{SSR}}{\text{SSR}+\text{SSE}}=\frac{\text{SSR}}{\text{SSR}+(n-p-1)\hat\sigma^2}.
\end{align*}
$R^2$ measures the **proportion of variation** of the response variable $Y$ that is **explained** by the predictors $X_1,\ldots,X_p$ through the regression. Intuitively, $R^2$ measures the *tightness of the data cloud around the regression plane*. Check in Figure \@ref(fig:anovaillus) how changing the value of $\sigma^2$ (not $\hat\sigma^2$, but $\hat\sigma^2$ is obviously dependent on $\sigma^2$) affects the $R^2$. 

$R^2$ is intimately related with the sample correlation coefficient. For example, if $p=1$, then it can be seen (exercise below) that $R^2=r^2_{xy}$. More importantly, $R^2=r^2_{y\hat y}$ for any $p$, that is, *the square of the sample correlation coefficient between $Y_1,\ldots,Y_n$ and $\hat Y_1,\ldots,\hat Y_n$ is $R^2$*, a fact that is not immediately evident. Let's see check this fact when $p=1$ by relying on $R^2=r^2_{xy}$. First, by the form of $\hat\beta_0$ given in \@ref(eq:3),
\begin{align}
\hat Y_i&=\hat\beta_0+\hat\beta_1X_i\nonumber\\
&=(\bar Y-\hat\beta_1X_i)+\hat\beta_1X_i\nonumber\\
&=\bar Y+\hat\beta_1(X_i-\bar X). (\#eq:yhat)
\end{align}
Then, we replace \@ref(eq:yhat) in
\begin{align*}
r^2_{y\hat y}&=\frac{s_{y\hat y}^2}{s_y^2s_{\hat y}^2}\\
&=\frac{\left(\sum_{i=1}^n \left(Y_i-\bar Y \right)\left(\hat Y_i-\bar Y \right)\right)^2}{\sum_{i=1}^n \left(Y_i-\bar Y \right)^2\sum_{i=1}^n \left(\hat Y_i-\bar Y \right)^2}\\
&=\frac{\left(\sum_{i=1}^n \left(Y_i-\bar Y \right)\left(\bar Y+\hat\beta_1(X_i-\bar X)-\bar Y \right)\right)^2}{\sum_{i=1}^n \left(Y_i-\bar Y \right)^2\sum_{i=1}^n \left(\bar Y+\hat\beta_1(X_i-\bar X)-\bar Y \right)^2}\\
&=r^2_{xy}.
\end{align*}
and as a consequence $r^2_{y\hat y}=r^2_{xy}=R^2$ when $p=1$.

```{block2, insR2connection, type = 'rmdinsight', cache = TRUE}
Show that $R^2=r^2_{xy}$ when $p=1$. *Hint*: start from the definition of $R^2$ and use \@ref(eq:3) to arrive to $r^2_{xy}$.
```

Trusting the $R^2$ blindly can lead to catastrophic conclusions, since the model may not be correct. Here are a couple of counterexamples of a linear regression performed in a data that clearly does not satisfy the assumptions discussed in Section \@ref(lm-i-assumps), but despite that, still has a large $R^2$. As a consequence, **inference built on the validity of the assumptions** (which do not hold!) **will be problematic, no matter what is the value of $R^2$**. For example, recall how biased the predictions will be for $x=0.35$ and $x=0.65$!

```{r, R2bad-1, collapse = TRUE, cache = TRUE, echo = TRUE}
# Simple linear model

# Create data that:
# 1) does not follow a linear model
# 2) the error is heteroskedastic
x <- seq(0.15, 1, l = 100)
set.seed(123456)
eps <- rnorm(n = 100, sd = 0.25 * x^2)
y <- 1 - 2 * x * (1 + 0.25 * sin(4 * pi * x)) + eps

# Great R^2!?
reg <- lm(y ~ x)
summary(reg)

# scatterplot is a quick alternative to
# plot(x, y)
# abline(coef = reg$coef, col = 3)

# But prediction is obviously problematic
scatterplot(y ~ x, smooth = FALSE)

# Multiple linear model

# Create data that:
# 1) does not follow a linear model
# 2) the error is heteroskedastic
x1 <- seq(0.15, 1, l = 100)
set.seed(123456)
x2 <- runif(100, -3, 3)
eps <- rnorm(n = 100, sd = 0.25 * x1^2)
y <- 1 - 3 * x1 * (1 + 0.25 * sin(4 * pi * x1)) + 0.25 * cos(x2) + eps

# Great R^2!?
reg <- lm(y ~ x1 + x2)
summary(reg)
```

We can visualize the fit of the latter multiple linear model since we are in $p=2$.
```{r, R2bad-2, collapse = TRUE, webgl = knitr:::is_html_output(), cache = TRUE, eval = knitr:::is_html_output()}
# But prediction is obviously problematic
scatter3d(y ~ x1 + x2, fit = "linear")
```

```{block, insR2-1, type = 'rmdinsight', cache = TRUE}
A large $R^2$ means *nothing* in terms of inference if the **assumptions of the model do not hold**. $R^2$ is the proportion of variance of $Y$ explained by $X_1,\ldots,X_p$, but, of course, *only when the linear model is correct*.
```
```{block, insR2-2, type = 'rmdinsight', cache = TRUE}
Remember that:

- $R^2$ does not measure the correctness of a linear model but its **usefulness**, assuming the model is correct.
- $R^2$ is the proportion of variance of $Y$ explained by $X_1,\ldots,X_p$, but, of course, *only when the linear model is correct*.
```

We finalize by pointing out a nice connection between the $R^2$, the ANOVA decomposition and the least squares estimator $\hat{\boldsymbol{\beta}}$:

```{block, insR2RSS, type = 'rmdinsight', cache = TRUE}
The ANOVA decomposition gives another interpretation of the least-squares estimates: **$\hat{\boldsymbol{\beta}}$ are the estimated coefficients that maximize the $R^2$** (among all the possible estimates we could think about). To see this, recall that
\[
R^2=\frac{\text{SSR}}{\text{SST}}=\frac{\text{SST} - \text{SSE}}{\text{SST}}=\frac{\text{SST} - \text{RSS}(\hat{\boldsymbol{\beta}})}{\text{SST}},
\]
so if $\text{RSS}(\hat{\boldsymbol{\beta}})=\min_{\boldsymbol{\beta}\in\mathbb{R}^{p+1}}\text{RSS}(\boldsymbol{\beta})$, then $R^2$ is maximal for $\hat{\boldsymbol{\beta}}$!
```

### The $R^2_{\text{Adj}}$ {#lm-i-modfit-R2Adj}

As we saw, these are equivalent forms for $R^2$:
\begin{align}
R^2:=&\,\frac{\text{SSR}}{\text{SST}}=\frac{\text{SST}-\text{SSE}}{\text{SST}}=1-\frac{\text{SSE}}{\text{SST}}\nonumber\\
=&\,1-\frac{\hat\sigma^2}{\text{SST}}\times(n-p-1).(\#eq:R2)
\end{align}
The SSE on the numerator always decreases as more predictors are added to the model, even if these are not significant. As a consequence, the **$R^2$ always increases with $p$**. Why is this so? Intuitively, because the complexity -- hence the flexibility -- of the model increases when we use more predictors to explain $Y$. Mathematically, because when $p$ approaches $n-1$ the second term in \@ref(eq:R2) is reduced and, as a consequence, $R^2$ grows.

The *adjusted $R^2$* is an important quantity specifically designed to cover this $R^2$'s flaw, which is ubiquitous in multiple linear regression. The purpose is to have a better tool for **comparing models without systematically favoring complexer models**. This alternative coefficient is defined as
\begin{align}
R^2_{\text{Adj}}&=1-\frac{\text{SSE}/(n-p-1)}{\text{SST}/(n-1)}=1-\frac{\text{SSE}}{\text{SST}}\times\frac{n-1}{n-p-1}\nonumber\\
&=1-\frac{\hat\sigma^2}{\text{SST}}\times (n-1).(\#eq:R2A)
\end{align}
The $R^2_{\text{Adj}}$ is independent of $p$, at least explicitly. If $p=1$ then $R^2_{\text{Adj}}$ is *almost* $R^2$ (practically identical if $n$ is large). Both \@ref(eq:R2) and \@ref(eq:R2A) are quite similar except for the last factor, which in the former does not depend on $p$. Therefore, \@ref(eq:R2A) will only increase if $\hat\sigma^2$ is reduced with $p$ -- in other words, if the new variables contribute in the reduction of variability around the regression plane.

The different behavior between $R^2$ and $R^2_\text{Adj}$ can be visualized by a small simulation. Suppose that we generate a random dataset, with $n=200$ observations of a response $Y$ and two predictors $X_1,X_2$. This is, the sample $\{(X_{i1},X_{i2},Y_i)\}_{i=1}^n$ with
\[
Y_i=\beta_0+\beta_1X_{i1}+\beta_2X_{i2}+\varepsilon_i,\quad \varepsilon_i\sim\mathcal{N}(0,1).
\]
To this data, we add $196$ *garbage* predictors that are completely independent from $Y$. Therefore, we end up with $p=198$ predictors. Now we compute the $R^2(j)$ and $R^2_\text{Adj}(j)$ for the models
\[
Y=\beta_0+\beta_1X_{1}+\ldots+\beta_jX_{j}+\varepsilon,
\]
with $j=1,\ldots,p$ and we plot them as the curves $(j,R^2(j))$ and $(j,R_\text{Adj}^2(j))$. Since **$R^2$ and $R^2_\text{Adj}$ are random variables**, we repeat the procedure $100$ times to have a measure of the variability.

(ref:R2title) Comparison of $R^2$ and $R^2_{\text{Adj}}$ for $n=200$ and $p$ ranging from $1$ to $198$. $M=100$ datasets were simulated with **only the first two** predictors being significant. The thicker curves are the mean of each color's curves.

Figure \@ref(fig:R2) contains the results of this experiment. As you can see $R^2$ increases linearly with the number of predictors considered, although only the first two ones were important! On the contrary, $R^2_\text{Adj}$ only increases in the first two variables and then is flat on average, but it has a huge variability when $p$ approaches $n-2$. This is a consequence of the explosive variance of $\hat\sigma^2$ in that *degenerate case*. The experiment demonstrates that **$R^2_\text{Adj}$ is more adequate than the $R^2$ for evaluating the fit of a multiple linear regression**.

```{r, R2, echo = FALSE, out.width = '70%', fig.pos = 'h!', fig.cap = '(ref:R2title)', fig.show = 'hold', cache = TRUE}
knitr::include_graphics("images/R/R2vsAdjR2.png")
```

An example of a simulated dataset considered in the experiment of Figure \@ref(fig:R2):
```{r, R2bad-3, eval = FALSE, cache = TRUE}
# Generate data
p <- 198
n <- 200
set.seed(3456732)
beta <- c(0.5, -0.5, rep(0, p - 2))
X <- matrix(rnorm(n * p), nrow = n, ncol = p)
Y <- drop(X %*% beta + rnorm(n, sd = 3))
data <- data.frame(y = Y, x = X)

# Regression on the two meaningful predictors
summary(lm(y ~ x.1 + x.2, data = data))

# Adding 20 garbage variables
# R^2 increases and adjusted R^2 decreases
summary(lm(y ~ X[, 1:22], data = data))
```

```{block, cauR2Adj, type = 'rmdcaution', cache = TRUE}
The $R^2_\text{Adj}$ no longer measures the proportion of variation of $Y$ explained by the regression, but the result of *correcting this proportion by the number of predictors employed*. As a consequence of this, $R^2_\text{Adj}\leq1$ but **it can be negative!**
```

The next code illustrates a situation where we have two predictors completely independent from the response. The fitted model has a negative $R^2_\text{Adj}$.
```{r, R2Adjneg, collapse = TRUE, cache = TRUE}
# Three independent variables
set.seed(234599)
x1 <- rnorm(100)
x2 <- rnorm(100)
y <- 1 + rnorm(100)

# Negative adjusted R^2
summary(lm(y ~ x1 + x2))
```

```{block, expreds, type = 'rmdexercise', cache = TRUE}
For the previous example, construct more predictors (`x3`, `x4`, ...) that are independent from `y`  and check that when the predictors are added to the model, the $R^2_\text{Adj}$ decreases and the $R^2$ increases.
```

### Case study application {#lm-i-modfit-case}

Coming back to the case study, we have studied so far three models:
```{r, case1-8, collapse = TRUE, cache = TRUE, echo = TRUE}
# Fit models
modWine1 <- lm(Price ~ ., data = wine)
modWine2 <- lm(Price ~ . - FrancePop, data = wine)
modWine3 <- lm(Price ~ Age + WinterRain, data = wine)

# Summaries
summary(modWine1)
summary(modWine2)
summary(modWine3)
```

`modWine2` is the model with the largest $R^2_\text{Adj}$. It is a model that explains the $82.75\%$ of the variability in a non-redundant way and with all its coefficients significant. Therefore, we have a formula for effectively explaining and predicting the quality of a vintage (answers Q1). Prediction and, importantly, quantification of the uncertainty in the prediction, can be done with `predict`.

Furthermore, the interpretation of `modWine2` agrees with well-known facts in viticulture that make perfect sense (Q2):

- Higher temperatures are associated with better quality (higher priced) wine.
- Rain before the growing season is good for the wine quality, but during harvest is bad.
- The quality of the wine improves with the age.

Although these were known facts, keep in mind that the this model allows to *quantify the effect of each variable on the wine quality* and provides us with a precise way of *predicting the quality of future vintages*.
