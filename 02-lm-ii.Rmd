
# Linear models II: model selection, extensions, and diagnostics {#lm-ii}

## Case study: Housing values in Boston {#lm-lab-ii-boston}

This case study is motivated by @Harrison1978, who proposed an *hedonic model* for determining the willingness of house buyers to pay for clean air. An hedonic model is a model that decomposes the price of an item into separate components that determine its price. For example, an hedonic model for the price of a house may decompose its price into the house characteristics, the kind of neighborhood, and the location. The study of @Harrison1978 employed data from the Boston metropolitan area, containing 560 suburbs and 14 variables. The `Boston` dataset is available through the file `Boston.xlsx` file ([download](https://raw.githubusercontent.com/egarpor/PM-UC3M/master/datasets/Boston.xlsx)) and through the dataset `Boston` in the `MASS` package.

The description of the related variables can be found in `?Boston` and @Harrison1978^[But be aware of the changes in units for `medv`, `black`, `lstat`, and `nox`.], but we summarize here the most important ones as they appear in `Boston`. They are aggregated into five topics:

- *Dependent* variable: `medv`, the median value of owner-occupied homes (in thousands of dollars).
- *Structural* variables indicating the house characteristics: `rm` (average number of rooms "in owner units") and `age` (proportion of owner-occupied units built prior to 1940).
- *Neighborhood* variables: `crim` (crime rate), `zn` (proportion of residential areas), `indus` (proportion of non-retail business area), `chas` (river limitation), `tax` (cost of public services in each community), `ptratio` (pupil-teacher ratio), `black` (variable $1000(B - 0.63)^2$, where $B$ is the black proportion of population -- low and high values of $B$ increase housing prices) and `lstat` (percent of lower status of the population).
- *Accesibility* variables: `dis` (distances to five Boston employment centers) and `rad` (accessibility to radial highways -- larger index denotes better accessibility).
- *Air pollution* variable: `nox`, the annual concentration of nitrogen oxide (in parts per ten million).

We begin by importing the data:
```{r, echo = FALSE, collapse = TRUE, cache = TRUE}
library(readxl)
Boston <- read_excel(path = "datasets/Boston.xlsx", sheet = 1, col_names = TRUE)
```
```{r, echo = TRUE, collapse = TRUE, cache = TRUE, eval = FALSE}
# Read data
library(readxl)
Boston <- read_excel(path = "Boston.xlsx", sheet = 1, col_names = TRUE)
```
A summary of the data is shown below:
```{r, echo = TRUE, collapse = TRUE, cache = TRUE}
summary(Boston)
```

The two goals of this case study are:

- Q1. *Quantify the influence of the predictor variables in the housing prices.*
- Q2. *Obtain the "best possible" model for decomposing the housing prices and interpret it.*

We begin by making an exploratory analysis of the data with a matrix scatterplot. Since the number of variables is high, we opt to plot only five variables: `crim`, `dis`, `medv`, `nox`, and `rm`. Each of them represents the five topics in which variables were classified.

(ref:scat2title) Scatterplot matrix for `crim`, `dis`, `medv`, `nox`, and `rm` from the `Boston` dataset.

```{r, scat2, collapse = TRUE, out.width = '70%', fig.asp = 1, fig.pos = 'h!', fig.cap = '(ref:scat2title)', cache = TRUE}
scatterplotMatrix(~ crim + dis + medv + nox + rm, reg.line = lm, smooth = FALSE,
                  spread = FALSE, span = 0.5, ellipse = FALSE, levels = c(.5, .9),
                  id.n = 0, diagonal = 'density', data = Boston)
```
The diagonal panels are showing an estimate of the unknown density of each variable. Note the peculiar distribution of `crim`, very concentrated at zero, and the asymmetry in `medv`, with a second mode associated to the most expensive properties. Inspecting the individual panels, it is clear that some nonlinearity exists in the data and that some predictors are going to be more important than others (recall that we have plotted just a subset of all the predictors).

## Model selection {#lm-ii-modsel}

In Chapter \@ref(lm-i) we briefly saw that **the inclusion of more predictors is not for free**: there is a price to pay in terms of more variability on the coefficients estimates, harder interpretation, and possible inclusion of highly-dependent predictors. Indeed, there is a **maximum number of predictors $p$** that can be considered in a linear model for a sample size $n$: **$p\leq n-2$**. Or equivalently, there is a **minimum sample size $n$** required for fitting a model with $p$ predictors: **$n\geq k + 2$**.

The interpretation of this fact is simple if we think on the geometry for $p=1$ and $p=2$:

- If $p=1$, we need at least $n=2$ points to fit uniquely a line. However, this line gives no information on the vertical variation around it and hence $\hat\sigma^2$ can not be estimated (applying its formula, we would have $\hat\sigma^2=\infty$). Therefore we need at least $n=3$ points, or in other words $n\geq k + 2=3$.
- If $p=2$, we need at least $n=3$ points to fit uniquely a plane. But this plane gives no information on the variation of the data around it and hence $\hat\sigma^2$ can not be estimated. Therefore we need $n\geq k + 2=4$.

Another interpretation is the following:

> The fitting of a linear model with $p$ predictors involves the estimation the $p+2$ parameters $(\boldsymbol{\beta},\sigma^2)$ from $n$ data points. The closer $p+2$ and $n$ are, the more variable the estimates $(\hat{\boldsymbol{\beta}},\hat\sigma^2)$ will be, since less information is available for estimating each one. In the limit case $n=p+2$, each sample point determines a parameter estimate.

The *degrees of freedom* $n-p-1$ quantify the increasing on the variability of $(\hat{\boldsymbol{\beta}},\hat\sigma^2)$ when $n-p-1$ decreases. For example:

- $t_{n-p-1;\alpha/2}$ appears in \@ref(eq:normp2) and influences the length of the CIs for $\beta_j$, see \@ref(eq:cip). It also influences the length of the CIs for the prediction. As Figure \@ref(fig:dft) shows, when the degrees of freedom $n-p-1$ decrease, $t_{n-p-1;\alpha/2}$ increases, thus the intervals become wider.
- $\hat\sigma^2=\frac{1}{n-p-1}\sum_{i=1}^n\hat\varepsilon_i^2$ influences the $R^2$ and $R^2_\text{Adj}$. If no relevant variables are added to the model then $\sum_{i=1}^n\hat\varepsilon_i^2$ will not change substantially. However, the reducing factor $\frac{1}{n-p-1}$ will decrease as $p$ augments, inflating $\hat\sigma^2$ and its variance. This is exactly what happened in Figure \@ref(fig:R2).

```{r, dft, echo = FALSE, warning = FALSE, results = 'hide', out.width = '70%', fig.show = 'hold', fig.asp = 1, fig.pos = 'h!', fig.cap = 'Effect of $\\text{df}=n-p-1$ in $t_{\\text{df};\\alpha/2}$ for $\\alpha=0.10,0.05,0.01$.', cache = TRUE}
df <- 1:30
alpha <- 0.10
plot(df, qt(p = alpha/2, df = df, lower.tail = FALSE), type = "o",
     xlab = expression(df), ylab = expression(t[df * ";" * alpha/2]),
     ylim = c(0, 10), col = rainbow(3)[1], pch = 16)
alpha <- 0.05
lines(df, qt(p = alpha/2, df = df, lower.tail = FALSE), col = rainbow(3)[2],
      type = "o", pch = 16)
alpha <- 0.01
lines(df, qt(p = alpha/2, df = df, lower.tail = FALSE), col = rainbow(3)[3],
      type = "o", pch = 16)
legend("topright", lwd = 2, col = rainbow(3),
       legend = expression(alpha == 0.10, alpha == 0.05, alpha == 0.01))
```

Now that we have added more light into the problem of having an excess of predictors, we turn the focus into **selecting the most adequate predictors for a multiple regression model**. This is a challenging task without a unique solution, and what is worse, without a method that is guaranteed to work in all the cases. However, there is a well-established procedure that usually gives good results: the **stepwise model selection**. Its principle is to compare multiple linear regression models with different predictors (and, of course, with the same responses).

Before introducing the method, we need to understand what is an **information criterion**. An information criterion balances the fitness of a model with the number of predictors employed. Hence, it determines objectively the best model as the one that *minimizes the information criterion*. Two common criteria are the *Bayesian Information Criterion* (BIC) and the *Akaike Information Criterion* (AIC). Both are based on a **balance between the model fitness and its complexity**:
\begin{align}
\text{BIC}(\text{model}) = \underbrace{-2\log\ell(\text{model})}_{\text{Model fitness}} + \underbrace{\text{npar(model)}\times\log n}_{\text{Complexity}}, (\#eq:bic)
\end{align}
where $\ell(\text{model})$ is the *likelihood of the model* (how well the model fits the data) and $\text{npar(model)}$ is the number of parameters of the model, $p+2$ in the case of a multiple linear regression model with $p$ predictors. The AIC replaces $\log n$ by $2$ in \@ref(eq:bic), so it **penalizes less complexer models**. This is one of the reasons why BIC is preferred by some practitioners for model comparison. Also, because is *consistent* in selecting the true model: if enough data is provided, the BIC is guaranteed to select the data-generating model among a list of candidate models.

The BIC and AIC can be computed in `R` through the functions `BIC` and `AIC`. They take a model as the input.
```{r, collapse = TRUE, cache = TRUE}
# Two models with different predictors
mod1 <- lm(medv ~ age + crim, data = Boston)
mod2 <- lm(medv ~ age + crim + lstat, data = Boston)

# BICs
BIC(mod1)
BIC(mod2) # Smaller -> better

# AICs
AIC(mod1)
AIC(mod2) # Smaller -> better

# Check the summaries
summary(mod1)
summary(mod2)
```

Let's go back to the selection of predictors. If we have $p$ predictors, a naive procedure would be to check *all the possible* models that can be constructed with them and then select the best one in terms of BIC/AIC. This is the so-called *best subset selection*. The problem is that there are $2^{p+1}$ possible models! Fortunately, the `stepAIC` procedure helps us navigating this ocean of models by iteratively adding useful predictors and removing the non-important ones. The function takes as input a *model employing all the available predictors*. Let's see how it works with the already studied `wine` dataset.
```{r, echo = FALSE, collapse = TRUE, cache = TRUE}
wine <- read.csv(file = "datasets/wine.csv", header = TRUE)
```
```{r, collapse = TRUE, cache = TRUE, eval = FALSE}
# Load data - notice that "Year" is also included
wine <- read.csv(file = "wine.csv", header = TRUE)
```
`stepAIC` takes the argument `k` as $2$ (default) or $\log n$, where $n$ is the sample size. With `k = 2` it uses the AIC criterion and with `k = log(n)` it considers the BIC. 
```{r, collapse = TRUE, cache = TRUE}
# Full model
mod <- lm(Price ~ ., data = wine)

# With BIC
modBIC <- stepAIC(mod, k = log(nrow(wine)))
summary(modBIC)

# With AIC
modAIC <- stepAIC(mod, k = 2)
summary(modAIC)
```

Note that the selected models `modBIC` and `modAIC` are equivalent to the `modWine2` we selected in Section \@ref(lm-i-modfit-case) as the best model. This is an illustration that the model selected by `stepAIC` is often a good starting point for further additions or deletions of predictors. 

```{block, type = 'rmdtip'}
When applying `stepAIC` for BIC/AIC, different final models might be selected depending on the choice of `direction`. This is the interpretation:

- `"backward"`: starts from the full model, *removes* predictors sequentially.
- `"forward"`: starts from the simplest model, *adds* predictors sequentially.
- `"both"` (default): combination of the above.

The **advice** is to try several of these methods and retain the one with minimum BIC/AIC. Set `trace = 0` to omit lengthy outputs of information of the search procedure.
```

Further options with `stepAIC`:
```{r, collapse = TRUE, cache = TRUE}
# Different search directions and omitting the trace,
# gives only the final model
modAICFor <- stepAIC(mod, trace = 0, direction = "forward")
modAICBack <- stepAIC(mod, trace = 0, direction = "backward")
modAICFor
modAICBack
```

```{block, type = 'rmdcaution'}
`stepAIC` assumes that no `NA`'s (missing values) are present in the data. It is advised to remove the missing values in the data before. Their presence might lead to errors. To do so, employ `data = na.omit(dataset)` in the call to `lm` (if your dataset is `dataset`).
```

We conclude highlighting a caveat on the use of the BIC and AIC: they are constructed assuming that the sample size $n$ is much larger than the number of parameters in the model ($p+2$). Therefore, they will work reasonably well if $n>>p+2$, but if this is not true they may favor unrealistic complex models. An illustration of this phenomena is Figure \@ref(fig:bic), which is the BIC/AIC version of Figure \@ref(fig:R2) for the experiment done in Section \@ref(lm-i-anova). The BIC and AIC curves tend to have local minimums close to $p=2$ and then increase. But when $p+2$ gets close to $n$, they quickly drop down. Note also how the BIC penalizes more the complexity than the AIC, which is more flat.

```{r, bic, echo = FALSE, out.width = '70%', fig.pos = 'h!', fig.cap = 'Comparison of BIC and AIC for $n=200$ and $p$ ranging from $1$ to $198$. $M=100$ datasets were simulated with **only the first two** predictors being significant. The thicker curves are the mean of each color\'s curves.', fig.show = 'hold', cache = TRUE}
knitr::include_graphics("images/R/BICandAIC.png")
```

### Case study application {#lm-ii-modsel-case}

We want to build a linear model for predicting and explaining `medv`. There are a good number of predictors and some of them might be of little use for predicting `medv`. However, there is no clear intuition of which predictors will yield better explanations of `medv` with the information at hand. Therefore, we can start by doing a linear model on *all* the predictors:
```{r, collapse = TRUE, cache = TRUE}
modHouse <- lm(medv ~ ., data = Boston)
summary(modHouse)
```
There are a couple of non-significant variables, but so far the model has an $R^2=0.74$ and the fitted coefficients are sensible with what it would be expected. For example, `crim`, `tax`, `ptratio`, and `nox` have negative effects on `medv`, while `rm`, `rad`, and `chas` have positive. However, the non-significant coefficients are not significantly the model, but only adding artificial noise and decreasing the overall accuracy of the coefficient estimates.

Let's polish a little bit the previous model. Instead of removing manually each non-significant variable to reduce the complexity, we employ `stepAIC` for selecting a candidate *best model*:
```{r, collapse = TRUE, cache = TRUE}
# Best models
modBIC <- stepAIC(modHouse, k = log(nrow(Boston)))
modAIC <- stepAIC(modHouse, trace = 0, k = 2)

# Comparison
compareCoefs(modBIC, modAIC)
summary(modBIC)

# Confidence intervals
confint(modBIC)
```
Note how the $R^2_\text{Adj}$ has slightly increased with respect to the full model and how all the predictors are significant. Note also that `modBIC` and `modAIC` are the same.

We have quantified the influence of the predictor variables in the housing prices (Q1) and we can conclude that, in the final model and with confidence level $\alpha=0.05$:

- `chas`, `age`, `rad`, and `black` have a **significantly positive** influence on `medv`.
- `nox`, `dis`, `tax`, `pratio`, and `lstat` have a **significantly negative** influence on `medv`.

## Use of qualitative predictors {#lm-ii-qualpred}

An important situation not covered so far is how to deal with *qualitative*, and not *quantitative*, predictors. Qualitative predictors, also known as *categorical* variables or, in `R`'s terminology, *factors*, are very common, for example in social sciences. Dealing with them requires some care and proper understanding of how these variables are represented.

The simplest case is the situation with **two levels**. A binary variable $C$ with two levels (for example, *a* and *b*) can be represented as
\[
D=\left\{\begin{array}{ll}1,&\text{if }C=b,\\0,& \text{if }C=a.\end{array}\right.
\]
$D$ now is a *dummy variable*: it codifies with zeros and ones the two possible levels of the categorical variable. An example of $C$ could be *gender*, which has levels *male* and *female*. The dummy variable associated is $D=0$ if the gender is male and $D=1$ if the gender is female.

The advantage of this *dummification* is its interpretability in regression models. Since level *a* corresponds to $0$, it can be seen as the *reference level* to which level *b* is compared. This is the key point in dummification: **set one level as the reference and codify the rest as departures from it** with ones.

The previous interpretation translates easily to the linear model. Assume that the dummy variable $D$ is available together with other predictors $X_1,\ldots,X_p$. Then:
\[
\mathbb{E}[Y|X_1=x_1,\ldots,X_p=x_p,D=d]=\beta_0+\beta_1X_1+\ldots+\beta_pX_p+\beta_{p+1}D.
\]
The coefficient associated to $D$ is easily interpretable. $\beta_{p+1}$ is the increment in mean of $Y$ associated to changing $D=0$ (reference) to $D=1$, while the rest of the predictors are fixed. Or in other words, $\beta_{p+1}$ is the increment in mean of $Y$ associated to changing of the level of the categorical variable from *a* to *b*.

`R` does the dummification automatically (translates a categorical variable $C$ into its dummy version $D$) if it detects that a factor variable is present in the regression model. 

Let's see now the case with **more than two levels**, for example, a categorical variable $C$ with levels *a*, *b*, and *c*. If we take *a* as the reference level, this variable can be represented by *two* dummy variables:
\[
D_1=\left\{\begin{array}{ll}1,&\text{if }C=b,\\0,& \text{if }C\neq b\end{array}\right.
\]
and
\[
D_2=\left\{\begin{array}{ll}1,&\text{if }C=c,\\0,& \text{if }C\neq c.\end{array}\right.
\]
Then $C=a$ is represented by $D_1=D_2=0$, $C=b$ is represented by $D_1=1,D_2=0$ and $C=c$ is represented by $D_1=0,D_2=1$. The interpretation of the regression models with the presence of $D_1$ and $D_2$ is the very similar to the one before. For example, for the linear model, the coefficient associated to $D_1$ gives the increment in mean of $Y$ when the category of $C$ changes from *a* to *b*. The coefficient for $D_2$ gives the increment in mean of $Y$ when it changes from *a* to *c*.

In general, if we have a categorical variable with $J$ levels, then the number of dummy variables required is $J-1$. Again, `R` does the dummification automatically for you if it detects that a factor variable is present in the regression model. Let's see an example with the `iris` dataset.

```{r, collapse = TRUE, cache = TRUE}
# Load the iris dataset - factors in the last column
data(iris)
summary(iris)

# Summary of a linear model
mod1 <- lm(Sepal.Length ~ ., data = iris)
summary(mod1)
# Speciesversicolor (D1) coefficient: -0.72356. The average increment of
# Sepal.Length when the species is versicolor instead of setosa (reference)
# Speciesvirginica (D2) coefficient: -1.02350. The average increment of
# Sepal.Length when the species is virginica instead of setosa (reference)
# Both dummy variables are significant

# How to set a different level as reference (versicolor)
iris$Species <- relevel(iris$Species, ref = "versicolor")

# Same estimates except for the dummy coefficients
mod2 <- lm(Sepal.Length ~ ., data = iris)
summary(mod2)
# Speciessetosa (D1) coefficient: 0.72356. The average increment of
# Sepal.Length when the species is setosa instead of versicolor (reference)
# Speciesvirginica (D2) coefficient: -0.29994.s The average increment of
# Sepal.Length when the species is virginica instead of versicolor (reference)
# Both dummy variables are significant

# Coefficients of the model
confint(mod2)
# The coefficients of Speciesversicolor and Speciesvirginica are significantly
# negative
```

```{block, type = 'rmdcaution'}
**Do not codify a categorical variable as a discrete variable**. This constitutes a major methodological fail that will flaw the subsequent statistical analysis.

For example if you have a categorical variable `party` with levels `partyA`, `partyB`, and `partyC`, do not encode it as a discrete variable taking the values `1`, `2`, and `3`, respectively. If you do so:

- You assume implicitly an order in the levels of `party`, since `partyA` is closer to `partyB` than to `partyC`.
- You assume implicitly that `partyC` is three times larger than `partyA`.
- The codification is completely arbitrary -- why not considering `1`, `1.5`, and `1.75` instead of?

The right way of dealing with categorical variables in regression is to set the variable as a factor and let `R` do internally the dummification.
```

### Case study application {#lm-ii-qualpred-case}

Let's see what are the dummy variables in the `Boston` dataset and what effect they have in `medv`.

```{r, collapse = TRUE, cache = TRUE}
# Load the Boston dataset
library(MASS)
data(Boston)

# Structure of the data
str(Boston)
# chas is a dummy variable measuring if the suburb is close to the river (1)
# or not (0). In this case it is not codified as a factor but as a 0 or 1
# (so it is already dummyfied)

# Summary of a linear model
mod <- lm(medv ~ chas + crim, data = Boston)
summary(mod)
# The coefficient associated to chas is 5.57772. That means that if the suburb
# is close to the river, the mean of medv increases in 5.57772 units for 
# the same house and neighborhood conditions
# chas is significant (the presence of the river adds a valuable information
# for explaining medv)

# Summary of the best model in terms of BIC
summary(modBIC)
# The coefficient associated to chas is 2.71871. If the suburb is close to 
# the river, the mean of medv increases in 2.71871 units
# chas is significant as well in the presence of more predictors
```

## Nonlinear relationships {#lm-ii-nonlin}

The linear model is termed *linear* not because the regression curve is a plane, but because *the effects of the **parameters** are linear*. Indeed, the predictor $X$ may exhibit a nonlinear effect on the response $Y$ and still be a linear model! For example, the following models can be transformed into simple linear models:

  1. $Y=\beta_0+\beta_1X^2+\varepsilon$
  2. $Y=\beta_0+\beta_1\log(X)+\varepsilon$
  3. $Y=\beta_0+\beta_1(X^3-\log(|X|) + 2^{X})+\varepsilon$

The trick is to work with the transformed predictors ($X^2$, $\log(X)$, ...), instead of with the original predictor $X$. Then, rather than working with the sample $(X_1,Y_1),\ldots,(X_n,Y_n)$, we consider the transformed sample $(\tilde X_1,Y_1),\ldots,(\tilde X_n,Y_n)$ with (for the above examples):

  1. $\tilde X_i=X_i^2$, $i=1,\ldots,n$.
  2. $\tilde X_i=\log(X_i)$, $i=1,\ldots,n$.
  3. $\tilde X_i=X_i^3-\log(|X_i|) + 2^{X_i}$, $i=1,\ldots,n$.

An example of this simple but powerful trick is given as follows. The left panel of Figure \@ref(fig:quadratic) shows the scatterplot for some data `y` and  `x`, together with its fitted regression line. Clearly, the data does not follow a linear pattern, but a nonlinear one, similar to a parabola $y=x^2$. Hence, `y` might be better explained by the *square* of `x`, `x^2`, rather than by `x`. Indeed, if we plot `y` against `x^2` in the right panel of Figure \@ref(fig:quadratic), we can see that the relation of `y` and `x^2` is now linear!

```{r, quadratic, echo = FALSE, out.width = '45%', fig.show = 'hold', fig.asp = 1, fig.pos = 'h!', fig.cap = 'Left: quadratic pattern when plotting $Y$ against $X$. Right: linearized pattern when plotting $Y$ against $X^2$. In red, the fitted regression line.', cache = TRUE}
set.seed(345607)
x <- round(seq(-2, 5, l = 50), 1)
y <- round(0.5 * x^2 + rnorm(50), 1)
mod1 <- lm(y ~ x)
plot(x, y, pch = 16)
abline(mod1$coefficients, col = 2, lwd = 2)
mod2 <- lm(y ~ I(x^2))
plot(x^2, y, xlab = "x^2", pch = 16)
abline(mod2$coefficients, col = 2, lwd = 2)
```

In conclusion, with a simple trick we have increased drastically the explanation of the response. However, there is a catch: knowing which transformation is required in order to linearise the relation between response and the predictor is a kind of art which requires some good eye. One first approach is to consider one of the usual transformations, which are displayed in Figure \@ref(fig:nonlineartransf), depending on the pattern of the data. Figure \@ref(fig:transf) illustrates how to choose an adequate transformation for linearizing nonlinear data patterns.

```{r, nonlineartransf, echo = FALSE, warning = FALSE, results = 'hide', out.width = '45%', fig.show = 'hold', fig.asp = 1, fig.pos = 'h!', fig.cap = 'Some common nonlinear transformations and their negative counterparts. Recall the domain of definition of each transformation.', cache = TRUE}
x <- seq(-2, 5, l = 200)
plot(x, x, xlab = "x", ylab = "y", type = "l", col = 1, lwd = 2)
lines(x, x^2, col = 2, lwd = 2)
lines(x, x^3, col = 3, lwd = 2)
lines(x, sqrt(x), col = 4, lwd = 2)
lines(x, exp(x), col = 5, lwd = 2)
lines(x, exp(-x), col = 6, lwd = 2)
lines(x, log(x), col = 7, lwd = 2)
legend("bottomright", legend = expression(y == x, y == x^2, y == x^3, y == sqrt(x),
                                          y == exp(x), y == exp(-x), y == log(x)),
       lwd = 2, col = 1:7)
plot(x, -x, xlab = "x", ylab = "y", type = "l", col = 1, lwd = 2)
lines(x, -x^2, col = 2, lwd = 2)
lines(x, -x^3, col = 3, lwd = 2)
lines(x, -sqrt(x), col = 4, lwd = 2)
lines(x, -exp(x), col = 5, lwd = 2)
lines(x, -exp(-x), col = 6, lwd = 2)
lines(x, -log(x), col = 7, lwd = 2)
legend("topright", legend = expression(y == -x, y == -x^2, y == -x^3, y == -sqrt(x),
                                       y == -exp(-x), y == -exp(x), y == -log(x)),
       lwd = 2, col = 1:7)
```

(ref:transftitle) Illustration of the choice of the nonlinear transformation. Application also available [here](https://ec2-35-177-34-200.eu-west-2.compute.amazonaws.com/non-linear/).

```{r, transf, echo = FALSE, fig.cap = '(ref:transftitle)', screenshot.alt = "images/screenshots/non-linear.png", dev = 'png', cache = TRUE, fig.pos = 'h!', out.width = '90%'}
knitr::include_app('https://ec2-35-177-34-200.eu-west-2.compute.amazonaws.com/non-linear/', height = '550px')
```

```{block2, type = 'rmdinsight'}
If you apply a nonlinear transformation, namely $f$, and fit the linear model $Y=\beta_0+\beta_1 f(X)+\varepsilon$, then there is no point in fit also the model resulting from the negative transformation $-f$. The model with $-f$ is exactly the same as the one with $f$ but with the sign of $\beta_1$ flipped!

As a rule of thumb, use Figure \@ref(fig:nonlineartransf) with the transformations to compare it with the data pattern, then choose the most similar curve, and finally apply the corresponding function with **positive sign**.
```

```{block2, type = 'rmdinsight'}
As you might have realized, applying nonlinear transformations to the predictors is a **simple trick that extends enormously the functionality of the linear model**. This is particularly useful in real applications, where linearity is hardly verified.
```

Let's see how we can compute transformations of our predictors and perform a linear regression with them. The data for Figure \@ref(fig:quadratic) is:
```{r, cache = TRUE}
# Data
x <- c(-2, -1.9, -1.7, -1.6, -1.4, -1.3, -1.1, -1, -0.9, -0.7, -0.6,
       -0.4, -0.3, -0.1, 0, 0.1, 0.3, 0.4, 0.6, 0.7, 0.9, 1, 1.1, 1.3,
       1.4, 1.6, 1.7, 1.9, 2, 2.1, 2.3, 2.4, 2.6, 2.7, 2.9, 3, 3.1,
       3.3, 3.4, 3.6, 3.7, 3.9, 4, 4.1, 4.3, 4.4, 4.6, 4.7, 4.9, 5)
y <- c(1.4, 0.4, 2.4, 1.7, 2.4, 0, 0.3, -1, 1.3, 0.2, -0.7, 1.2, -0.1,
       -1.2, -0.1, 1, -1.1, -0.9, 0.1, 0.8, 0, 1.7, 0.3, 0.8, 1.2, 1.1,
       2.5, 1.5, 2, 3.8, 2.4, 2.9, 2.7, 4.2, 5.8, 4.7, 5.3, 4.9, 5.1,
       6.3, 8.6, 8.1, 7.1, 7.9, 8.4, 9.2, 12, 10.5, 8.7, 13.5)

# Data frame (a matrix with column names)
nonLinear <- data.frame(x = x, y = y)

# We create a new column inside nonLinear, called x2, that contains the
# newvariable x^2
nonLinear$x2 <- nonLinear$x^2
# If you wish to remove it
# nonLinear$x2 <- NULL

# Regressions
mod1 <- lm(y ~ x, data = nonLinear)
mod2 <- lm(y ~ x2, data = nonLinear)
summary(mod1)
summary(mod2) 
# mod2 has a larger R^2. Also notice the intercept is not significative.
```

```{block, type = 'rmdtip'}
A fast way of performing and summarizing the quadratic fit is

    summary(lm(y ~ I(x^2), data = nonLinear))

The `I()` function wrapping `x^2` is fundamental when applying arithmetic operations in the predictor. The symbols `+`, `\*`, `^`, ... have **different meaning** when inputted in a formula, so is required to use `I()` to indicate that they must be interpreted in their arithmetic meaning and that the result of the expression denotes a new predictor variable. For example, use `I((x - 1)^3 - log(3 \* x))` if you want to apply the transformation `(x - 1)^3 - log(3 * x)`.

```

```{block2, type = 'rmdexercise'}
Load the dataset `assumptions.RData`. We are going to work with the regressions `y2 ~ x2`, `y3 ~ x3`, `y8 ~ x8`, and `y9 ~ x9`, in order to identify which transformation of Figure \@ref(fig:nonlineartransf) gives the best fit. (For the purpose of illustration, we do not care if the assumptions are respected.) For these, do the following:

- Find the transformation that yields the largest $R^2$.
- Compare the original and the transformed linear models.

Some hints:

- `y2 ~ x2` has a negative dependence, so look at the right panel of Figure \@ref(fig:transf).
- `y3 ~ x3` seems to have just a subtle nonlinearity... Will it be worth to attempt a transformation?
- For `y9 ~ x9`, try with also with `exp(-abs(x9))`, `log(abs(x9))`, and `2^abs(x9)`.

```

```{block2, type = 'rmdexercise'}

*Moore's law* [@Moore1965] is an empirical law that states that the power of a computer doubles approximately every two years. Translated into a mathematical formula, Moore's law is
\begin{align*}
\text{transistors}\approx 2^{\text{years}/2}.
\end{align*}
Applying logarithms to both sides gives
\begin{align*}
\log(\text{transistors})\approx \frac{\log(2)}{2}\text{years}.
\end{align*}
We can write the above formula more generally as
\begin{align*}
\log(\text{transistors})=\beta_0+\beta_1 \text{years}+\varepsilon,
\end{align*}
where $\varepsilon$ is a random error. This is a linear model!

The dataset `cpus.txt` ([download](https://raw.githubusercontent.com/egarpor/PM-UC3M/master/datasets/cpus.txt)) contains the transistor counts for the CPUs appeared in the time range 1971--2015. For this data, do the following:

- Import conveniently the data and name it as `cpus`.
- Show a scatterplot of `Transistor.count` vs `Date.of.introduction` with a linear regression.
- Are the assumptions verified in `Transistor.count ~ Date.of.introduction`? Which ones are which are more "problematic"?
- Create a new variable, named `Log.Transistor.count`, containing the logarithm of `Transistor.count`.
- Show a scatterplot of `Log.Transistor.count` vs `Date.of.introduction`  with a linear regression.
- Are the assumptions verified in `Log.Transistor.count ~ Date.of.introduction`? Which ones are which are more "problematic"?
- Regress `Log.Transistor.count ~ Date.of.introduction`.
- Summarize the fit. What are the estimates $\hat\beta_0$ and $\hat\beta_1$? Is $\hat\beta_1$ close to $\frac{\log(2)}{2}$?
- Compute the CI for $\beta_1$ at $\alpha=0.05$. Is $\frac{\log(2)}{2}$ inside it? What happens at levels $\alpha=0.10,0.01$?
- We want to forecast the average log-number of transistors for the CPUs to be released in 2017. Compute the adequate prediction and CI.
- A new CPU design is expected for 2017. What is the range of log-number of transistors expected for it, at a 95% level of confidence?
- Compute the ANOVA table for `Log.Transistor.count ~ Date.of.introduction`. Is $\beta_1$ significant?

```

```{block, type = 'rmdexercise'}
The dataset `gpus.txt` ([download](https://raw.githubusercontent.com/egarpor/PM-UC3M/master/datasets/gpus.txt)) contains the transistor counts for the GPUs appeared in the period 1997--2016. Repeat the previous analysis for this dataset.
```

<!-- Polynomials and interactions 
This is partially alleviated by the extension of this technique to deal with *polynomials* rather than *monomials*.
-->

### Case study application {#lm-ii-nonlin-case}

```{block2, type = 'rmdexercise'}
The model employed in @Harrison1978 is different from the `modBIC` model. In the paper, several nonlinear transformations of the predictors and the response are done to improve the linear fit. Also, different units are used for `medv`, `black`, `lstat`, and `nox`. The authors considered these variables:

- *Response*: `log(1000 * medv)`
- *Linear predictors*: `age`, `black / 1000` (this variable corresponds to their $(B-0.63)^2$), `tax`, `ptratio`, `crim`, `zn`, `indus` and `chas`.
- *Nonlinear predictors*: `rm^2`, `log(dis)`, `log(rad)`, `log(lstat / 100)`, and `(10 * nox)^2`.

Do the following:

1. Check if the model with such predictors corresponds to the one in the first column, Table VII, page 100 of @Harrison1978
(open-access paper available [here](https://deepblue.lib.umich.edu/bitstream/handle/2027.42/22636/0000186.pdf)). To do so, Save this model as `modelHarrison` and summarize it. **Hint**: the formula should be something like `I(log(1000 * medv)) ~ age + I(black / 1000) + ... + I(log(lstat / 100)) + I((10 * nox)^2)`.

2. Make a `stepAIC` selection of the variables in `modelHarrison` (use BIC) and save it as `modelHarrisonSel`. Summarize it.

3. Which model has a larger $R^2$? And adjusted $R^2$? Which is simpler and has more significant coefficients?

```
<!--
lm(I(log(medv*1000)) ~ I(rm^2) + age + log(dis) + log(rad) + tax + ptratio + I(black/1000) + I(log(lstat/100)) + crim + zn + indus + chas + I((10*nox)^2), data = Boston)
-->

<!-- ## Model diagnostics and multicollinearity {#lm-ii-diagnostics} -->

<!-- As we saw in Section \@ref(lm-i-assumps), checking the assumptions of the multiple linear model through the data scatterplots becomes tricky even when $p=2$. To solve this issue, a series of *diagnostic plots* have been designed in order to evaluate graphically and in a simple way the validity of the assumptions. For illustration, we retake the `wine` dataset ([download](https://raw.githubusercontent.com/egarpor/PM-UC3M/master/datasets/wine.csv)). -->

<!-- ```{r, collapse = TRUE, cache = TRUE} -->
<!-- mod <- lm(Price ~ Age + AGST + HarvestRain + WinterRain, data = wine) -->
<!-- ``` -->

<!-- We will focus only in three plots: -->

<!-- 1. **Residuals vs. fitted values plot**. This plot serves mainly to check the **linearity**, although lack of homoscedasticity or independence can also be detected. Here is an example: -->

<!--     ```{r, collapse = TRUE, cache = TRUE} -->
<!--     plot(mod, 1) -->
<!--     ``` -->

<!--     **Under linearity, we expect the red line** (a nonlinear fit of the mean of the residuals) **to be almost flat**. This which means that the trend of $Y_1,\ldots,Y_n$ is linear with respect to the predictors. Heteroskedasticity can be detected also in the form of irregular vertical dispersion around the red line. The dependence between residuals can be detected (harder) in the form of non randomly spread residuals. -->

<!-- 2. **QQ-plot**. Checks the **normality**: -->

<!--     ```{r, collapse = TRUE, cache = TRUE} -->
<!--     plot(mod, 2) -->
<!--     ``` -->

<!--     **Under normality, we expect the points** (sample quantiles of the standardized residuals vs. theoretical quantiles of a $\mathcal{N}(0,1)$) **to align with the diagonal line**, which represents the ideal position of the points if those were sampled from a $\mathcal{N}(0,1)$. It is usual to have larger departures from the diagonal in the extremes than in the center, even under normality, although these departures are more clear if the data is non-normal. -->

<!-- 3. **Scale-location plot**. Serves for checking the **homoscedasticity**. It is similar to the first diagnostic plot, but now with the residuals standardized and transformed by a square root (of the absolute value). This change transforms the task of spotting heteroskedasticity by looking into irregular vertical dispersion patterns into spotting for nonlinearities, which is somehow simpler. -->

<!--     ```{r, collapse = TRUE, cache = TRUE} -->
<!--     plot(mod, 3) -->
<!--     ``` -->

<!--     **Under homoscedasticity, we expect the red line to be almost flat.** If there are consistent nonlinear patterns, then there is evidence of heteroskedasticity. -->

<!-- ```{block, type = 'rmdtip'} -->
<!-- If you type `plot(mod)`, several diagnostic plots will be shown sequentially. In order to advance them, hit `'Enter'` in the `R` console. -->
<!-- ``` -->

<!-- The next figures present datasets where the assumptions are satisfied and violated. -->

<!-- ```{r, diagnostics1, echo = FALSE, out.width = '70%', fig.pos = 'h!', fig.cap = 'Residuals vs. fitted values plots for datasets respecting (left column) and violating (right column) the linearity assumption.', cache = TRUE} -->
<!-- knitr::include_graphics("images/R/diagnostics1.png") -->
<!-- ``` -->

<!-- ```{r, diagnostics2, echo = FALSE, out.width = '70%', fig.pos = 'h!', fig.cap = 'QQ-plots for datasets respecting (left column) and violating (right column) the normality assumption.', cache = TRUE} -->
<!-- knitr::include_graphics("images/R/diagnostics2.png") -->
<!-- ``` -->

<!-- ```{r, diagnostics3, echo = FALSE, out.width = '70%', fig.pos = 'h!', fig.cap = 'Scale-location plots for datasets respecting (left column) and violating (right column) the homoscedasticity assumption.', cache = TRUE} -->
<!-- knitr::include_graphics("images/R/diagnostics3.png") -->
<!-- ``` -->

<!-- ```{block, type = 'rmdexercise'} -->
<!-- Load the dataset `assumptions3D.RData`([download](https://raw.githubusercontent.com/egarpor/PM-UC3M/master/datasets/assumptions3D.RData)) and compute the regressions `y.3 ~ x1.3 + x2.3`, `y.4 ~ x1.4 + x2.4`, `y.5 ~ x1.5 + x2.5` and `y.8 ~ x1.8 + x2.8`. Use the three diagnostic plots to test the assumptions of the linear model. -->
<!-- ``` -->

<!-- A common problem that arises in multiple linear regression is **multicollinearity**. This is the situation when two or more predictors are highly linearly related between them. Multicollinearitiy has important effects on the fit of the model: -->

<!-- - It **reduces the precision of the estimates**. As a consequence, sings of fitted coefficients may be reversed and valuable predictors may appear as non significant. -->
<!-- - It is **difficult to determine how each of the highly related predictors affects the response**, since one masks the other. This may result in numerical instabilities. -->

<!-- Intuitively, multicollinearity can be thought as a plane that pins around a straight line (think about holding a card on its opposite corners and make it spinning on its axis of rotation). Animation: Insight into straight line in 3D -->

<!-- An approach is to detect multicollinearity is to compute the correlation matrix between the predictors by `cor` -->
<!-- ```{r, collapse = TRUE, cache = TRUE} -->
<!-- cor(wine) -->
<!-- ``` -->
<!-- Here we can see what we already knew from Section \@ref(lm-i-lab-wine), that `Age` and `Year` are perfectly linearly related and that `Age` and `FrancePop` are highly linearly related. -->

<!-- However, is not enough to inspect remove pair by pair correlations in order to get rid of multicollinearity. Here is a counterexample: -->
<!-- ```{r, collapse = TRUE, cache = TRUE} -->
<!-- # Create predictors with multicollinearity: x4 depends on the rest -->
<!-- set.seed(45678) -->
<!-- x1 <- rnorm(100) -->
<!-- x2 <- 0.5 * x1 + rnorm(100) -->
<!-- x3 <- 0.5 * x2 + rnorm(100) -->
<!-- x4 <- -x1 + x2 + rnorm(100, sd = 0.25) -->

<!-- # Response -->
<!-- y <- 1 + 0.5 * x1 + 2 * x2 - 3 * x3 - x4 + rnorm(100) -->
<!-- data <- data.frame(x1 = x1, x2 = x2, x3 = x3, x4 = x4, y = y) -->

<!-- # Correlations - none seems suspicious -->
<!-- cor(data) -->
<!-- ``` -->

<!-- A better approach is to compute the **Variance Inflation Factor** (VIF) of each coefficient $\hat\beta_j$. This is a *measure of how linearly dependent is $X_j$ with the rest of predictors*: -->
<!-- \[ -->
<!-- \text{VIF}(\hat\beta_j)=\frac{1}{1-R^2_{X_j|X_{-j}}} -->
<!-- \] -->
<!-- where $R^2_{X_j|X_{-j}}$ is the $R^2$ from a regression of $X_j$ into the remaining predictors. The next rule of thumb gives direct insight into which predictors are multicollinear: -->

<!-- - VIF close to 1: absence of multicollinearity. -->
<!-- - **VIF larger than 5 or 10: problematic amount of multicollinearity**. Advised to remove the predictor with largest VIF. -->

<!-- VIF is called by `vif` and takes as argument a linear model. We continue with the previous example. -->
<!-- ```{r, collapse = TRUE, cache = TRUE} -->
<!-- # Abnormal variance inflation factors: largest for x4, we remove it -->
<!-- modMultiCo <- lm(y ~ x1 + x2 + x3 + x4) -->
<!-- vif(modMultiCo) -->

<!-- # Without x4 -->
<!-- modClean <- lm(y ~ x1 + x2 + x3) -->

<!-- # Comparison -->
<!-- summary(modMultiCo) -->
<!-- summary(modClean) -->

<!-- # Variance inflation factors normal -->
<!-- vif(modClean) -->
<!-- ``` -->

<!-- ## Dimension reduction {#lm-ii-dimred} -->

<!-- ### Principal Components Regression {#lm-ii-dimred-pcr} -->

<!-- ### Partial Least Squares {#lm-ii-dimred-pls} -->

