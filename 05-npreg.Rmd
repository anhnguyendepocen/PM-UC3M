
# Nonparametric regression {#npreg}

The models we saw in the previous chapters share a common root: all of them are **parametric**. This means that they *assume* a certain structure on the regression function $m$, which is controlled by *parameters*. For example, generalized linear models assume that $m$ is of the form $m(\mathbf{x})=g^{-1}(\beta_0+\beta_1x_1+\ldots+\beta_px_p)$ for some unknown coefficients $\boldsymbol{\beta}$ that have to be estimated. *If* this assumption holds (*i.e.*, if $m$ truly has the assumed form), then parametric methods are the best approach for estimating $m$. As we have seen, in practice it is rarely the case where parametric methods work out-of-the-box, and several tricks are needed in order to expand their degree of flexibility in a case-by-case basis. This is the strongest point of **nonparametric** methods: they do not assume major hard-to-satisfy hypothesis on the regression function, but just minimal assumptions. Their weak points: they are more challenging to estimate and to interpret.

In this chapter we will focus mainly on the situation with only **one continuous predictor** $X$ for predicting the response $Y$. The extension to more predictors is possible, but requires some extra work on notation that we do not address in full generality. In this case, the complete knowledge of $Y$ when $X=x$ is given by the conditional pdf $f_{Y| X=x}(y)=\frac{f(x,y)}{f_X(x)}$. While this pdf provides full knowledge about $Y| X=x$, it is also a challenging task to estimate it: for each $x$ we have to estimate a *curve*! A simpler approach, yet still challenging, is to estimate the conditional mean (a scalar) for each $x$ through the regression function
$$
m(x)=\mathbb{E}[Y| X=x]=\int yf_{Y| X=x}(y)\mathrm{d}y.
$$
This density-based view of the regression function is useful in order to construct estimators, as we will see.

Finally, recall that $Y$ can expressed in terms of $m$ by means of the *location-scale model*:
$$
Y=m(X)+\sigma(X)\varepsilon,
$$
where $\sigma^2(x):=\mathbb{V}\mathrm{ar}[Y| X=x]$ is the *conditional variance* and $\varepsilon$ is independent from $X$ and such that $\mathbb{E}[\varepsilon]=0$ and $\mathbb{V}\mathrm{ar}[\varepsilon]=1$.

## Kernel density estimation {#npreg-kde}

In order to introduce a nonparametric estimator for the $m$, we need to introduce first a nonparametric *density* estimator for the *density* of the predictor $X$. This estimator is aimed to estimate $f$, the density of $X$, from a sample $X_1,\ldots,X_n$ and without assuming any specific form for $f$. The **kernel density estimator**^[Also known as the *Parzen-Rosemblatt estimator* to honor the proposals by @Parzen1962 and @Rosenblatt1956.] does this job with the estimate
\begin{align}
\hat f(x;h):=\frac{1}{nh}\sum_{i=1}^nK\left(\frac{x-X_i}{h}\right), (\#eq:kde)
\end{align}
where $K$ is known as a *kernel*, a density that is typically symmetric and unimodal at $0$, and $h>0$ is the *bandwidth*. A common notation is $K_h(z):=\frac{1}{h}K\left(\frac{z}{h}\right)$, so $\hat f(x;h)=\frac{1}{n}\sum_{i=1}^nK_h(x-X_i)$. It is useful to recall \@ref(eq:kde) with the normal kernel. If that is the case, then $K_h(x-X_i)=\phi_h(x-X_i)$ and the kernel is the density of a $\mathcal{N}(X_i,h^2)$. Thus the bandwidth $h$ can be thought as the *standard deviation of the normal densities that have mean zero*. Figure \@ref(fig:kdeconst) illustrates the construction of the kernel density estimator, and the bandwidth and kernel effects.

(ref:kdeconsttitle) Construction of the kernel density estimator. The animation shows how the bandwidth and kernel affect the density estimate, and how the kernels are rescaled densities with modes at the data points. Application also available [here](https://ec2-35-177-34-200.eu-west-2.compute.amazonaws.com/kde/).

```{r, kdeconst, echo = FALSE, fig.cap = '(ref:kdeconsttitle)', screenshot.alt = "images/screenshots/kde.png", dev = 'png', cache = TRUE, fig.pos = 'h!', out.width = '90%'}
knitr::include_app('https://ec2-35-177-34-200.eu-west-2.compute.amazonaws.com/kde/', height = '1000px')
```

Several types of kernels are possible. The most popular is the *normal kernel* $K(z)=\phi(z)$, although the *Epanechnikov kernel*, $K(z)=\frac{3}{4}(1-z^2)1_{\{|z|<1\}}$, is the most efficient. The *rectangular kernel* $K(z)=\frac{1}{2}1_{\{|z|<1\}}$ yields the moving histogram as a particular case. The kernel density estimator inherits the smoothness properties of the kernel. That means, for example, \@ref(eq:kde) with a normal kernel is infinitely differentiable. But with an Epanechnikov kernel, \@ref(eq:kde) is not differentiable, and with a rectangular kernel is not even continuous. However, if a certain smoothness is guaranteed (continuity at least), the *choice of the kernel has little importance in practice* (at least compared with the choice of $h$). 

Implementation of the kernel density estimator in `R` is built-in through the `density` function. The function automatically chooses the bandwidth $h$ using a *bandwidth selector*.

```{r, kdeR, echo = TRUE, eval = TRUE, collapse = TRUE, cache = TRUE}
# Sample 100 points from a N(0, 1)
set.seed(1234567)
samp <- rnorm(n = 100, mean = 0, sd = 1)

# Quickly compute a kernel density estimator and plot the density object
# Automatically chooses bandwidth and uses normal kernel
plot(density(x = samp))

# Select a particular bandwidth (0.5) and kernel (Epanechnikov)
lines(density(x = samp, bw = 0.5, kernel = "epanechnikov"), col = 2)

# density automatically chooses the interval for plotting the kernel density 
# estimator (observe that the black line goes to roughly between -3 and 3)
# This can be tuned using "from" and "to"
plot(density(x = samp, from = -4, to = 4), xlim = c(-5, 5))

# The density object is a list
kde <- density(x = samp, from = -5, to = 5, n = 1024)
str(kde)
# Note that the evaluation grid "x"" is not directly controlled, only through
# "from, "to", and "n" (better use powers of 2)
plot(kde$x, kde$y, type = "l")
curve(dnorm(x), col = 2, add = TRUE) # True density
rug(samp)
```

```{block, exfaithful, type = 'rmdexercise', cache = TRUE}
Load the dataset `faithful`. Then:

- Estimate and plot the density of `faithful$eruptions`.
- Create a new plot and superimpose different density estimations with bandwidths equal to $0.1$, $0.5$, and $1$.
- Get the density estimate at *exactly* the point $x=3.1$ using $h=0.15$ and the Epanechnikov kernel.

```

The kernel density estimator can be extended to the multivariate setting by using *product kernels*. For a sample $\mathbf{X}_1,\ldots,\mathbf{X}_n$ in $\mathbb{R}^p$, the multivariate kernel density estimator employing product kernels is defined as
\begin{align}
\hat f(\mathbf{x};\mathbf{h})=\frac{1}{n}\sum_{i=1}^nK_{h_1}(x_1-X_{i,1})\times\stackrel{p}{\cdots}\times K_{h_p}(x_p-X_{i,p}),(\#eq:multkde)
\end{align}
where $\mathbf{x}=(x_{1},\ldots,x_{p})$, $\mathbf{X}_i=(X_{i,1},\ldots,X_{i,p})$, and $\mathbf{h}=(h_1,\ldots,h_p)$ is a vector of bandwidths (if the variables $X_1,\ldots,X_p$ have been standardized, then a simple choice is to consider $h=h_1=\ldots=h_p$). The interpretation of \@ref(eq:multkde) is analogous to the one of \@ref(eq:kde): build a mixture of densities with each density centered at the each data point.

## Kernel regression estimation {#npreg-kre}

### Nadaraya-Watson estimator {#npreg-nw}

Our objective is to estimate the regression function $m$ nonparametrically. Due to its definition, we can rewrite it as follows:
\begin{align*}
m(x)=&\,\mathbb{E}[Y| X=x]\nonumber\\
=&\,\int y f_{Y| X=x}(y)\mathrm{d}y\nonumber\\
=&\,\frac{\int y f(x,y)\mathrm{d}y}{f_X(x)}.
\end{align*}
This expression shows an interesting point: the regression function can be computed from the joint density $f$ and the marginal $f_X$. Therefore, given a sample $(X_1,Y_1),\ldots,(X_n,Y_n)$, an estimate of $m$ follows by replacing the previous densities by their kernel density estimators. To that aim, recall that in \@ref(eq:multkde) we defined a multivariate extension of \@ref(eq:kde) based on product kernels. For the two dimensional case, the kernel density estimator with equal bandwidths $\mathbf{h}=(h,h)$ is
\begin{align}
\hat f(x,y;h)=\frac{1}{n}\sum_{i=1}^nK_{h}(x_1-X_{i})K_{h}(y-Y_{i}).(\#eq:kdem)
\end{align}
Using \@ref(eq:kdem),
\begin{align*}
m(x)\approx&\,\frac{\int y \hat f(x,y;h)\mathrm{d}y}{\hat f_X(x;h)}\\
=&\,\frac{\int y \hat f(x,y;h)\mathrm{d}y}{\hat f_X(x;h)}\\
=&\,\frac{\int y \frac{1}{n}\sum_{i=1}^nK_h(x-X_i)K_h(y-Y_i)\mathrm{d}y}{\frac{1}{n}\sum_{i=1}^nK_h(x-X_i)}\\
=&\,\frac{\frac{1}{n}\sum_{i=1}^nK_h(x-X_i)\int y K_h(y-Y_i)\mathrm{d}y}{\frac{1}{n}\sum_{i=1}^nK_h(x-X_i)}\\
=&\,\frac{\frac{1}{n}\sum_{i=1}^nK_h(x-X_i)Y_i}{\frac{1}{n}\sum_{i=1}^nK_h(x-X_i)}.
\end{align*}
This yields the so-called **Nadaraya-Watson**^[Termed due to the coetaneous proposals by @Nadaraya1964 and @Watson1964.] estimate:
\begin{align}
\hat m(x;0,h):=\frac{1}{n}\sum_{i=1}^n\frac{K_h(x-X_i)}{\frac{1}{n}\sum_{i=1}^nK_h(x-X_i)}Y_i=\sum_{i=1}^nW^0_{i}(x)Y_i, (\#eq:nw)
\end{align}
where $W^0_{i}(x):=\frac{K_h(x-X_i)}{\sum_{i=1}^nK_h(x-X_i)}$. This estimate can be seen as a weighted average of $Y_1,\ldots,Y_n$ by means of the set of weights $\{W_{n,i}(x)\}_{i=1}^n$ (check that they add to one). The set of weights depends on the evaluation point $x$. That means that the Nadaraya-Watson estimator is a **local mean of $Y_1,\ldots,Y_n$ around $X=x$** (see Figure \@ref(fig:kreg)).

Let's implement the Nadaraya-Watson estimate to get a feeling of how it works in practice.

```{r, nw-1, echo = TRUE, collapse = TRUE, cache = TRUE}
# Nadaraya-Watson
mNW <- function(x, X, Y, h, K = dnorm) {

  # Arguments
  # x: evaluation points
  # X: vector (size n) with the predictors
  # Y: vector (size n) with the response variable
  # h: bandwidth
  # K: kernel

  # Matrix of size n x length(x)
  Kx <- sapply(X, function(Xi) K((x - Xi) / h) / h)

  # Weights
  W <- Kx / rowSums(Kx) # Column recycling!

  # Means at x ("drop" to drop the matrix attributes)
  drop(W %*% Y)

}

# Generate some data to test the implementation
set.seed(12345)
n <- 100
eps <- rnorm(n, sd = 2)
m <- function(x) x^2 * cos(x)
X <- rnorm(n, sd = 2)
Y <- m(X) + eps
xGrid <- seq(-10, 10, l = 500)

# Bandwidth
h <- 0.5

# Plot data
plot(X, Y)
rug(X, side = 1); rug(Y, side = 2)
lines(xGrid, m(xGrid), col = 1)
lines(xGrid, mNW(x = xGrid, X = X, Y = Y, h = h), col = 2)
legend("topright", legend = c("True regression", "Nadaraya-Watson"),
       lwd = 2, col = 1:2)
```

```{block, exnadwat, type = 'rmdexercise', cache = TRUE}
Implement your own version of the Nadaraya-Watson estimator in `R` and compare it with `mNW`. You may focus only on the normal kernel and reduce the accuracy of the final computation up to `1e-7` to achieve better efficiency. Are you able to improve the speed of `mNW`? Use `system.time` or the `microbenchmark` package to measure the running times for a sample size of $n=10000$.
```

The code below illustrates the effect of varying $h$ for the Nadaraya-Watson estimator using `manipulate`.

```{r, nw-2, echo = TRUE, eval = FALSE, collapse = TRUE, cache = TRUE}
# Simple plot of N-W for varying h's
library(manipulate)
manipulate({

  # Plot data
  plot(X, Y)
  rug(X, side = 1); rug(Y, side = 2)
  lines(xGrid, m(xGrid), col = 1)
  lines(xGrid, mNW(x = xGrid, X = X, Y = Y, h = h), col = 2)
  legend("topright", legend = c("True regression", "Nadaraya-Watson"),
         lwd = 2, col = 1:2)

}, h = slider(min = 0.01, max = 2, initial = 0.5, step = 0.01))
```

### Local polynomial regression {#npreg-locpoly}

Nadaraya-Watson can be seen as a particular case of a *local polynomial fit*, specifically, the one corresponding to a *local constant fit*. The motivation for the local polynomial fit comes from attempting to the minimize the RSS
\begin{align}
\sum_{i=1}^n(Y_i-m(X_i))^2.(\#eq:mrss)
\end{align}
This is not achievable directly, since no knowledge on $m$ is available. However, by a $p$-th order Taylor expression it is possible to obtain that, for $x$ close to $X_i$,
\begin{align}
m(X_i)\approx&\, m(x)+m'(x)(X_i-x)+\frac{m''(x)}{2}(X_i-x)^2\nonumber\\
&+\cdots+\frac{m^{(p)}(x)}{p!}(X_i-x)^p.(\#eq:mtay)
\end{align}
Replacing \@ref(eq:mtay) in \@ref(eq:mrss), we have that
\begin{align}
\sum_{i=1}^n\left(Y_i-\sum_{j=0}^p\frac{m^{(j)}(x)}{j!}(X_i-x)^j\right)^2.(\#eq:mder)
\end{align}
This expression is still not workable: it depends on $m^{(j)}(x)$, $j=0,\ldots,p$, which of course are unknown! The *great idea* is to set $\beta_j:=\frac{m^{(j)}(x)}{j!}$ and turn \@ref(eq:mder) into a linear regression problem where the unknown parameters are $\boldsymbol{\beta}=(\beta_0,\beta_1,\ldots,\beta_p)'$:
\begin{align}
\sum_{i=1}^n\left(Y_i-\sum_{j=0}^p\beta_j(X_i-x)^j\right)^2.(\#eq:minbe)
\end{align}
While doing so, an estimate of $\boldsymbol{\beta}$ automatically will yield estimates for $m^{(j)}(x)$, $j=0,\ldots,p$, and we know how to obtain $\hat{\boldsymbol{\beta}}$ by minimizing \@ref(eq:minbe). The final touch is to make the contributions of $X_i$ dependent on the distance to $x$ by weighting with kernels:
\begin{align}
\hat{\boldsymbol{\beta}}:=\arg\min_{\boldsymbol{\beta}\in\mathbb{R}^{p+1}}\sum_{i=1}^n\left(Y_i-\sum_{j=0}^p\beta_j(X_i-x)^j\right)^2K_h(x-X_i).(\#eq:hatb)
\end{align}
Denoting
$$
\mathbf{X}:=\begin{pmatrix}
1 & X_1-x & \cdots & (X_1-x)^p\\
\vdots & \vdots & \ddots & \vdots\\
1 & X_n-x & \cdots & (X_n-x)^p\\
\end{pmatrix}_{n\times(p+1)}
$$
and
$$
\mathbf{W}:=\mathrm{diag}(K_h(X_1-x),\ldots, K_h(X_n-x)),\quad
\mathbf{Y}:=\begin{pmatrix}
Y_1\\
\vdots\\
Y_n
\end{pmatrix}_{n\times 1},
$$
we can re-express \@ref(eq:hatb) into a *weighted least squares problem* whose exact solution is
\begin{align}
\hat{\boldsymbol{\beta}}&=\arg\min_{\boldsymbol{\beta}\in\mathbb{R}^{p+1}} (\mathbf{Y}-\mathbf{X}\boldsymbol{\beta})'\mathbf{W}(\mathbf{Y}-\mathbf{X}\boldsymbol{\beta})\\
&=(\mathbf{X}'\mathbf{W}\mathbf{X})^{-1}\mathbf{X}'\mathbf{W}\mathbf{Y}.(\#eq:betaw)
\end{align}

The estimate for $m(x)$ is then computed as
\begin{align*}
\hat m(x;p,h):&=\hat\beta_0\\
&=\mathbf{e}_1'(\mathbf{X}'\mathbf{W}\mathbf{X})^{-1}\mathbf{X}'\mathbf{W}\mathbf{Y}\\
&=\sum_{i=1}^nW^p_{i}(x)Y_i,
\end{align*}
where $W^p_{i}(x):=\mathbf{e}_1'(\mathbf{X}'\mathbf{W}\mathbf{X})^{-1}\mathbf{X}'\mathbf{W}\mathbf{e}_i$ and $\mathbf{e}_i$ is the $i$-th canonical vector. Just as the Nadaraya-Watson, the local polynomial estimator is a *linear combination of the responses*. Two cases deserve special attention:

- $p=0$ is the *local constant estimator* or the Nadaraya-Watson estimator. In this situation, the estimator has explicit weights, as we saw before:
$$
W_i^0(x)=\frac{K_h(x-X_i)}{\sum_{j=1}^nK_h(x-X_j)}.
$$
- $p=1$ is the *local linear estimator*, which has weights equal to:
\begin{align}
W_i^1(x)=\frac{\hat s_2(x;h)-\hat s_1(x;h)(X_i-x)}{\hat s_2(x;h)\hat s_0(x;h)-\hat s_1(x;h)^2}K_h(x-X_i),(\#eq:we)
\end{align}
where $\hat s_r(x;h):=\frac{1}{n}\sum_{i=1}^n(X_i-x)^rK_h(x-X_i)$.

Figure \@ref(fig:kreg) illustrates the construction of the local polynomial estimator (up to cubic degree) and shows how $\hat\beta_0=\hat m(x;p,h)$, the intercept of the local fit, estimates $m$ at $x$.

(ref:kregtitle) Construction of the local polynomial estimator. The animation shows how local polynomial fits in a neighborhood of $x$ are combined to provide an estimate of the regression function, which depends on the polynomial degree, bandwidth, and kernel (gray density at the bottom). The data points are shaded according to their weights for the local fit at $x$. Application also available [here](https://ec2-35-177-34-200.eu-west-2.compute.amazonaws.com/kreg/).

```{r, kreg, echo = FALSE, fig.cap = '(ref:kregtitle)', screenshot.alt = "images/screenshots/kreg.png", dev = 'png', cache = TRUE, fig.pos = 'h!', out.width = '100%'}
knitr::include_app('https://ec2-35-177-34-200.eu-west-2.compute.amazonaws.com/kreg/', height = '1000px')
```

`KernSmooth`'s `locpoly` implements the local polynomial estimator. Below are some examples of its usage.

```{r, lp-1, echo = TRUE, collapse = TRUE, cache = TRUE}
# Generate some data to test the implementation
set.seed(123456)
n <- 100
eps <- rnorm(n, sd = 2)
m <- function(x) x^3 * sin(x)
X <- rnorm(n, sd = 1.5)
Y <- m(X) + eps
xGrid <- seq(-10, 10, l = 500)

# Fits
h <- 0.25
lp0 <- locpoly(x = X, y = Y, bandwidth = h, degree = 0, range.x = c(-10, 10),
               gridsize = 500)
lp1 <- locpoly(x = X, y = Y, bandwidth = h, degree = 1, range.x = c(-10, 10),
               gridsize = 500)

# Prediction at x = 2
x <- 2
lp1$y[which.min(abs(lp1$x - x))] # Prediction
m(x) # Reality

# Plot data
plot(X, Y)
rug(X, side = 1); rug(Y, side = 2)
lines(xGrid, m(xGrid), col = 1)
lines(lp0$x, lp0$y, col = 2)
lines(lp1$x, lp1$y, col = 3)
legend("bottom", legend = c("True regression", "Local constant",
                         "Local linear"),
       lwd = 2, col = 1:3)
```

```{r, lp-2, echo = TRUE, eval = FALSE, collapse = TRUE, cache = TRUE}
# Simple plot of local polynomials for varying h's
library(manipulate)
manipulate({

  # Plot data
  lpp <- locpoly(x = X, y = Y, bandwidth = h, degree = p, range.x = c(-10, 10),
                 gridsize = 500)
  plot(X, Y)
  rug(X, side = 1); rug(Y, side = 2)
  lines(xGrid, m(xGrid), col = 1)
  lines(lpp$x, lpp$y, col = p + 2)
  legend("bottom", legend = c("True regression", "Local polynomial fit"),
         lwd = 2, col = c(1, p + 2))

}, h = slider(min = 0.01, max = 2, initial = 0.5, step = 0.01),
p = slider(min = 0, max = 4, initial = 0, step = 1))

```

A more sophisticated framework for performing nonparametric estimation of the regression function, for **multiple predictors** that are continuous or **discrete**, is the `np` package. The code below illustrates the usage of the function that implements nonparametric regression, `npreg`.

```{r, lp-3, echo = TRUE, collapse = TRUE, cache = TRUE}
# TODO
```

## Asymptotic properties {#npreg-asymp}

The purpose of this section is to provide some highlights on the asymptotic bias and variance of the local linear and local constant estimators. These provide useful insights on the effect of $p$, $m$, $f$, and $\sigma^2$ in the performance of the estimators.

Along this section we will make the following assumptions^[These are the only assumptions done so far in the model (in addition to $\varepsilon$ independent from $X$).]:

- **A1**. $m$ is twice continuously differentiable.
- **A2**. $\sigma^2$ is continuous and positive.
- **A3**. $f$ is continuously differentiable and positive.
- **A4**. The kernel $K$ is a symmetric and bounded pdf with finite second moment and is square integrable.
- **A5**. $h=h_n$ is a deterministic sequence of bandwidths such that, when $n\to\infty$, $h\to0$ and $nh\to\infty$.

The bias and variance are expanded in their *conditional* versions on the predictor's sample $X_1,\ldots,X_n$. The reason for analyzing the conditional instead of the *unconditional* versions is avoiding technical difficulties that integration with respect to the predictor's density may pose. The main result is the following.

```{theorem, thbvloc, label = "biasvarloc", cache = TRUE}
Under **A1**--**A5**, the conditional bias and variance of the local constant ($p=0$) and local linear ($p=1$) estimators are^[The notation $o_\mathbb{P}(a_n)$ stands for a random variable that converges in probability to zero at a rate faster than $a_n$. It is mostly employed for denoting non-important terms in asymptotic expansions, like the ones in \@ref(eq:mbias)--\@ref(eq:mvar).]
\begin{align}
\mathrm{Bias}[\hat m(x;p,h)| X_1,\ldots,X_n]&=B_p(x)h^2+o_\mathbb{P}(h^2),(\#eq:mbias)\\
\mathbb{V}\mathrm{ar}[\hat m(x;p,h)| X_1,\ldots,X_n]&=\frac{R(K)}{nhf(x)}\sigma^2(x)+o_\mathbb{P}((nh)^{-1}),(\#eq:mvar)
\end{align}
where
$$
B_p(x)=\frac{\mu_2(K)}{2}\left\{m''(x)+2\frac{m'(x)f'(x)}{f(x)}1_{\{p=0\}}\right\}.
$$
```

The bias and variance expressions \@ref(eq:mbias) and \@ref(eq:mvar) yield very interesting insights:

- The bias decreases with $h$ *quadratically* for $p=0,1$. The bias at $x$ is directly proportional to $m''(x)$ if $p=1$ and affected by $m''(x)$ if $p=0$. Therefore:

    - The bias is negative in concave regions, *i.e.* $\{x\in\mathbb{R}:m(x)''<0\}$. These regions correspond to *peaks and modes of $m$*
    - Conversely, the bias is positive in convex regions, *i.e.* $\{x\in\mathbb{R}:m(x)''>0\}$. These regions correspond to *valleys of $m$*.
    - **The wilder the curvature $m''$, the harder to estimate $m$**.

- The bias for $p=0$ at $x$ is affected by $m'(x)$, $f'(x)$, and $f(x)$. Precisely, **the lower the density $f(x)$, the larger the bias**. And **the faster $m$ and $f$ change at $x$, the larger the bias**. Thus the bias of the local constant estimator is much more sensible to $m(x)$ and $f(x)$ than the local linear (which is only sensible to $m''(x)$).Particularly, the fact that it depends on $f'(x)$ and $f(x)$ is referred as the *design bias* since it depends merely on the predictor's distribution.

- The variance depends directly on $\frac{\sigma^2(x)}{f(x)}$ for $p=0,1$. As a consequence, **the lower the density and larger the conditional variance, the more variable is $\hat m(\cdot;p,h)$.** The variance decreases at a factor of $(nh)^{-1}$ due to the *effective sample size*, which can be thought as the amount of data in the neighborhood of $x$ for performing the estimation.

An extended version of Theorem \@ref(thm:biasvarloc), given in Theorem 3.1 of @Fan1996, shows that **odd order polynomial fits are preferable to even order polynomial fits**. The reason is that odd orders introduce an extra coefficient for the polynomial fit that allows them to reduce the bias, while at the same time they keep the variance unchanged. In summary, the conclusions of the above analysis of $p=0$ vs. $p=1$, namely that **$p=1$ has smaller bias than $p=0$ (but of the same order) and the same variance as $p=0$**, extend to the case $p=2\nu$ vs. $p=2\nu+1$, $\nu\in\mathbb{N}$. This allows us to claim that local polynomial fitting *is an odd world* (@Fan1996).

## Bandwidth selection {#npreg-bwd}

Bandwidth selection is of key practical importance. Several bandwidth selectors have been proposed for kernel regression but, for simplicity, we will focus only on the **cross-validation** bandwidth selector.

Following an analogy with the fit of the linear model, we could look for the bandwidth $h$ such that it minimizes the RSS of the form
\begin{align}
\frac{1}{n}\sum_{i=1}^n(Y_i-\hat m(X_i;p,h))^2.(\#eq:badcv)
\end{align}
However, this is a bad idea. Attempting to minimize \@ref(eq:badcv) always leads to $h\approx 0$ that results in a useless interpolation of the data. Let's see an example.

```{r, bwd-1, echo = TRUE, eval = TRUE, collapse = TRUE, cache = TRUE}
# Grid for representing (3.22)
hGrid <- seq(0.1, 1, l = 200)^2
error <- sapply(hGrid, function(h)
  mean((Y - mNW(x = X, X = X, Y = Y, h = h))^2))

# Error curve
plot(hGrid, error, type = "l")
rug(hGrid)
abline(v = hGrid[which.min(error)], col = 2)
```
The root of the problem is the comparison of $Y_i$ with $\hat m(X_i;p,h)$, since there is nothing that forbids $h\to0$ and as a consequence $\hat m(X_i;p,h)\to Y_i$. We can change this behavior if we compare $Y_i$ with $\hat m_{-i}(X_i;p,h)$, the **leave-one-out estimate** computed without the $i$-th datum $(X_i,Y_i)$:
\begin{align*}
\mathrm{CV}(h)&:=\frac{1}{n}\sum_{i=1}^n(Y_i-\hat m_{-i}(X_i;p,h))^2,\\
h_\mathrm{CV}&:=\arg\min_{h>0}\mathrm{CV}(h).
\end{align*}
The optimization of the above criterion might seem to be computationally expensive, since it is required to compute $n$ regressions for a *single* evaluation of the objective function.

```{proposition, thbwdcv, label = "cv", cache = TRUE}
The weights of the leave-one-out estimator $\hat m_{-i}(x;p,h)=\sum_{\substack{j=1\\j\neq i}}^nW_{-i,j}^p(x)Y_j$ can be obtained from $\hat m(x;p,h)=\sum_{i=1}^nW_{i}^p(x)Y_i$:
\begin{align*}
W_{-i,j}^p(x)=\frac{W^p_j(x)}{\sum_{\substack{k=1\\k\neq i}}^nW_k^p(x)}=\frac{W^p_j(x)}{1-W_i^p(x)}.
\end{align*}
This implies that
\begin{align*}
\mathrm{CV}(h)=\frac{1}{n}\sum_{i=1}^n\left(\frac{Y_i-\hat m(X_i;p,h)}{1-W_i^p(X_i)}\right)^2.
\end{align*}

```

Let's implement this simple bandwidth selector in `R`.

```{r, bwd-2, echo = TRUE, collapse = TRUE, cache = TRUE}
# Generate some data to test the implementation
set.seed(12345)
n <- 100
eps <- rnorm(n, sd = 2)
m <- function(x) x^2 + sin(x)
X <- rnorm(n, sd = 1.5)
Y <- m(X) + eps
xGrid <- seq(-10, 10, l = 500)

# Objective function
cvNW <- function(X, Y, h, K = dnorm) {
  	sum(((Y - mNW(x = X, X = X, Y = Y, h = h, K = K)) /
  	       (1 - K(0) / colSums(K(outer(X, X, "-") / h))))^2)
}

# Find optimum CV bandwidth, with sensible grid
bw.cv.grid <- function(X, Y,
                       h.grid = diff(range(X)) * (seq(0.1, 1, l = 200))^2,
                       K = dnorm, plot.cv = FALSE) {
	obj <- sapply(h.grid, function(h) cvNW(X = X, Y = Y, h = h, K = K))
  h <- h.grid[which.min(obj)]
  if (plot.cv) {
    plot(h.grid, obj, type = "o")
    rug(h.grid)
    abline(v = h, col = 2, lwd = 2)
  }
  h
}

# Bandwidth
h <- bw.cv.grid(X = X, Y = Y, plot.cv = TRUE)
h

# Plot result
plot(X, Y)
rug(X, side = 1); rug(Y, side = 2)
lines(xGrid, m(xGrid), col = 1)
lines(xGrid, mNW(x = xGrid, X = X, Y = Y, h = h), col = 2)
legend("topright", legend = c("True regression", "Nadaraya-Watson"),
       lwd = 2, col = 1:2)
```

A more sophisticated bandwidth selection can be achieved by `npregbw` in the `np` package. The code below illustrates the usage of this function.

```{r, bwd-3, echo = TRUE, collapse = TRUE, cache = TRUE}
# TODO
```

## Local likelihood {#npreg-loclik}

We explore in this section an extension of the local polynomial estimator. This extension aims to estimate the regression function by relying in the likelihood, rather than the least squares. Thus, the idea behind the local likelihood is to **fit, locally, parametric models by maximum likelihood**.

We begin by seeing that local likelihood using the the linear model is equivalent to local polynomial modelling. Theorem \@ref(thm:lik) showed that, under the assumptions given in Section \@ref(lm-i-assumps), the maximum likelihood estimate of $\boldsymbol{\beta}$ in the linear model
\begin{align}
Y|(X_1,\ldots,X_p)\sim\mathcal{N}(\beta_0+\beta_1X_1+\ldots+\beta_pX_p,\sigma^2)(\#eq:linmod)
\end{align}
was equivalent to the least squares estimate, $\hat{\boldsymbol{\beta}}=(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}\mathbf{Y}$. The reason was the form of the conditional (on $X_1,\ldots,X_p$) likelihood:
\begin{align*}
\ell(\boldsymbol{\beta})=&\,-\frac{n}{2}\log(2\pi\sigma^2)\nonumber\\
&-\frac{1}{2\sigma^2}\sum_{i=1}^n(Y_i-\beta_0-\beta_1X_{i1}-\ldots-\beta_pX_{ip})^2.
\end{align*}
If there is a single predictor $X$, polynomial fitting of order $p$ of the conditional mean can be achieved by the well-known trick of identifying the $j$-th predictor $X_j$ in \@ref(eq:linmod) by $X^j$. This results in
\begin{align}
Y|X\sim\mathcal{N}(\beta_0+\beta_1X+\ldots+\beta_pX^p,\sigma^2).(\#eq:linmodp)
\end{align}
Therefore, maximizing with respect to $\boldsymbol{\beta}$ the **weighted log-likelihood of the linear model around $x$** of \@ref(eq:linmodp),
\begin{align*}
\ell_{x,h}(\boldsymbol{\beta})=&\,-\frac{n}{2}\log(2\pi\sigma^2)\\
&-\frac{1}{2\sigma^2}\sum_{i=1}^n(Y_i-\beta_0-\beta_1X_i-\ldots-\beta_pX_i^p)^2K_h(x-X_i),
\end{align*}
**provides $\hat\beta_0=\hat m(x;p,h)$, the local polynomial estimator**, as it was obtained in \@ref(eq:hatb), but **from a likelihood-based perspective**. The same idea can be applied to the family of **generalized linear models**.

(ref:locliktitle) Construction of the local likelihood estimator. The animation shows how local likelihood fits in a neighborhood of $x$ are combined to provide an estimate of the regression function for binary response, which depends on the polynomial degree, bandwidth, and kernel (gray density at the bottom). The data points are shaded according to their weights for the local fit at $x$. Application also available [here](https://ec2-35-177-34-200.eu-west-2.compute.amazonaws.com/loclik/).

```{r, loclik, echo = FALSE, fig.cap = '(ref:locliktitle)', screenshot.alt = "images/screenshots/loclik.png", dev = 'png', cache = TRUE, fig.pos = 'h!', out.width = '100%'}
knitr::include_app('https://ec2-35-177-34-200.eu-west-2.compute.amazonaws.com/loclik/', height = '900px')
```

We illustrate the local likelihood principle for the logistic regression. In this case, $(X_1,Y_1),\ldots,(X_n,Y_n)$ with
$$
Y_i|X_i\sim \mathrm{Ber}(\mathrm{logistic}(\eta(X_i))),\quad i=1,\ldots,n.
$$
with the polynomial term^[If $p=1$, then we have the usual logistic model.]
\[
\eta(x):=\beta_0+\beta_1x+\ldots+\beta_px^p.
\]
The log-likelihood of $\boldsymbol{\beta}$ is
\begin{align*}
\ell(\boldsymbol{\beta})
=&\,\sum_{i=1}^n\left\{Y_i\log(\mathrm{logistic}(\eta(X_i)))+(1-Y_i)\log(1-\mathrm{logistic}(\eta(X_i)))\right\}\nonumber\\
=&\,\sum_{i=1}^n\ell(Y_i,\eta(X_i)),
\end{align*}
where we consider the *log-likelihood addend* $\ell(y,\eta)=y\eta-\log(1+e^\eta)$, and make explicit the dependence on $\eta(x)$ for clarity in the next developments, and implicit the dependence on $\boldsymbol{\beta}$. 

The **local log-likelihood of $\boldsymbol{\beta}$ around $x$** is then
\begin{align}
\ell_{x,h}(\boldsymbol{\beta})
:=\sum_{i=1}^n\ell(Y_i,\eta(X_i-x))K_h(x-X_i).(\#eq:logploclik)
\end{align}
Maximizing^[No analytical solution for the optimization problem, numerical optimization is needed.] the local log-likelihood \@ref(eq:logploclik) with respect to $\boldsymbol{\beta}$ provides
$$
\hat{\boldsymbol{\beta}}=\arg\max_{\boldsymbol{\beta}\in\mathbb{R}^{p+1}}\ell_{x,h}(\boldsymbol{\beta}).
$$
The local likelihood estimate of $\eta(x)$ is
\[
\hat\eta(x):=\hat\beta_0.
\]
Note that the dependence of $\hat\beta_0$ on $x$ and $h$ is omitted. From $\hat\eta(x)$, we can obtain the *local logistic regression* evaluated at $x$ as
\begin{align}
\hat m_{\ell}(x;h,p):=g^{-1}\left(\hat\eta(x)\right)=\mathrm{logistic}(\hat\beta_0).(\#eq:glmloc)
\end{align}
Each evaluation of $\hat m_{\ell}(x;h,p)$ in a different $x$ requires, thus, a fit of the underlying logistic model.

The code below shows three different ways of implementing the local logistic regression (of first degree) in `R`.

```{r, ll-1, echo = TRUE, eval = TRUE, collapse = TRUE, cache = TRUE}
# Simulate some data
n <- 200
logistic <- function(x) 1 / (1 + exp(-x))
p <- function(x) logistic(1 - 3 * sin(x))
set.seed(123456)
X <- runif(n = n, -3, 3)
Y <- rbinom(n = n, size = 1, prob = p(X))

# Set bandwidth and evaluation grid
h <- 0.25
x <- seq(-3, 3, l = 501)

# Optimize the weighted log-likelihood through glm's built in procedure
suppressWarnings(
  fitGlm <- sapply(x, function(x) {
    K <- dnorm(x = x, mean = X, sd = h)
    glm.fit(x = cbind(1, X - x), y = Y, weights = K,
            family = binomial())$coefficients[1]
  })
)

# Optimize the weighted log-likelihood explicitly
suppressWarnings(
  fitNlm <- sapply(x, function(x) {
    K <- dnorm(x = x, mean = X, sd = h)
    nlm(f = function(beta) {
      -sum(K * (Y * (beta[1] + beta[2] * (X - x)) -
                  log(1 + exp(beta[1] + beta[2] * (X - x)))))
      }, p = c(0, 0))$estimate[1]
  })
)

# Using locfit
# Bandwidth can not be controlled explicitly - only though nn in ?lp
library(locfit)
fitLocfit <- locfit(Y ~ lp(X, deg = 1, nn = 0.25), family = "binomial",
                    kern = "gauss")

# Compare fits
plot(x, p(x), ylim = c(0, 1.5), type = "l", lwd = 2)
lines(x, logistic(fitGlm), col = 2)
lines(x,logistic(fitNlm), col = 2, lty = 2)
plot(fitLocfit, add = TRUE, col = 4)
legend("topright", legend = c("p(x)", "glm", "nlm", "locfit"), lwd = 2,
       col = c(1, 2, 2, 4), lty = c(1, 2, 1, 1))
```

Bandwidth selection can be done by means of *likelihood cross-validation*. The objective is to maximize the local likelihood fit at $(X_i,Y_i)$ but removing the influence by the datum itself. That is, maximizing
\begin{align}
\mathrm{LCV}(h)=\sum_{i=1}^n\ell(Y_i,\hat\eta_{-i}(X_i)),(\#eq:cvloclik)
\end{align}
where $\hat\eta{-i}(X_i)$ represents the local fit at $X_i$ without the $i$-th datum $(X_i,Y_i)$. Unfortunately, the nonlinearity of \@ref(eq:glmloc) forbids a simplifying result as Proposition \@ref(prp:cv). Thus, in principle, it is required to fit $n$ local likelihoods for sample size $n-1$ for obtaining a single evaluation of \@ref(eq:cvloclik). The interested reader is referred to Sections 4.3.3 and 4.4.3 of @Loader1999 for an approximation of \@ref(eq:cvloclik) that only requires a local likelihood fit for a single sample.

We conclude by illustrating how to compute the LCV function and optimize it (keep in mind that much more efficient implementations are possible!).

```{r, ll-2, echo = TRUE, eval = TRUE, collapse = TRUE, cache = TRUE}
# Exact LCV - recall that we *maximize* the LCV!
h <- seq(0.1, 2, by = 0.1)
suppressWarnings(
  LCV <- sapply(h, function(h) {
  sum(sapply(1:n, function(i) {
    K <- dnorm(x = X[i], mean = X[-i], sd = h)
    nlm(f = function(beta) {
      -sum(K * (Y[-i] * (beta[1] + beta[2] * (X[-i] - X[i])) -
                  log(1 + exp(beta[1] + beta[2] * (X[-i] - X[i])))))
      }, p = c(0, 0))$minimum
    }))
  })
)
plot(h, LCV, type = "o")
abline(v = h[which.max(LCV)], col = 2)
```
