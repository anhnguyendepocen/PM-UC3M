---
title: "Notes for Predictive Modeling"
subtitle: "MSc in Big Data Analytics at Carlos III University of Madrid"
author: "Eduardo García Portugués"
date: "`r Sys.Date()`, v4.2"
knit: "bookdown::render_book"
documentclass: book
bibliography: PM-UC3M.bib
biblio-style: apalike
link-citations: yes
site: bookdown::bookdown_site
---

# Introduction {#intro}

<!--
(Move up once decided what options)
cover-image: no
description: "A subject within the MSc in Big Data Analytics at Carlos III University of Madrid (Spain)."
github-repo: egarpor/PM-UC3M
apple-touch-icon: "touch-icon.png"
apple-touch-icon-size: 120
favicon: "favicon.ico"
-->

```{block2, cauanimations, type = 'rmdcaution', cache = TRUE}
The **animations** of these notes will not be displayed the first time they are browsed^[The reason is because they are hosted at `https` websites with auto-signed SSL certificates.]. See for example Figure \@ref(fig:overfitting). **To see them**, click on the caption's link *"Application also available [here](https://ec2-35-177-34-200.eu-west-2.compute.amazonaws.com/over-fitting/)"*. You will get a warning from your browser saying that *"Your connection is not private"*. Click in *"Advanced"* and **allow an exception** in your browser (I guarantee you I will not do anything evil!). The next time the animation will show up correctly within the notes.
```

## Course overview {#intro-course}

Welcome to the notes for *Predictive Modeling*. These notes contain both the theory and practice for the statistical methods presented in the course. The emphasis is placed in building intuition behind the methods, gaining insights into their properties, and showing their application through the use of statistical software. The topics we will cover are an in-depth analysis of **linear models** ([I](https://bookdown.org/egarpor/PM-UC3M/lm-i.html), [II](https://bookdown.org/egarpor/PM-UC3M/lm-ii.html), and [III](https://bookdown.org/egarpor/PM-UC3M/lm-iii.html)), their extension to
[**generalized linear models**](https://bookdown.org/egarpor/PM-UC3M/glm.html), and an introduction to [**nonparametric regression**](https://bookdown.org/egarpor/PM-UC3M/npreg.html).

The notes contain a substantial amount of snippets of code that are fully self-contained within the chapter in which they are included. This allows to see how the methods and theory translate neatly to the practice. The **software** employed in the course is *the* statistical language [`R`](https://cran.r-project.org/) and its most common IDE (Integrated Development Environment) nowadays, [`RStudio`](https://www.rstudio.com/products/rstudio/download/). A basic prior knowledge of both is assumed^[Among others: basic programming in `R`, ability to work with objects and data structures, ability to produce graphics, knowledge of the main statistical functions, and ability to run scripts in `RStudio`.]. The appendix presents some basic introductions to `RStudio` and `R` for those students lacking basic expertise on them.

The **Shiny interactive apps** on the notes can be downloaded and run locally, which allows in particular to examine their codes. Check [this GitHub repository](https://github.com/egarpor/ShinyServer) for the sources.

## Required packages {#intro-packages}

We will employ several packages that are not contained in `R` (*note: list to be updated*). These can be installed as:
```{r, install, echo = TRUE, cache = TRUE, warning = FALSE, message = FALSE, eval = FALSE}
# Installation of required packages
install.packages(c("car", "readxl", "rgl", "nortest", "pca3d", "ISLR", "pls", 
                   "glmnet", "biglm", "mice", "VIM"))
#, "caret", "KernSmooth", "locfit"))
```

The codes in the notes may assume that the packages have been loaded, so it is better to do it now (*note: list to be updated*):
```{r, library, echo = TRUE, cache = TRUE, warning = FALSE, message = FALSE}
# Load packages
library(MASS)
library(car)
library(readxl)
library(rgl)
library(nortest)
library(pca3d)
library(ISLR)
library(pls)
library(glmnet)
library(biglm)
# library(caret)
# library(KernSmooth)
# library(locfit)
library(mice)
library(VIM)
```

## What is *predictive modeling*? {#intro-what-is}

> Predictive modeling is the process of developing a mathematical tool or model that generates an accurate prediction about a random quantity of interest.

In predictive modeling we are interested in predicting a random variable, typically denoted by $Y$, from a set of related variables $X_1,\ldots,X_p$. The focus is on learning what is the probabilistic model that relates $Y$ with $X_1,\ldots,X_p$, and use that acquired knowledge for predicting $Y$ given an observation of $X_1,\ldots,X_p$. Some concrete examples of this are:

- Predicting the wine quality ($Y$) from a set of environmental variables ($X_1,\ldots,X_p$).
- Predicting the number of sales ($Y$) from a set of marketing actions ($X_1,\ldots,X_p$).
- Modelling the average house value in a given suburb ($Y$) from a set of community-related features ($X_1,\ldots,X_p$).
- Predicting the probability of failure ($Y$) of a rocket launcher from the ambient temperature ($X_1$).

The process of predictive modelling can be statistically abstracted in the following way. We believe that $Y$ and $X_1,\ldots,X_p$ are related by a *regression model* of the form
\begin{align}
Y=m(X_1,\ldots,X_p)+\varepsilon,(\#eq:ym)
\end{align}
where $m$ is the *regression function* and $\varepsilon$ is a random error (which accounts for the uncertainty of knowing $Y$ if $X_1,\ldots,X_p$ are known) with zero mean. The function $m:\mathbb{R}^p\rightarrow\mathbb{R}$ is unknown in practice and is the target of predictive modelling: **$m$ encodes the relation**^[The relation is encoded in *average* by means of the *conditional expectation*.] **between $Y$ and $X_1,\ldots,X_p$**. $m$ captures the *trend* of the relation between $Y$ and $X_1,\ldots,X_p$, and $\varepsilon$ the *stochasticity* of that relation. 

Therefore, knowing $m$ allows to predict $Y$. We are going to devote this course to see some statistical models to came up with an estimate of $m$, denoted by $\hat m$, and use $\hat m$ to predict $Y$. 

Let's see a concrete example of this with an artificial dataset. Suppose $Y$ represents average fuel consumption (l/100km) of a car and $X$ is the average speed (km/h). It is well known from physics that the energy and speed have a quadratic relationship, and therefore we may assume that $Y$ and $X$ are *truly* quadratically-related for the sake of exposition:
\[
Y=a+bX+cX^2+\varepsilon.
\]
Then $m:\mathbb{R}\rightarrow\mathbb{R}$ ($p=1$) with $m(x)=a+bx+cx^2$. Suppose the following data are measurements from a given car model, measured in different drivers and conditions (we do not have data for accounting for all those effects, which go to the $\varepsilon$ term):
```{r, speedfuel, echo = TRUE, out.width = '70%', fig.pos = 'h!', fig.asp = 1, cache = TRUE}
x <- c(64, 20, 14, 64, 44, 39, 25, 53, 48, 9, 
       100, 112, 78, 105, 116, 94, 71, 71, 101, 109)
y <- c(4, 6, 6.4, 4.1, 4.9, 4.4, 6.6, 4.4, 3.8, 7, 
       7.4, 8.4, 5.2, 7.6, 9.8, 6.4, 5.1, 4.8, 8.2, 8.7)
plot(x, y, xlab = "Speed", ylab = "Fuel consumption")
```

From this data, we can estimate $m$ by means of a polynomial model^[Note we use the information that $m$ *has to be* of a particular form (in this case quadratic) which is an unrealistic situation for other data applications.]:
```{r, abcx2, echo = TRUE, cache = TRUE}
# Estimates for a, b, and c
lm(y ~ x + I(x^2))
```
Then the estimate of $m$ is $\hat m(x)=\hat a+\hat b x+\hat c x^2=8.512-0.153x+0.001x^2$ and its fit to the data is pretty good. As a consequence, we can use this precise mathematical function to predict the $Y$ from a particular observation $X$. For example, the average fuel consumption of at a speed 90 km/h is `8.512421 - 0.153291 * 90 + 0.001408 * 90^2=6.1210`.
```{r, fitparabola, echo = TRUE, fig.asp = 1, out.width = '70%', fig.pos = 'h!', cache = TRUE}
plot(x, y, xlab = "Speed", ylab = "Fuel consumption")
curve(8.512421 - 0.153291 * x + 0.001408 * x^2, add = TRUE, col = 2)
```

There are a number of generic issues and decisions to take when building and estimating regression models that are worth to highlight:

- The **prediction accuracy versus interpretability** trade-off. *Prediction accuracy* is key in any predictive model. However, some of them achieve this predictive accuracy in exchange of a clear *interpretability* of the model (the so-called *black boxes*). Interpretability is key in order to gain insights on the prediction process, to know which variables are most influential in $Y$, to be able to interpret the parameters of the model, and to translate the prediction process to non-experts. The models covered in these notes are clearly interpretable and hence make a sacrifice in terms of prediction accuracy at an exchange of a neat interpretability.

- **Model correctness versus model usefulness**. Correctness and usefulness are two very different concepts in modeling. The first refers to the model being *statistically* correct, this is, if the **assumptions** on which the model between $Y$ and $X_1,\ldots,X_p$ is built are satisfied. The second refers to the model being **useful for explaining or predicting** $Y$ from $X_1,\ldots,X_p$. Both dimensions are related (if the model is correct/useful, then *likely* it is useful/correct) but neither is implied by the other. For example, a regression model might be correct but useless if the variance of $\varepsilon$ is large (too much noise). And yet if the model is nor completely correct, it may give useful insights and predictions (but inference will be problematic!).

- **Flexibility versus simplicity**. *The best model* is the one which is very simple (low number of parameters), highly interpretable, and delivers great predictions. This is often unachievable in practice. What it can be achieved is *a good model*: the one that balances the simplicity with the prediction accuracy, which is often increased the more flexible the model is. However, **flexibility comes at a price**: more flexible (hence more complex) models use more parameters that need to be estimated from a finite amount of information -- the sample. This is problematic, as overly flexible models are more dependent on the sample of $Y$ and $X_1,\ldots,X_p$, and therefore do not estimate the true relation between $Y$ and $X_1,\ldots,X_p$. This phenomenon is called **over-fitting** and it can be avoided by splitting the dataset in two parts: the *training dataset* (used for estimating the model) and the *testing dataset* (used for evaluating its predictive performance). On the other hand, excessive simplicity (**under-fitting**) is also problematic, since the true relation between $Y$ and $X_1,\ldots,X_p$ may not be that simple. Therefore, a trade-off in the degree of flexibility has to be attained for having a good model. This is often referred as the **bias-variance trade-off** (low flexibility increases the bias of the fitted model, high flexibility increases the variance). An illustration of this point is given in Figure \@ref(fig:overfitting).

(ref:overfittingtitle) Illustration of over-fitting in polynomial regression. The left plot shows the training dataset and the right plot the testing dataset. Better fitting of the training data with a higher polynomial order does not imply better performance in new observations (prediction), but just an over-fitting of the available data with an overly-parametrized model (too flexible for the amount of information available). Reduction in the predictive error is only achieved with fits (in red) of polynomial degrees close to the true regression (in black). Application also available [here](https://ec2-35-177-34-200.eu-west-2.compute.amazonaws.com/over-fitting/).

```{r, overfitting, echo = FALSE, fig.cap = '(ref:overfittingtitle)', screenshot.alt = "images/screenshots/over-fitting.png", dev = 'png', cache = TRUE, fig.pos = 'h!', out.width = '90%'}
knitr::include_app('https://ec2-35-177-34-200.eu-west-2.compute.amazonaws.com/over-fitting/', height = '650px')
```

## General notation and background {#intro-notation}

We use capital letters to denote *random variables*, such as $X$, and lowercase, such as $x$, to denote deterministic values. For example $\mathbb{P}[X=x]$ means ``the probability that the random variable $X$ takes the particular value $x$''. In predictive modelling we are concerned about the prediction or explanation of a *response* $Y$ from a set of *predictors* $X_1,\ldots,X_p$. Both $Y$ and $X_1,\ldots,X_p$ are random variables, but we use them in a different way: our interest lays in prediction or explaining $Y$ *from* $X_1,\ldots,X_p$. Other name for $Y$ is *dependent variable* and $X_1,\ldots,X_p$ are sometimes referred as *independent variables*, *covariates*, or *explanatory variables*. We will not use these terminologies.

The *cumulative distribution function* (cdf) of a random variable $X$ is $F(x):=\mathbb{P}[X\leq x]$ and is a function that completely characterizes the randomness of $X$. Continuous random variables are also characterized by the *probability density function* (pdf) $f(x)=F'(x)$ (respectively, $F(x)=\int_{-\infty}^xf(t)\mathrm{d}t$), which represents the *infinitesimal relative probability* of $X$ per unit of length. On the other hand, discrete random variables are also characterized by the *probability mass function* $\mathbb{P}[X= x]$. We write $X\sim F$ (or $X\sim f$ if $X$ is continuous) to denote that $X$ has a cdf $F$ (or a pdf $f$). If two random variables $X$ and $Y$ have the same distribution, we write $X\stackrel{d}{=}Y$.

For a random variable $X\sim F$, the *expectation* of $g(X)$ is
\begin{align*}
\mathbb{E}[g(X)]:=&\,\int g(x)\mathrm{d}F(x)\\
:=&\,
\begin{cases}
\int g(x)f(x)\mathrm{d}x,&\text{ if }X\text{ is continuous,}\\\sum_{\{i:\mathbb{P}[X=x_i]>0\}} g(x_i)\mathbb{P}[X=x_i],&\text{ if }X\text{ is discrete.}
\end{cases}
\end{align*}
Unless otherwise stated, the integration limits of any integral are $\mathbb{R}$ or $\mathbb{R}^p$. The *variance* is defined as $\mathbb{V}\mathrm{ar}[X]:=\mathbb{E}[(X-\mathbb{E}[X])^2]=\mathbb{E}[X^2]-\mathbb{E}[X]^2$.

We employ boldface to denote vectors (assumed to be column matrices, although sometimes written in row-layout), like $\mathbf{a}$, and matrices, like $\mathbf{A}$. We denote by $\mathbf{A}'$ to the transpose of $\mathbf{A}$. Boldfaced capitals will be used simultaneously for denoting matrices and also *random vectors* $\mathbf{X}=(X_1,\ldots,X_p)$, which are collections random variables $X_1,\ldots,X_p$. The (joint) cdf of $\mathbf{X}$ is 
$$
F(\mathbf{x}):=\mathbb{P}[\mathbf{X}\leq \mathbf{x}]:=\mathbb{P}[X_1\leq x_1,\ldots,X_p\leq x_p]
$$
(understood as the probability that $X_1\leq x_1$ *and* $\ldots$ *and* $X_p\leq x_p$) and, if $\mathbf{X}$ is continuous, its (joint) pdf is $f:=\frac{\partial^p}{\partial x_1\cdots\partial x_p}F$. The *marginals* of $F$ and $f$ are the cdf and pdf of $X_j$, $j=1,\ldots,p$, respectively. They are defined as:
\begin{align*}
F_{X_j}(x_j)&:=\mathbb{P}[X_j\leq x]=\int_{\mathbb{R}^{p-1}} F(\mathbf{x})\mathrm{d}\mathbf{x}_{-i},\\
f_{X_j}(x_j)&:=\frac{\partial}{\partial x_j}F_{X_j}(x_j)=\int_{\mathbb{R}^{p-1}} f(\mathbf{x})\mathrm{d}\mathbf{x}_{-i},
\end{align*}
where $\mathbf{x}_{-i}:=(x_1,\ldots,x_{i-1},x_{i+1},x_p)$. The definitions can be extended analogously to the marginals of the cdf and pdf of different subsets of $\mathbf{X}$.

The *conditional* cdf and pdf of $X_1\vert(X_2,\ldots,X_p)$ are defined, respectively, as
\begin{align*}
F_{X_1\vert \mathbf{X}_{-1}=\mathbf{x}_{-1}}(x_1)&:=\mathbb{P}[X_1\leq x_1\vert \mathbf{X}_{-1}=\mathbf{x}_{-1}],\\
f_{X_1\vert \mathbf{X}_{-1}=\mathbf{x}_{-1}}(x_1)&:=\frac{f(\mathbf{x})}{f_{\mathbf{X}_{-1}}(\mathbf{x}_{-1})}.
\end{align*}
The *conditional expectation* of $Y\vert X$ is the following random variable^[Recall that the $X$-part is random!]
$$
\mathbb{E}[Y\vert X]:=\int y \mathrm{d}F_{Y\vert X}(y\vert X).
$$

For two random variables $X_1$ and $X_2$, the *covariance* between them is defined as
\[
\mathrm{Cov}[X_1,X_2]:=\mathbb{E}[(X_1-\mathbb{E}[X_1])(X_2-\mathbb{E}[X_2])]=\mathbb{E}[X_1X_2]-\mathbb{E}[X_1]\mathbb{E}[X_2],
\]
and the *correlation* between them is defined as 
\[
\mathrm{Cor}[X_1,X_2]:=\frac{\mathrm{Cov}[X_1,X_2]}{\sqrt{\mathbb{V}\mathrm{ar}[X_1]\mathbb{V}\mathrm{ar}[X_2]}}.
\]
The variance and covariance are extended to a random vector $\mathbf{X}=(X_1,\ldots,X_p)'$ by means of the so-called *variance-covariance matrix*:
\begin{align*}
\mathbb{V}\mathrm{ar}[\mathbf{X}]:=&\mathbb{E}[(\mathbf{X}-\mathbb{E}[\mathbf{X}])(\mathbf{X}-\mathbb{E}[\mathbf{X}])']=\mathbb{E}[\mathbf{X}\mathbf{X}']-\mathbb{E}[\mathbf{X}]\mathbb{E}[\mathbf{X}]'\\
=&
\begin{pmatrix}
\mathbb{V}\mathrm{ar}[X_1] & \mathrm{Cov}[X_1,X_2] & \cdots & \mathrm{Cov}[X_1,X_p]\\
\mathrm{Cov}[X_2,X_1] & \mathbb{V}\mathrm{ar}[X_2] & \cdots & \mathrm{Cov}[X_2,X_p]\\
\vdots & \vdots & \ddots & \vdots \\
\mathrm{Cov}[X_p,X_1] & \mathrm{Cov}[X_p,X_2] & \cdots & \mathbb{V}\mathrm{ar}[X_p]\\
\end{pmatrix},
\end{align*}
where $\mathbb{E}[\mathbf{X}]:=(\mathbb{E}[X_1],\ldots,\mathbb{E}[X_p])'$. As in the univariate case, the expectation is a linear operator, which means that
\begin{align}
\mathbb{E}[\mathbf{A}\mathbf{X}\mathbf{B}+\mathbf{C}]=\mathbf{A}\mathbb{E}[\mathbf{X}]\mathbf{B}+\mathbf{C},\quad\text{for matrices } \mathbf{A}, \mathbf{B}\text{, and }\mathbf{C}.(\#eq:linexp)
\end{align}

The *$p$-dimensional normal* of mean $\boldsymbol{\mu}\in\mathbb{R}^p$ and covariance matrix $\boldsymbol{\Sigma}$ (a $p\times p$ symmetric and positive definite matrix) is denoted by $\mathcal{N}_{p}(\boldsymbol{\mu},\boldsymbol{\Sigma})$ and is the generalization to $p$ random variables of usual the *normal* distribution. Its (joint) pdf is given by
$$
\phi(\mathbf{x};\boldsymbol{\mu},\boldsymbol{\Sigma}):=\frac{1}{(2\pi)^{p/2}|\boldsymbol{\Sigma}|^{1/2}}e^{-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})'\boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})},\quad \mathbf{x}\in\mathbb{R}^p.
$$
Notice that when $p=1$, and $\boldsymbol{\mu}=\mu$ and $\boldsymbol{\Sigma}=\sigma^2$, then the pdf of the usual normal is recovered: $\phi(x;\mu,\sigma):=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$. When $p=2$, the pdf is expressed in terms of $\boldsymbol{\mu}=(\mu_1,\mu_2)$ and $\boldsymbol{\Sigma}=(\sigma_1^2,\rho\sigma_1\sigma_2;\rho\sigma_1\sigma_2,\sigma_2^2)$, for $\mu_1,\mu_2\in\mathbb{R}$, $\sigma_1,\sigma_2>0$, and $-1<\rho<1$:
\begin{align}
\phi(x_1,x_2;\mu_1,\mu_2,\sigma_1,\sigma_2,\rho):=\frac{1}{2\pi\sigma_1\sigma_1\sqrt{1-\rho^2}}e^{-\frac{1}{2(1-\rho^2)}\left[\frac{(x_1-\mu_1)^2}{\sigma_1^2}+\frac{(x_2-\mu_2)^2}{\sigma_2^2}-\frac{2\rho(x_1-\mu_1)(x_2-\mu_2)}{\sigma_1\sigma_2}\right]}.(\#eq:norm2)
\end{align}
The surface defined by \@ref(eq:norm2) that can be regarded as a bell (in three dimensions). In addition, it serves to provide concrete examples of the functions introduced above:

- Joint pdf: $f(x_1,x_2)=\phi(x_1,x_2;\mu_1,\mu_2,\sigma_1,\sigma_2,\rho)$.
- Marginal pdfs: $f_{X_1}(x_1)=\int \phi(x_1,t_2;\mu_1,\mu_2,\sigma_1,\sigma_2,\rho)\mathrm{d}t_2=\phi(x_1;\mu_1,\sigma_1)$ and $\phi(x_2;\mu_2,\sigma_2)$. Hence $X_1\sim\mathcal{N}(\mu_1,\sigma_1)$ and $X_2\sim\mathcal{N}(\mu_2,\sigma_2)$.
- Conditional pdfs: $f_{X_1\vert X_2=x_2}(x_1)=\frac{f(x_1,x_2)}{f_{X_2}(x_2)}=\phi\left(x_1;\mu_1+\rho\frac{\sigma_1}{\sigma_2}(x_2-\mu_2),(1-\rho^2)\sigma_1^2\right)$ and $\phi\left(x_2;\mu_2+\rho\frac{\sigma_2}{\sigma_1}(x_1-\mu_1),(1-\rho^2)\sigma_2^2\right)$. Hence 
\begin{align*}
X_1&\vert X_2=x_2\sim\mathcal{N}\left(\mu_1+\rho\frac{\sigma_1}{\sigma_2}(x_2-\mu_2),(1-\rho^2)\sigma_1^2\right),\\
X_2&\vert X_1=x_1\sim\mathcal{N}\left(\mu_2+\rho\frac{\sigma_2}{\sigma_1}(x_1-\mu_1),(1-\rho^2)\sigma_2^2\right).
\end{align*}
- Conditional expectations: $\mathbb{E}[X_1|X_2=x_2]=\mu_1+\rho\frac{\sigma_1}{\sigma_2}(x_2-\mu_2)$ and $\mathbb{E}[X_2|X_1=x_1]=\mu_2+\rho\frac{\sigma_2}{\sigma_1}(x_1-\mu_1)$.
- Joint cdf: $\int_{-\infty}^{x_2}\int_{-\infty}^{x_1}\phi(t_1,t_2;\mu_1,\mu_2,\sigma_1,\sigma_2,\rho)\mathrm{d}t_1\mathrm{d}t_2$.
- Marginal cdfs: $\int_{-\infty}^{x_1}\phi(t;\mu_1,\sigma_1)\mathrm{d}t=:\Phi(x_1;\mu_1,\sigma_1)$ and $\Phi(x_2;\mu_2,\sigma_2)$.
- Conditional cdfs: $\int_{-\infty}^{x_1}\phi(t;\mu_1,\sigma_1)\mathrm{d}t=\Phi\left(x_1;\mu_1+\rho\frac{\sigma_1}{\sigma_2}(x_2-\mu_2),(1-\rho^2)\sigma_1^2\right)$ and $\Phi\left(x_2;\mu_2+\rho\frac{\sigma_2}{\sigma_1}(x_1-\mu_1),(1-\rho^2)\sigma_2^2\right)$.

(ref:norm2title) Visualization of the joint pdf (in blue), marginal pdfs (green), conditional pdf of $X_2\vert X_1=x_1$ (orange), expectation (red point), and conditional expectation $\mathbb{E}[X_2\vert X_1=x_1]$ (orange point) of a $2$-dimensional normal. The conditioning point of $X_1$ is $x_1=-2$. Note the different scales of the densities, as they have to integrate one over different supports. Note how the conditional density (upper orange curve) is *not* the joint pdf $f(2,x_2)$ (lower orange curve) but a rescaling of this curve by $\frac{1}{f_{X_1}(x_1)}$. The parameters of the $2$-dimensional normal are $\mu_1=\mu_2=0$, $\sigma_1=\sigma_2=1$ and $\rho=0.75$. $500$ observations sampled from the distribution are shown in black.

```{r, norm2, echo = FALSE, out.width = '70%', fig.pos = 'h!', fig.cap = '(ref:norm2title)', cache = TRUE}
knitr::include_graphics("images/R/norm2.png")
```

In the predictive models we will consider an *independent and identically distributed* (iid) sample of the response and the predictors. We use the following notation: $Y_i$ is the $i$-th observation of the response $Y$ and $X_{ij}$ represents the $i$-th observation of the $j$-th predictor $X_j$.

## Datasets for the course {#intro-datasets}

This is a (constantly updated) handy list of all the relevant datasets used in the course. To download them, simply **save the link as a file** in your browser.

- `wine.csv` ([download](https://raw.githubusercontent.com/egarpor/PM-UC3M/master/datasets/wine.csv)). The dataset is formed by the auction `Price` of 27 red Bordeaux vintages, five vintage descriptors (`WinterRain`, `AGST`, `HarvestRain`, `Age`, `Year`), and the population of France in the year of the vintage, `FrancePop`.

- `least-squares.RData` ([download](https://raw.githubusercontent.com/egarpor/PM-UC3M/master/datasets/least-squares.RData)). Contains a single `data.frame`, named `leastSquares`, with 50 observations of the variables `x`, `yLin`, `yQua`, and `yExp`. These are generated as $X\sim\mathcal{N}(0,1)$, $Y_\mathrm{lin}=-0.5+1.5X+\varepsilon$, $Y_\mathrm{qua}=-0.5+1.5X^2+\varepsilon$, and $Y_\mathrm{exp}=-0.5+1.5\cdot2^X+\varepsilon$, with $\varepsilon\sim\mathcal{N}(0,0.5^2)$. The purpose of the dataset is to illustrate the least squares fitting.

- `least-squares-3D.RData` ([download](https://raw.githubusercontent.com/egarpor/PM-UC3M/master/datasets/least-squares-3D.RData)). Contains a single `data.frame`, named `leastSquares3D`, with 50 observations of the variables `x1`, `x2`, `x3`, `yLin`, `yQua`, and `yExp`. These are generated as $X_1,X_2\sim\mathcal{N}(0,1)$, $X_3=X_1+\mathcal{N}(0,0.05^2)$, $Y_\mathrm{lin}=-0.5 + 0.5 X_1 + 0.5 X_2 +\varepsilon$, $Y_\mathrm{qua}=-0.5 + X_1^2 + 0.5 X_2+\varepsilon$, and $Y_\mathrm{exp}=-0.5 + 0.5 e^{X_2} + X_3+\varepsilon$, with $\varepsilon\sim\mathcal{N}(0,1)$. The purpose of the dataset is to illustrate the least squares fitting with several predictors.

- `assumptions.RData` ([download](https://raw.githubusercontent.com/egarpor/PM-UC3M/master/datasets/assumptions.RData)). Contains the data frame `assumptions` with 200 observations of the variables `x1`, ..., `x9` and `y1`, ..., `y9`. The purpose of the dataset is to identify which regression `y1 ~ x1`, ..., `y9 ~ x9` fulfills the assumptions of the linear model. The dataset `moreAssumptions.RData` ([download](https://raw.githubusercontent.com/egarpor/PM-UC3M/master/datasets/moreAssumptions.RData)) has the same structure.

- `assumptions3D.RData` ([download](https://raw.githubusercontent.com/egarpor/PM-UC3M/master/datasets/assumptions3D.RData)). Contains the data frame `assumptions3D` with 200 observations of the variables `x1.1`, ..., `x1.8`, `x2.1`, ..., `x2.8` and `y.1`, ..., `y.8`. The purpose of the dataset is to identify which regression `y.1 ~ x1.1 + x2.1`, ..., `y.8 ~ x1.8 + x2.8` fulfills the assumptions of the linear model.

- `Boston.xlsx` ([download](https://raw.githubusercontent.com/egarpor/PM-UC3M/master/datasets/Boston.xlsx)). The dataset contains 14 variables describing 506 suburbs in Boston. Among those variables, `medv` is the median house value, `rm` is the average number of rooms per house and `crim` is the per capita crime rate. The full description is available in `?Boston`.

- `cpus.txt` ([download](https://raw.githubusercontent.com/egarpor/PM-UC3M/master/datasets/cpus.txt)) and `gpus.txt` ([download](https://raw.githubusercontent.com/egarpor/PM-UC3M/master/datasets/gpus.txt)). The datasets contain 102 and 35 rows, respectively, of commercial CPUs and GPUs appeared since the first models up to nowadays. The variables in the datasets are `Processor`, `Transistor count`, `Date of introduction`, `Manufacturer`, `Process`, and `Area`.

- `la-liga-2015-2016.xlsx` ([download](https://raw.githubusercontent.com/egarpor/PM-UC3M/master/datasets/la-liga-2015-2016.xlsx)). Contains 19 performance metrics for the 20 football teams in La Liga 2015/2016.

<!-- - `challenger.txt` ([download](https://raw.githubusercontent.com/egarpor/PM-UC3M/master/datasets/challenger.txt)). Contains data for 23 Space-Shuttle launches. The data consists of 23 shuttle flights. There are 8 variables. Among them: `temp` (the temperature in Celsius degrees at the time of launch), and `fail.field` and `fail.nozzle` (indicators of whether there were an incidents in the O-rings of the field joints and nozzles of the solid rocket boosters). -->

<!-- - `Chile.txt` ([download](https://raw.githubusercontent.com/egarpor/PM-UC3M/master/datasets/Chile.txt)). Contains data for 2700 respondents on a survey for the voting intentions in the 1988 Chilean national plebiscite. There are 8 variables: `region`, `population`, `sex`, `age`, `education`, `income`, `statusquo` (scale of support for the status quo), and `vote`. `vote` is a factor with levels `A` (abstention), `N` (against Pinochet), `U` (undecided), and `Y` (for Pinochet). Available in `R` through the package `car` and `data(Chile)`. -->

## Main references and credits {#intro-credits}

Several great reference books have been used for preparing these notes. The following list details the sections in which each of them has been consulted (*note: list to be updated*):

- @Kuhn2013 <!-- (Sections \@ref(intro-what-is)) -->
- @James2013 <!-- (Sections \@ref(lm-i-model) - \@ref(lm-i-modfit), \@ref(lm-ii-lab-boston)) -->
- @Fan1996 <!-- (Sections \@ref(reg-kre), \@ref(reg-asymp), \@ref(reg-bwd)). -->
- @Pena2002 <!--(Sections \@ref(lm-i-model) - \@ref(lm-i-modfit)) -->
- @Loader1999 <!-- (Section \@ref(reg-loclik)). -->
- @Wand1995 <!-- (Sections \@ref(reg-loclik)). -->
- @Wasserman2004 <!-- (Sections \@ref(reg-param), \@ref(reg-loclik)). -->
- @Wasserman2006 <!-- (Sections \@ref(reg-bwd)). -->

In addition, these notes are possible due to the existence of these incredible pieces of software: @R-bookdown, @R-knitr, @R-rmarkdown, and @R-base.

The icons used in the notes were designed by [madebyoliver](http://www.flaticon.com/authors/madebyoliver), [freepik](http://www.flaticon.com/authors/freepik), and [roundicons](http://www.flaticon.com/authors/roundicons) from [Flaticon](http://www.flaticon.com/).

All material in these notes is licensed under [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/).
