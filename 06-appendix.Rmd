# (APPENDIX) Appendix {-}

# Installation of `R` and `RStudio` {#app-install}

This is what you have to do in order to install `R` and `RStudio` in your own computer:

1. In Mac OS X, download and install first [`XQuartz`](https://www.xquartz.org/) and log out and back on your Mac OS X account (this is an **important** step that is required for 3D graphics to work). Be sure that your Mac OS X system is up-to-date.
2. Download the latest version of `R` at <https://cran.r-project.org/>. For Windows, you can download it directly [here](https://cran.r-project.org/bin/windows/base/release.html). For Mac OS X you can download the latest version (at the time of writing this, `3.4.1`) [here](https://cran.r-project.org/bin/macosx/R-3.4.1.pkg).
3. Install `R`. In Windows, be sure to select the `'Startup options'` and then choose `'SDI'` in the `'Display Mode'` options. Leave the rest of installation options as default.
4. Download the latest version of `RStudio` for your system at <https://www.rstudio.com/products/rstudio/download/#download> and install it.

If there is any Linux user, kindly follow the corresponding instructions [here](https://cran.r-project.org/) for installing `R`,  [download](https://www.rstudio.com/products/rstudio/download/#download) `RStudio` (only certain `Ubuntu` and `Fedora` versions are supported), and install it using your package manager.

# Introduction to `RStudio` {#app-introRStudio}

`RStudio` is the most employed Integrated Development Environment (IDE) for `R` nowadays. When you start `RStudio` you will see a window similar to Figure \@ref(fig:rstudio). There are a lot of items in the GUI, most of them described in the [`RStudio` IDE Cheat Sheet](https://www.rstudio.com/wp-content/uploads/2016/01/rstudio-IDE-cheatsheet.pdf). The most important things to keep in mind are:

1. The code is written in scripts in the *source panel* (upper-right panel in Figure \@ref(fig:rstudio));
2. for running a line or code selection from the script in the *console* (first tab in the lower-right panel in Figure \@ref(fig:rstudio)), you do it with the keyboard shortcut `'Ctrl+Enter'` (Windows and Linux) or `'Cmd+Enter'` (Mac OS X).

```{r, rstudio, echo = FALSE, fig.pos = 'h!', out.width = '70%', fig.cap = 'Main window of `RStudio`. The red shows the code panel and the yellow shows the console output. Extracted from [here](https://www.rstudio.com/wp-content/uploads/2016/01/rstudio-IDE-cheatsheet.pdf).', cache = TRUE}
knitr::include_graphics("images/figures/RStudio.png")
```

# Introduction to `R` {#app-introR}

This section provides a collection of self-explainable snippets of the programming language `R` [@R-base] that show the very basics of the language. It is not meant to be an exhaustive introduction to `R`, but rather a reminder/panoramic of a collection of basic functions and methods.

In the following, `#` denotes comments to the code and `##` outputs of the code.

```{r, r-1, echo = FALSE, eval = TRUE, cache = TRUE}
rm(list = ls())
```

#### Simple computations {-}

```{r, r-2, echo = TRUE, error = TRUE, collapse = TRUE, cache = TRUE}
# The console can act as a simple calculator
1.0 + 1.1
2 * 2
3/2
2^3
1/0
0/0

# Use ";" for performing several operations in the same line
(1 + 3) * 2 - 1; 3 + 2

# Elemental mathematical functions
sqrt(2); 2^0.5
exp(1)
log(10) # Neperian logarithm
log10(10); log2(10) # Logs in base 10 and 2
sin(pi); cos(0); asin(0)
tan(pi/3)
sqrt(-1)
```

```{r, r-3, echo = TRUE, error = TRUE, collapse = TRUE}
# Remember to close the parenthesis
1 +
(1 + 3
```

```{block, exsimp, type = 'rmdexercise', cache = TRUE}
Compute:

- $\frac{e^{2}+\sin(2)}{\cos^{-1}\left(\tfrac{1}{2}\right)+2}$. *Answer*: `2.723274`.
- $\sqrt{3^{2.5}+\log(10)}$. *Answer*: `4.22978`.
- $(2^{0.93}-\log_2(3 + \sqrt{2+\sin(1)}))10^{\tan(1/3))}\sqrt{3^{2.5}+\log(10)}$. *Answer*: `-3.032108`.

```

#### Variables and assignment {-}

```{r, r-4, echo = TRUE, error = TRUE, collapse = TRUE, cache = TRUE}
# Any operation that you perform in R can be stored in a variable (or object)
# with the assignment operator "<-"
x <- 1

# To see the value of a variable, simply type it
x

# A variable can be overwritten
x <- 1 + 1

# Now the value of x is 2 and not 1, as before
x

# Careful with capitalization
X

# Different
X <- 3
x; X

# The variables are stored in your workspace (a .RData file)
# A handy tip to see what variables are in the workspace
ls()
# Now you know which variables can be accessed!

# Remove variables
rm(X)
X
```

```{block, exvars, type = 'rmdexercise', cache = TRUE}
Do the following:

- Store $-123$ in the variable `y`.
- Store the log of the square of `y` in `z`.
- Store $\frac{y-z}{y+z^2}$ in `y` and remove `z`.
- Output the value of `y`. *Answer*: `4.366734`.

```

#### Vectors {-}

```{r, r-5, echo = TRUE, collapse = TRUE, error = TRUE, cache = TRUE}
# These are vectors - arrays of numbers
# We combine numbers with the function "c"
c(1, 3)
c(1.5, 0, 5, -3.4)

# A handy way of creating integer sequences is the operator ":"
# Sequence from 1 to 5
1:5

# Storing some vectors
myData <- c(1, 2)
myData2 <- c(-4.12, 0, 1.1, 1, 3, 4)
myData
myData2

# Entrywise operations
myData + 1
myData^2

# If you want to access a position of a vector, use [position]
myData[1]
myData2[6]

# You also can change elements
myData[1] <- 0
myData

# Think on what you want to access...
myData2[7]
myData2[0]

# If you want to access all the elements except a position, use [-position]
myData2[-1]
myData2[-2]

# Also with vectors as indexes
myData2[1:2]
myData2[myData]

# And also
myData2[-c(1, 2)]

# But do not mix positive and negative indexes!
myData2[c(-1, 2)]

# Remove the first element
myData2 <- myData2[-1]
```

```{block, exvec, type = 'rmdexercise', cache = TRUE}
Do the following:

- Create the vector $x=(1, 7, 3, 4)$.
- Create the vector $y=(100, 99, 98, ..., 2, 1)$.
- Create the vector $z=c(4, 8, 16, 32, 96)$.
- Compute $x_2+y_4$ and $\cos(x_3) + \sin(x_2) e^{-y_2}$. *Answers*: `104` and `-0.9899925`.
- Set $x_{2}=0$ and $y_{2}=-1$. Recompute the previous expressions. *Answers*: `97` and `2.785875`.
- Index $y$ by $x+1$ and store it as `z`. What is the output? *Answer*: `z` is `c(-1, 100, 97, 96)`.

```

#### Some functions {-}

```{r, r-6, echo = TRUE, collapse = TRUE, error = TRUE, cache = TRUE}
# Functions take arguments between parenthesis and transform them into an output
sum(myData)
prod(myData)

# Summary of an object
summary(myData)

# Length of the vector
length(myData)

# Mean, standard deviation, variance, covariance, correlation
mean(myData)
var(myData)
cov(myData, myData^2)
cor(myData, myData * 2)
quantile(myData)

# Maximum and minimum of vectors
min(myData)
which.min(myData)

# Usually the functions have several arguments, which are set by "argument = value"
# In this case, the second argument is a logical flag to indicate the kind of sorting
sort(myData) # If nothing is specified, decreasing = FALSE is assumed
sort(myData, decreasing = TRUE)

# Do not know what are the arguments of a function? Use args and help!
args(mean)
?mean
```

```{block, exfuncs-1, type = 'rmdexercise', cache = TRUE}
Do the following:

- Compute the mean, median and variance of $y$. *Answers*: `49.5`, `49.5`, `843.6869`.
- Do the same for $y+1$. What are the differences?
- What is the maximum of $y$? Where is it placed?
- Sort $y$ increasingly and obtain the 5th and 76th positions. *Answer*: `c(4,75)`.
- Compute the covariance between $y$ and $y$. Compute the variance of $y$. Why do you get the same result?

```

#### Matrices, data frames, and lists {-}

```{r, r-7, collapse = TRUE, error = TRUE, cache = TRUE}
# A matrix is an array of vectors
A <- matrix(1:4, nrow = 2, ncol = 2)
A

# Another matrix
B <- matrix(1, nrow = 2, ncol = 2, byrow = TRUE)
B

# Matrix is a vector with dimension attributes
dim(A)

# Binding by rows or columns
rbind(1:3, 4:6)
cbind(1:3, 4:6)

# Entrywise operations
A + 1
A * B

# Accessing elements
A[2, 1] # Element (2, 1)
A[1, ] # First row - this is a vector
A[, 2] # First column - this is a vector

# Obtain rows and columns as matrices (and not as vectors)
A[1, , drop = FALSE]
A[, 2, drop = FALSE]

# Matrix transpose
t(A)

# Matrix multiplication
A %*% B
A %*% B[, 1]
A %*% B[1, ]

# Care is needed
A %*% B[1, , drop = FALSE] # Incompatible product

# Compute inverses with "solve"
solve(A) %*% A

# A data frame is a matrix with column names
# Useful when you have multiple variables
myDf <- data.frame(var1 = 1:2, var2 = 3:4)
myDf

# You can change names
names(myDf) <- c("newname1", "newname2")
myDf

# The nice thing is that you can access variables by its name with
# the "$" operator
myDf$newname1

# And create new variables also (it has to be of the same
# length as the rest of variables)
myDf$myNewVariable <- c(0, 1)
myDf

# A list is a collection of arbitrary variables
myList <- list(myData = myData, A = A, myDf = myDf)

# Access elements by names
myList$myData
myList$A
myList$myDf

# Reveal the structure of an object
str(myList)
str(myDf)

# A less lengthy output
names(myList)
```

```{block, exmats, type = 'rmdexercise', cache = TRUE}
Do the following:

- Create a matrix called `M` with rows given by `y[3:5]`, `y[3:5]^2` and `log(y[3:5])`.
- Create a data frame called `myDataFrame` with column names "y", "y2" and "logy" containing the vectors `y[3:5]`, `y[3:5]^2` and `log(y[3:5])`, respectively.
- Create a list, called `l`, with entries for `x` and `M`. Access the elements by their names.
- Compute the squares of `myDataFrame` and save the result as `myDataFrame2`.
- Compute the log of the sum of `myDataFrame` and `myDataFrame2`. *Answer*:

          ##         y       y2     logy
          ## 1 9.180087 18.33997 3.242862
          ## 2 9.159678 18.29895 3.238784
          ## 3 9.139059 18.25750 3.234656

```

#### More on data frames {-}

```{r, r-8, echo = TRUE, collapse = TRUE, error = TRUE, cache = TRUE}
# The iris dataset is already imported in R
# (beware: locfit has also an iris dataset, with different names and shorter)
# data(iris)

# The beginning of the data
head(iris)

# "names" gives you the variables in the data frame
names(iris)

# So we can access variables by "$" or as in a matrix
iris$Sepal.Length[1:10]
iris[1:10, 1]
iris[3, 1]

# Information on the dimension of the data frame
dim(iris)

# "str" gives the structure of any object in R
str(iris)

# Recall the species variable: it is a categorical variable (or factor),
# not a numeric variable
iris$Species[1:10]

# Factors can only take certain values
levels(iris$Species)

# If a file contains a variable with character strings as observations (either
# encapsulated by quotation marks or not), the variable will become a factor
# when imported into R
```

```{block, exdf, type = 'rmdexercise', cache = TRUE}
Do the following:

- Load the `faithful` dataset into `R`.
- Get the dimensions of `faithful` and show beginning of the data.
- Retrieve the fifth observation of `eruptions` in two different ways.
- Obtain a summary of `waiting`.

```

#### Vector-related functions {-}

```{r, r-9, echo = TRUE, collapse = TRUE, error = TRUE, cache = TRUE}
# The function "seq" creates sequences of numbers equally separated
seq(0, 1, by = 0.1)
seq(0, 1, length.out = 5)

# You can short the latter argument
seq(0, 1, l = 5)

# Repeat number
rep(0, 5)

# Reverse a vector
myVec <- c(1:5, -1:3)
rev(myVec)

# Another way
myVec[length(myVec):1]

# Count repetitions in your data
table(iris$Sepal.Length)
table(iris$Species)
```

```{block, exvecfun, type = 'rmdexercise', cache = TRUE}
Do the following:

- Create the vector $x=(0.3, 0.6, 0.9, 1.2)$.
- Create a vector of length 100 ranging from $0$ to $1$ with entries equally separated.
- Compute the amount of zeros and ones in `x <- c(0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0)`. Check that they are the same as in `rev(x)`.
- Compute the vector $(0.1, 1.1, 2.1, ..., 100.1)$ in four different ways using `seq` and `rev`. Do the same but using `:` instead of `seq`. (*Hint*: add `0.1`)

```

#### Logical conditions and subsetting {-}

```{r, r-10, echo = TRUE, collapse = TRUE, error = TRUE, cache = TRUE}
# Relational operators: x < y, x > y, x <= y, x >= y, x == y, x!= y
# They return TRUE or FALSE

# Smaller than
0 < 1

# Greater than
1 > 1

# Greater or equal to
1 >= 1 # Remember: ">="" and not "=>"" !

# Smaller or equal to
2 <= 1 # Remember: "<="" and not "=<"" !

# Equal
1 == 1 # Tests equality. Remember: "=="" and not "="" !

# Unequal
1 != 0 # Tests iequality

# TRUE is encoded as 1 and FALSE as 0
TRUE + 1
FALSE + 1

# In a vector-like fashion
x <- 1:5
y <- c(0, 3, 1, 5, 2)
x < y
x == y
x != y

# Subsetting of vectors
x
x[x >= 2]
x[x < 3]

# Easy way of work with parts of the data
data <- data.frame(x = c(0, 1, 3, 3, 0), y = 1:5)
data

# Data such that x is zero
data0 <- data[data$x == 0, ]
data0

# Data such that x is larger than 2
data2 <- data[data$x > 2, ]
data2

# In an example
iris$Sepal.Width[iris$Sepal.Width > 3]

# Problem - what happened?
data[x > 2, ]

# In an example
summary(iris)
summary(iris[iris$Sepal.Width > 3, ])

# On the factor variable only makes sense == and !=
summary(iris[iris$Species == "setosa", ])

# Subset argument in lm
lm(Sepal.Width ~ Petal.Length, data = iris, subset = Sepal.Width > 3)
lm(Sepal.Width ~ Petal.Length, data = iris, subset = iris$Sepal.Width > 3)
# Both iris$Sepal.Width and Sepal.Width in subset are fine: data = iris
# tells R to look for Sepal.Width in the iris dataset

# AND operator "&"
TRUE & TRUE
TRUE & FALSE
FALSE & FALSE

# OR operator "|"
TRUE | TRUE
TRUE | FALSE
FALSE | FALSE

# Both operators are useful for checking for ranges of data
y
index1 <- (y <= 3) & (y > 0)
y[index1]
index2 <- (y < 2) | (y > 4)
y[index2]

# In an example
summary(iris[iris$Sepal.Width > 3 & iris$Sepal.Width < 3.5, ])
```

```{block, exlogical, type = 'rmdexercise', cache = TRUE}
Do the following for the `iris` dataset:

- Compute the subset corresponding to `Petal.Length` either smaller than `1.5` or larger than `2`. Save this dataset as `irisPetal`.
- Compute and summarize a linear regression of `Sepal.Width` into `Petal.Width + Petal.Length` for the dataset `irisPetal`. What is the $R^2$? *Solution*: `0.101`.
- Check that the previous model is the same as regressing `Sepal.Width` into `Petal.Width + Petal.Length` for the dataset `iris` with the appropriate `subset` expression.
- Compute the variance for `Petal.Width` when `Petal.Width` is smaller or equal that `1.5` and larger than `0.3`. *Solution*: `0.1266541`.

```

#### Plotting functions {-}

```{r, r-11, echo = TRUE, collapse = TRUE, error = TRUE, cache = TRUE, out.width = '70%', fig.pos = 'h!'}
# "plot" is the main function for plotting in R
# It has a different behaviour depending on the kind of object that it receives

# How to plot some data
plot(iris$Sepal.Length, iris$Sepal.Width, main = "Sepal.Length vs Sepal.Width")

# Alrernatively
plot(iris[, 1:2], main = "Sepal.Length vs Sepal.Width")

# Change the axis limits
plot(iris$Sepal.Length, iris$Sepal.Width, xlim = c(0, 10), ylim = c(0, 10))

# How to plot a curve (a parabola)
x <- seq(-1, 1, l = 50)
y <- x^2
plot(x, y)
plot(x, y, main = "A dotted parabola")
plot(x, y, main = "A parabola", type = "l")
plot(x, y, main = "A red and thick parabola", type = "l", col = "red", lwd = 3)

# Plotting a more complicated curve between -pi and pi
x <- seq(-pi, pi, l = 50)
y <- (2 + sin(10 * x)) * x^2
plot(x, y, type = "l") # Kind of rough...

# Remember that we are joining points for creating a curve!
# More detailed plot
x <- seq(-pi, pi, l = 500)
y <- (2 + sin(10 * x)) * x^2
plot(x, y, type = "l")

# For more options in the plot customization see
?plot
?par

# "plot" is a first level plotting function. That means that whenever is called,
# it creates a new plot. If we want to add information to an existing plot, we
# have to use a second level plotting function such as "points", "lines" or "abline"

plot(x, y) # Create a plot
lines(x, x^2, col = "red") # Add lines
points(x, y + 10, col = "blue") # Add points
abline(a = 5, b = 1, col = "orange", lwd = 2) # Add a straight line y = a + b * x
```

```{block, explots, type = 'rmdexercise', cache = TRUE}
Do the following:

- Plot the `faithful` dataset.
- Add the straight line $y=110-15x$ (red).
- Make a new plot for the function $y=\sin(x)$ (black). Add $y=\sin(2x)$ (red), $y=\sin(3x)$ (blue), and $y=\sin(4x)$ (orange).

```

#### Distributions {-}

```{r, r-12, echo = TRUE, collapse = TRUE, error = TRUE, cache = TRUE, out.width = '70%', fig.pos = 'h!'}
# R allows to sample [r], compute density/probability mass functions [d],
# compute distribution function [p], and compute quantiles [q] for several
# continuous and discrete distributions. The format employed is [rdpq]name,
# where name stands for:
# - norm -> Normal
# - unif -> Uniform
# - exp -> Exponential
# - t -> Student's t
# - f -> Snedecor's F
# - chisq -> Chi squared
# - pois -> Poisson
# - binom -> Binomial
# More distributions:
?Distributions

# Sampling from a Normal - 100 random points from a N(0, 1)
rnorm(n = 10, mean = 0, sd = 1)

# If you want to have always the same result, set the seed of the random number
# generator
set.seed(45678)
rnorm(n = 10, mean = 0, sd = 1)

# Plotting the density of a N(0, 1) - the Gauss bell
x <- seq(-4, 4, l = 100)
y <- dnorm(x = x, mean = 0, sd = 1)
plot(x, y, type = "l")

# Plotting the distribution function of a N(0, 1)
x <- seq(-4, 4, l = 100)
y <- pnorm(q = x, mean = 0, sd = 1)
plot(x, y, type = "l")

# Computing the 95% quantile for a N(0, 1)
qnorm(p = 0.95, mean = 0, sd = 1)

# All distributions have the same syntax: rname(n,...), dname(x,...), dname(p,...)  
# and qname(p,...), but the parameters in ... change. Look them in ?Distributions
# For example, here is que same for the uniform distribution

# Sampling from a U(0, 1)
set.seed(45678)
runif(n = 10, min = 0, max = 1)

# Plotting the density of a U(0, 1)
x <- seq(-2, 2, l = 100)
y <- dunif(x = x, min = 0, max = 1)
plot(x, y, type = "l")

# Computing the 95% quantile for a U(0, 1)
qunif(p = 0.95, min = 0, max = 1)

# Sampling from a Bi(10, 0.5)
set.seed(45678)
samp <- rbinom(n = 200, size = 10, prob = 0.5)
table(samp) / 200

# Plotting the probability mass of a Bi(10, 0.5)
x <- 0:10
y <- dbinom(x = x, size = 10, prob = 0.5)
plot(x, y, type = "h") # Vertical bars

# Plotting the distribution function of a Bi(10, 0.5)
x <- 0:10
y <- pbinom(q = x, size = 10, prob = 0.5)
plot(x, y, type = "h")
```

```{block, exdistribs, type = 'rmdexercise', cache = TRUE}
Do the following:

- Compute the $90\%$, $95\%$ and $99\%$ quantiles of a $F$ distribution with `df1 = 1` and `df2 = 5`. *Answer*: `c(4.060420, 6.607891, 16.258177)`.
- Plot the distribution function of a $U(0,1)$. Does it make sense with its density function?
- Sample $100$ points from a Poisson with `lambda = 5`.
- Sample $100$ points from a $U(-1,1)$ and compute its mean.
- Plot the density of a $t$ distribution with `df = 1` (use a sequence spanning from `-4` to `4`). Add lines of different colors with the densities for `df = 5`, `df = 10`, `df = 50` and `df = 100`. Do you see any pattern?

```

#### Functions {-}

```{r, r-13, echo = TRUE, collapse = TRUE, error = TRUE, cache = TRUE, out.width = '70%', fig.pos = 'h!'}
# A function is a way of encapsulating a block of code so it can be reused easily
# They are useful for simplifying repetitive tasks and organize the analysis

# This is a silly function that takes x and y and returns its sum
# Note the use of "return" to indicate what should be returned
add <- function(x, y) {
  z <- x + y
  return(z)
}

# Calling add - you need to run the definition of the function first!
add(x = 1, y = 2)
add(1, 1) # Arguments names can be omitted

# A more complex function: computes a linear model and its posterior summary.
# Saves us a few keystrokes when computing a lm and a summary
lmSummary <- function(formula, data) {
  model <- lm(formula = formula, data = data)
  summary(model)
}
# If no return(), the function returns the value of the last expression

# Usage
lmSummary(Sepal.Length ~ Petal.Width, iris)

# Recall: there is no variable called model in the workspace.
# The function works on its own workspace!
model

# Add a line to a plot
addLine <- function(x, beta0, beta1) {
  lines(x, beta0 + beta1 * x, lwd = 2, col = 2)
}

# Usage
plot(x, y)
addLine(x, beta0 = 0.1, beta1 = 0)

# The function "sapply" allows to sequentially apply a function
sapply(1:10, sqrt)
sqrt(1:10) # The same

# The advantage of "sapply" is that you can use with any function
myFun <- function(x) c(x, x^2)
sapply(1:10, myFun) # Returns a 2 x 10 matrix

# "sapply" is usefuf for plotting non-vectorized functions
sumSeries <- function(n) sum(1:n)
plot(1:10, sapply(1:10, sumSeries), type = "l")

# "apply" applies iteratively a function to rows (1) or columns (2)
# of a matrix or data frame
A <- matrix(1:10, nrow = 5, ncol = 2)
A
apply(A, 1, sum) # Applies the function by rows
apply(A, 2, sum) # By columns

# With other functions
apply(A, 1, sqrt)
apply(A, 2, function(x) x^2)
```

```{block, exfuncs-2, type = 'rmdexercise', cache = TRUE}
Do the following:

- Create a function that takes as argument $n$ and returns the value of $\sum_{i=1}^n i^2$.
- Create a function that takes as input the argument $N$ and then plots the curve $(n, \sum_{i=1}^n \sqrt{i})$ for $n=1,\ldots,N$. *Hint*: use `sapply`.

```

#### Control structures {-}

```{r, r-14, echo = TRUE, collapse = TRUE, error = TRUE, cache = TRUE}
# The "for" statement allows to create loops that run along a given vector
# Prints 5 times a message (i varies in 1:5)
for (i in 1:5) {
  print(i)
}

# Another example
a <- 0
for (i in 1:3) {
  a <- i + a
}
a

# Nested loops are possible
A <- matrix(0, nrow = 2, ncol = 3)
for (i in 1:2) {
  for (j in 1:3) {
    A[i, j] <- i + j
  }
}

# The "if" statement allows to create conditional structures of the forms:
# if (condition) {
#  # Something
# } else {
#  # Something else
# }
# These structures are thought to be inside functions

# Is the number positive?
isPositive <- function(x) {
  if (x > 0) {
    print("Positive")
  } else {
    print("Not positive")
  }
}
isPositive(1)
isPositive(-1)

# A loop can be interrupted with the "break" statement
# Stop when x is above 100
x <- 1
for (i in 1:1000) {
  x <- (x + 0.01) * x
  print(x)
  if (x > 100) {
    break
  }
}
```

```{block, excontr, type = 'rmdexercise', cache = TRUE}
Do the following:

- Compute $\mathbf{C}_{n\times k}$ in $\mathbf{C}_{n\times k}=\mathbf{A}_{n\times m} \mathbf{B}_{m\times k}$ from $\mathbf{A}$ and $\mathbf{B}$. Use that $c_{i,j}=\sum_{l=1}^ma_{i,l}b_{l,j}$. Test the implementation with simple examples.
- Create a function that samples a $\mathcal{N}(0,1)$ and returns the first sampled point that is larger than $4$.
- Create a function that simulates $N$ samples from the distribution of $\max(X_1,\ldots,X_n)$ where $X_1,\ldots,X_n$ are iid $\mathcal{U}(0,1)$.

```

# Review on hypothesis testing {#app-ht}

The process of hypothesis testing has an interesting analogy with a trial that helps on understanding the elements present in a formal hypothesis test in an intuitive way.

|         Hypothesis testing       |                      Trial                        |
|:---------------------------------|:--------------------------------------------------|
| Null hypothesis $H_0$ | **Accused of comitting a crime**. It has the "presumption of innocence", which means that it is *not guilty* until there is enough evidence to supporting its guilt |
| Sample $X_1,\ldots,X_n$ | Collection of small **evidences supporting innocence and guilt**. These evidences contain a certain degree of uncontrollable randomness because of how they were collected and the context regarding the case  |
| Statistic $T_n$ | **Summary of the evicences presented** by the prosecutor and defense lawyer   |
| Distribution of $T_n$ under $H_0$ | The **judge** conducting the trial. Evaluates the evidence presented by both sides and presents a verdict for $H_0$  |
| Significance level $\alpha$ | $1-\alpha$ is the **strength of evidences required by the judge for condemning $H_0$**. The judge allows evidences that on average condemn $100\alpha\%$ of the innocents, due to the randomness inherent to the evidence collection process. $\alpha=0.05$ is considered a reasonable level |
| $p$-value | **Decision** of the judge that measures the degree of compatibility, in a scale $0$--$1$, of the presumption of innocence with the summary of the evidences presented. If $p$-value$<\alpha$, $H_0$ is declared guilty. Otherwise, is declared not guilty |
| $H_0$ is rejected | $H_0$ is declared guilty: there are **strong evidences supporting its guilt** |
| $H_0$ is not rejected | $H_0$ is declared not guilty: either is **innocent or there are no enough evidences supporting its guilt** |

More formally, the $p$-value of an hypothesis test about $H0$ is defined as:

> The $p$-value is the probability of obtaining a statistic more unfavourable to $H_0$ than the observed, assuming that $H_0$ is true.

Therefore, **if the $p$-value is small** (smaller than the chosen level $\alpha$), **it is unlikely that the evidence against $H_0$ is due to randomness. As a consequence, $H_0$ is rejected**. If the $p$-value is large (larger than $\alpha$), then it is more possible that the evidences against $H_0$ are merely due to the randomness of the data. In this case, we do not reject $H_0$.

```{block, inspvalue, type = 'rmdinsight', cache = TRUE}
If $H_0$ holds, then the $p$-value (which is a random variable) is distributed uniformly in $(0,1)$. If $H_0$ does not hold, then the distribution of the $p$-value is not uniform but concentrated at $0$ (where the rejections of $H_0$ take place).
```

# Least squares and maximum likelihood {#app-mle}

Least squares had a prominent role in linear models. Why is so? After all, it is a purely *geometrical argument* for fitting a plane to a cloud of points. It is not motivated as a *statistical procedure* for estimating the unknown parameters $\boldsymbol{\beta}$... at least apparently.

However, **least squares estimation is equivalent to maximum likelihood estimation** under the assumptions of the model seen in Section \@ref(lm-i-assumps) (Normality is specially important here due to the squares present in the exponential of the Normal pdf). So maximum likelihood estimation, the most well-known statistical estimation method, is behind least squares if the assumptions of the model hold. 

First, recall that given the sample $(\mathbf{X}_1,Y_1),\ldots,(\mathbf{X}_n, Y_n)$, due to the assumptions introduced in Section \@ref(lm-i-assumps), we have that:
\begin{align*}
Y_i|(X_{i1}=x_{i1},\ldots,X_{ip}=x_{ip})\sim \mathcal{N}(\beta_0+\beta_1x_{i1}+\ldots+\beta_px_{ip},\sigma^2),\quad i=1,\ldots,n
\end{align*}
with $Y_1,\ldots,Y_n$ being independent conditionally on the sample of predictors. Equivalently stated in a compact matrix way:
\begin{align*}
\mathbf{Y}|\mathbf{X}\sim\mathcal{N}_n(\mathbf{X}\boldsymbol{\beta},\sigma^2\mathbf{I}).
\end{align*}
From these two equations we can obtain the log-likelihood function of $Y_1,\ldots,Y_n$ conditionally^[We assume that the randomness is on the response only.] on $\mathbf{X}_1,\ldots,\mathbf{X}_n$ as
\begin{align}
\ell(\boldsymbol{\beta})=\log\phi(\mathbf{Y};\mathbf{X}\boldsymbol{\beta},\sigma^2\mathbf{I})=\sum_{i=1}^n\log\phi(Y_i;(\mathbf{X}\boldsymbol{\beta})_i,\sigma).(\#eq:ell)
\end{align}

Now we are ready to show the next result.

```{theorem, thlik, label = "lik", cache = TRUE}
Under assumptions i--iv in Section \@ref(lm-i-assumps), the maximum likelihood estimate of $\boldsymbol{\beta}$ is the least squares estimate \@ref(eq:k3):
$$
\hat{\boldsymbol{\beta}}_\mathrm{ML}=\arg\max_{\boldsymbol{\beta}\in\mathbb{R}^{p+1}}\ell(\boldsymbol{\beta})=(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}\mathbf{Y}.
$$
```
```{proof, prooflik, cache = TRUE}
Expanding the first equality at \@ref(eq:ell) gives (recall that $|\sigma^2\mathbf{I}|^{1/2}=\sigma^{n}$)
$$
\ell(\boldsymbol{\beta})=-\log((2\pi)^{n/2}\sigma^n)-\frac{1}{2\sigma^2}(\mathbf{Y}-\mathbf{X}\boldsymbol{\beta})'(\mathbf{Y}-\mathbf{X}\boldsymbol{\beta}).
$$
In order to differentiate with respect to $\boldsymbol{\beta}$, we use that, for two vector-valued functions $f$ and $g$:
$$
\frac{\partial \mathbf{A}\mathbf{x}}{\partial \mathbf{x}}=\mathbf{A}\text{ and } \frac{\partial f(\mathbf{x})'g(\mathbf{x})}{\partial \mathbf{x}}=f(\mathbf{x})'\frac{\partial g(\mathbf{x})}{\partial \mathbf{x}}+g(\mathbf{x})'\frac{\partial f(\mathbf{x})}{\partial \mathbf{x}}.
$$
Then, differentiating with respect to $\boldsymbol{\beta}$ and equating to zero gives 
$$
\frac{1}{\sigma^2}(\mathbf{Y}-\mathbf{X}\boldsymbol{\beta})'\mathbf{X}=\frac{1}{\sigma^2}(\mathbf{Y}'\mathbf{X}-\boldsymbol{\beta}'\mathbf{X}'\mathbf{X})=0.
$$
This means that optimizing $\ell$ does not require knowledge on $\sigma^2$! Nicely, solving the above equation yields
$$
\hat{\boldsymbol{\beta}}=(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}\mathbf{Y}.
$$
```

# Dealing with missing data {#app-nas}

Missing data, codified as `NA` in `R`, can be problematic in predictive modeling. By default, most of the regression models in `R` work with the complete cases of the data, this is, **they exclude the cases in which there is at least one** `NA`. This may be problematic in certain cases. Let's see an example.

```{r, nas-1, echo = TRUE, collapse = TRUE, error = TRUE, cache = TRUE}
# The airquality dataset contains NA's
data("airquality")
head(airquality)
summary(airquality)

# Let's add more NA's for the sake of illustration
set.seed(123456)
airquality$Solar.R[runif(nrow(airquality)) < 0.7] <- NA
airquality$Day[runif(nrow(airquality)) < 0.1] <- NA

# See what are the fully-observed cases
comp <- complete.cases(airquality)
mean(comp) # Only 15% of cases are fully observed

# Complete cases
head(airquality[comp, ])

# Linear model on all the variables
summary(lm(Ozone ~ ., data = airquality)) # 129 not included

# Caution! Even if the problematic variable is excluded, only the complete 
# observations are employed
summary(lm(Ozone ~ . - Solar.R, data = airquality))

# Notice the difference with
summary(lm(Ozone ~ ., data = subset(airquality, select = -Solar.R)))

# Model selection can be problematic with missing data, since the number 
# of complete cases changes with the addition or removing of predictors
mod <- lm(Ozone ~ ., data = airquality)

# stepAIC drops an error
modAIC <- stepAIC(mod)

# Also, this will be problematic (the number of complete cases changes 
# with the predictors considered!)
modBIC <- stepAIC(mod, k = log(nrow(airquality)))

# Comparison of AICs or BICs is spurious: the scale of the likelihood changes 
# with the sample size (the likelihood decreases with n), which increases 
# AIC / BIC with n. Hence using BIC / AIC is not adequate for model selection
# with missing data.
AIC(lm(Ozone ~ ., data = airquality))
AIC(lm(Ozone ~ ., data = subset(airquality, select = -Solar.R)))

# Considers only complete cases including Solar.R
AIC(lm(Ozone ~ . - Solar.R, data = airquality))
```
We have seen the problems that missing data may cause in regression models. There are many techniques designed to handle missing data, depending on the missing data mechanism (whether is it completely at random or there is some pattern in the missing process) and the approach to impute the data (parametric, nonparametric, Bayesian, etc). We do not give an exhaustive view of the topic here, but we outline **three concrete approaches to handle missing data in practice**:

1. **Use complete cases**. This is the simplest solution and can be achieved by restricting the analysis to the set of fully-observed observations. The advantage of this solution is that it can be implemented very easily by using the `complete.cases` or `na.omit` f, and therefore the precision of the estimates will be much lower. In addition, it may lead to a biased representation of the original data (if the the missing process is associated with the values of the predictors).
2. **Remove predictors with many missing data**. This is another simple solution in case most of the missing data is concentrated in one predictor.
3. **Use imputation for the missing values**. The idea is to replace the missing observations on the response or the predictors with suitable values:
    - When the response is missing, we can use a **predictive model to predict the missing response**, then create a new fully-observed dataset containing the predictions instead of the missing values, and finally re-estimate the predictive model in this expanded dataset. This approach is attractive if most of the missing data is in the response (see next point).
    - When different predictors and the response are missing, we can use a direct imputation for them. The simplest approach is to **replace the missing data with the sample mean** of the observed cases (in the case of quantitative variables). This and **more sophisticated imputation methods**, based on predictive models, are available within the `mice` package [@R-mice]. This approach is interesting if the data contains many `NA`s scattered in different predictors (hence a complete cases analysis will be inefficient).

Let's put in practise these three approaches in the previous example.
```{r, nas-2, echo = TRUE, collapse = TRUE, error = TRUE, cache = TRUE, out.width = '70%', fig.pos = 'h!'}
# The complete cases approach is the default in R
summary(lm(Ozone ~ ., data = airquality))

# However, since the complete cases that R is going to consider depends on 
# which predictors are included, it is safer to exclude NA's explicitly
# before the fitting of a model
airqualityNoNA <- na.exclude(airquality)
summary(airqualityNoNA)

# The package VIM has a function to visualize where the missing data is 
# present. It gives the percentage of NA's for each variable and most 
# important combinations of NA's
library(VIM)
aggr(airquality)
aggr(airqualityNoNA)

# Stepwise regression without NA's - no problem
modBIC1 <- stepAIC(lm(Ozone ~ ., data = airqualityNoNA), 
                   k = log(nrow(airqualityNoNA)), trace = 0)
summary(modBIC1)

# But we only take into account 16% of the original data
nrow(airqualityNoNA) / nrow(airquality)

# Removing the predictor with many NA's, as we did before
# We also exclude NA's from other predictors
airqualityNoSolar.R <- na.exclude(subset(airquality, select = -Solar.R))
modBIC2 <- stepAIC(lm(Ozone ~ ., data = airqualityNoSolar.R), 
                   k = log(nrow(airqualityNoSolar.R)), trace = 0)
summary(modBIC2)
# In this example the approach works well because most of the NA's are 
# associated to the variable Solar.R

# Imput data using the sample mean
library(mice)
airqualityMean <- complete(mice(data = airquality, 
                                m = 1, method = "mean"))
head(airqualityMean)
# Explanation of the sintaxis:
# - complete() serves to retrieve the completed dataset from the mids object
# - m = 1 specifies that we only want a reconstruction of the dataset 
#   because the imputation method is deterministic (it can have randomness)
# - method = "mean" says that we want the sample mean to be used to fill
#   NA's in all the columns. This only works properly for quantitative variables.

# Impute using linear regression for the response (first column) and mean 
# for the predictors (remaining five columns)
airqualityLm <- complete(mice(data = airquality, m = 1,
                              method = c("norm.predict", rep("mean", 5))))
head(airqualityLm)

# Imputed data - some extrapolation problems may happen
airqualityLm$Ozone[is.na(airquality$Ozone)]

# Notice that the imputed data is the same (except for a small truncation that
# is introduced by mice) as
predict(lm(airquality$Ozone ~ ., data = airqualityMean), 
        newdata = airqualityMean[is.na(airquality$Ozone), -1])

# Removing the truncation with ridge = 0
complete(mice(data = airquality, m = 1,
              method = c("norm.predict", rep("mean", 5)), 
              ridge = 0))[is.na(airquality$Ozone), 1]

# The default mice's method (predictive mean matching) works better in this 
# case (in the sense that it does not yield negative Ozone values)
# Notice that there is randomness in the imputation!
airqualityMice <- complete(mice(data = airquality, m = 1, seed = 123))
head(airqualityMice)
```
See the available methods for `mice()` at `?mice`. There are specific methods for different kinds of variables (numeric, factor with two levels, and factors of more than two levels) and fairly advanced imputation methods.

# Course evaluation {#app-evaluation}

Evaluation is done by means of **two group projects**, to be done in groups of 3 (preferable) or 2 students, plus a presentation of one randomly-selected report. The final grade (in the scale 0-10) is:

    min(0.70 * reports_grade + 0.30 * individual_presentation_grade, 10)

The `individual_presentation_grade` is in the scale 0-10. The `reports_grade` (in the scale 0-12) is the weighted grade of the two reports about the following topics:

1.  Linear models I, II, and III (weight: 60%; **deadline: 2017/12/31**).
2.  Generalized linear models **or** Nonparametric regression (weight: 40%; **deadline: 2018/01/25**).

Deadlines are subjected to minor changes due to the course development.

#### Groups {-}

It is up to you to form the groups based on your grade expectations, affinity, complementary skills, etc. Take into account that all the students in a group will be graded evenly for the `reports_grade`.

Communicate the group compositions by filling this [Google Sheet](https://docs.google.com/a/uc3m.es/spreadsheets/d/10zWtuhpAEtfZs7tuL9wEPdGsLVVYT-MKDLudvuhEFCA/edit?usp=sharing) (you need to log in with your UC3M account). An example: if students A, B, and C form the group number `4` in the second project, add a `4` to the the rows of A, B, and C in the `Project II` column. Keep the same numbers in both columns if the composition of the group does not change. Under extraordinary circumstances to be communicated in advance to the professor, it is possible to switch group members from project to project. Be advised that this might have undesirable consequences if you do so: you might have to present two different randomly-chosen group reports.

#### Report structure {-}

The report must analyze a real dataset of your choice using the statistical methodology that we have seen in the lessons and labs. The purpose is to demonstrate that you understand and know how to apply and interpret the studied statistical techniques in a real-case scenario. Simulation studies (*i.e.*, analysis of artificially generated data) are also allowed if they are meant to illustrate interesting features or insights of the methods. The data analyzed does not have to be necessarily 'Big Data' and you may use the same dataset for different reports (if that makes sense).

Compulsory format guidelines:

-   Structure: title, authors, abstract, first section, second section and so on. Do not use a cover.
-   Font size: 11 points.
-   Font type: any sensible choice (`Comic Sans` is not).
-   Margins: any sensible choice. `Rmd` defaults works quite well.
-   Spacing: single space, single column.
-   Length: **10 pages at most**. This includes tables, graphics, code, equations, appendices, etc. Everything must fit in 10 pages. Want extra space? You can buy it at `-1.00` points per extra page! ;)
-   Style: be concise, specific, and insightful. Make a clever use of the space.
-   Code: do not include *all* the code in the report. Instead of, just add the parts you want to highlight (or that you believe are particularly interesting) and describe what the code is doing, but provide it for reproducibility. A great way of doing this is to create an `R Markdown` document and omit the lengthy code chunks, providing the `.pdf` and `.Rmd` outputs.

Mandatory report structure:

1.  **Abstract**. Provide a concise summary of the project. It must not exceed 250 words.
2.  **Introduction**. State what is the problem to be studied. Provide some context, the question(s) that you want to address, a motivation of its importance, references, etc. Must not be *very* extensive.
3.  **Methods**. Describe in detail the methods you are going to use, showing that you know what you are writing about. For example, you can include: background, useful interpretations, connections with other methods, alternative expositions, discussions, remarks, etc. if you wish, you can describe generally the method and then focus in exploring in more detail one particular feature that you use for the analysis. Optionally, go beyond what was taught in the lessons by considering extensions, improvements or better (in certain sense) implementations.
4.  **Statistical analysis**. Apply the statistical methods to the real data. Justify their adequacy to the data studied. Obtain analyses, in the form of summaries and plots. Provide a discussion about the outputs, interpret them and obtain insights about the data. These last points tend to be skipped and are very important!
5.  **Conclusions**. Summary of what was addressed in the project and of the most important conclusions. Takeaway messages. The conclusions are not required to be spectacular, but *fair and honest* in terms of what you have discovered.
6.  **References**. Refer to the sources of information that you have employed (for the data, for information on the data, for the methodology, for the statistical analyses, etc).

#### Report grading {-}

The report grading will be performed according to the following breakdown:

-   **Method description** (4.5 points). In general, graded according to the evidence you show about the knowledge of the methods. For example, method descriptions in your own words that are accurate, detailed, and present your own insights are positively received.
-   **Statistical analysis** (3.5 points). Graded according to the adequacy of the method to the data, the quantity and quality of the analysis provided, and, very importantly, the interpretation and the insights obtained from the statistical analyses.
-   **Presentation** of the report (2 points). This involves the readability, the conciseness, the correct usage of English, and the overall presentation quality (`R Markdown` is preferred!). Code documentation (comment what the code is doing!), reproducibility, and clarity are also evaluated.
-   **Excellence** (2 bonus points). Awarded for creativity of the analysis, originality of the problem studied and/or of the data acquisition process, use of advanced statistical tools, use of points briefly covered in lessons/labs, use of extensions of the methods, description of advanced insights of the methods, completeness of the report, use of advanced presentation tools, etc. Only awarded if the sum of regular points is above 7.5.

#### Report hand in {-}

Upload **one** report per group, in the form of a `.pdf`, to the `Turnitin` task associated to that report in AulaGlobal. Upload only a `.pdf`, not other file, so this can be checked for originality. **Also**, send **one** email per group to <edgarcia@est-econ.uc3m.es> with the subject "PM - Report X - Group Y" (replace X and Y as convenience) and write the group members in the body of the email. Attach the report (in `.pdf`) and all relevant scripts (e.g., `.Rmd` and `.R`) in a single `.zip` file. Be sure to ensure easy reproducibility of analysis (e.g., include the data you employed or the pertinent `.RData`).

**Deadlines**: the deadlines finish at **23:59** and the list of days can be seen above. Recall that the last deadline, the 25th of January at 23:59, is quite close to the presentation day, the 30th of January.

Need extra time? Buy it at expenses of a penalization in your grade: `-0.04` points per extra hour! ;) But only until the **25th of January**, reports received after that date will not be graded. The maximum grade according to the hours past the deadline is given in the graph below.

```{r, penalization, message = FALSE, echo = FALSE, out.width = '70%', fig.pos = 'h!', cache = TRUE}
# Requires lubridate
# install.packages("lubridate")
library(lubridate)

# Continuous discounting, each extra hour penalizes with -0.04 points
penalization <- function(date, deadline) {

  stopifnot(is.POSIXct(date) & is.POSIXct(deadline))
  as.numeric(date - deadline, "hours") * 0.04

}

# The discount graph
library(ggplot2)
deadline <- ymd_hms("2017-12-31 23:59:59")
hours <- seq(0, 216, by = 1)
penaliz <- penalization(date = deadline + duration(hours, units = "hours"),
                        deadline = deadline)
df <- data.frame(hours = hours, max_grade = pmax(10 - penaliz, 0))
ggplot(data = df, mapping = aes(x = hours, y = max_grade)) +
  geom_line() + geom_hline(yintercept = 5, color = "green") +
  geom_vline(xintercept = c(24, 48, 72), color = "red", lty = 2) +
  xlab("Extra hours") + ylab("Maximum grade = grade - penalization") +
  annotate("text", label = c("1 day ", "2 days", "3 days"),
           x = c(24, 48, 72) + 9, y = 0, colour = "red") +
  scale_x_continuous(breaks = seq(0, 216, by = 12)) +
  scale_y_continuous(breaks = 0:10)
```

#### Project presentations {-}

The **30th of January, from 09:00 to 21:00 in room 0.B.08**, each group must present one of the four projects. The project to be presented is randomly selected. On the 25th of January, a list with the selected reports and presentation schedule will be made public, so you know at which time you have to present. Restrictions, if any, must be communicated and justified in advance. The presentation will determine the `individual_presentation_grade`.

The evaluation will be carried out as follows:

-   Each group must present one report. Strict presentation policy: **all students must present for 5 minutes**. This is a 15-minutes or 10-minutes limit for the whole presentation, depending on the group size. Tailor your presentation to that time limit and break it evenly among the group members.
-   All group members must be present in the presentation. Failure to be at the presentation results in `individual_presentation_grade <- 0`.
-   The **presentation must follow tightly the report contents**. You just need to explain properly what you did in the group project without adding new material. Take this into account when preparing the reports.
-   In the presentation, you may want to highlight your main (personal or group) contributions and show your understanding of the methods.
-   **Questions may be asked during and after the presentation of each student.** Questions may be about the theory, implementation, or other matters related with the report. You are assumed to know and be able to defend all these topics. Together with the presentation, the accurateness, conciseness, and detail in the answers will determine the **grade**.

#### Academic fraud {-}

Evidences of academic fraud will have serious consequences, such as a zero grade for the whole group and the reporting of the fraud detection to the pertinent academic authorities. Academic fraud includes (but is not limited to) plagiarism, use of sources without proper credit, project outsourcing, and the use of external tutoring not mentioned explicitly.
